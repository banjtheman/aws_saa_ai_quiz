{"question_list": [{"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Knowledge of", "item": "Access controls and management across multiple accounts", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Establish common controls that restrict access to all identities in your organization. For example, you can restrict access to specific AWS Regions, or prevent your operators from deleting common resources, such as an IAM role used for your central security team.Common anti-patterns:Running workloads in your Organizational administrator account.Running production and non-production workloads in the same account.Level of risk exposed if this best practice is not established: MediumAs you grow and manage additional workloads in AWS, you should separate these workloads using accounts and manage those accounts using AWS Organizations. We recommend that you establish common permission guardrails that restrict access to all identities in your organization. For example, you can restrict access to specific AWS Regions, or prevent your team from deleting common resources, such as an IAM role used by your central security team.You can get started by implementing example service control policies, such as preventing users from disabling key services. SCPs use the IAM policy language and enable you to establish controls that all IAM principals (users and roles) adhere to. You can restrict access to specific service actions, resources and based on specific condition to meet the access control needs of your organization. If necessary, you can define exceptions to your guardrails. For example, you can restrict service actions for all IAM entities in the account except for a specific administrator role.We recommend you avoid running workloads in your management account. The management account should be used to govern and deploy security guardrails that will affect member accounts. Some AWS services support the use of a delegated administrator account. When available, you should use this delegated account instead of the management account. You should strongly limit access to the Organizational administrator account.Using a multi-account strategy allows you to have greater flexibility in applying guardrails to your workloads. The AWS Security Reference Architecture gives prescriptive guidance on how to design your account structure. AWS services such as AWS Control Tower provide capabilities to centrally manage both preventative and detective controls across your organization. Define a clear purpose for each account or OU within your organization and limit controls in line with that purpose.Related documents:AWS OrganizationsService control policies (SCPs)Get more out of service control policies in a multi-account environmentAWS Security Reference Architecture (AWS SRA)Related videos:Enforce Preventive Guardrails using Service Control PoliciesBuilding governance at scale with AWS Control TowerAWS Identity and Access Management deep dive\n* Govern the consumption of shared resources across accounts or within your AWS Organizations. Monitor shared resources and review shared resource access.Common anti-patterns:Using the default IAM trust policy when granting third party cross-account access.Level of risk exposed if this best practice is not established: LowAs you manage your workloads using multiple AWS accounts, you may need to share resources between accounts. This will very often be cross-account sharing within an AWS Organizations. Several AWS services, such as AWS Security Hub, Amazon GuardDuty, and AWS Backup have cross-account features integrated with Organizations. You can use AWS Resource Access Manager to share other common resources, such as VPC Subnets or Transit Gateway attachments, AWS Network Firewall, or Amazon SageMaker pipelines. If you want to ensure that your account only shares resources within your Organizations, we recommend using Service Control Policies (SCPs) to prevent access to external principals.When sharing resources, you should put measures in place to protect against unintended access. We recommend combining identity-based controls and network controls to create a data perimeter for your organization. These controls should place strict limits on what resources can be shared and prevent sharing or exposing resources that should not be allowed. For example, as a part of your data perimeter you could use VPC endpoint policies and the aws:PrincipalOrgId condition to ensure the identities accessing your Amazon S3 buckets belong to your organization.In some cases, you may want to allow share resources outside of your Organizations or grant third parties access to your account. For example, a partner may provide a monitoring solution that needs to access resources within your account. In those cases, you should create an IAM cross-account role with only the privileges needed by the third party. You should also craft a trust policy using the external ID condition. When using an external ID, you should generate a unique ID for each third party. The unique ID should not be supplied by or controlled by the third party. If the third party no longer needs access to your environment, you should remove the role. You should also avoid providing long-term IAM credentials to a third-party in all cases. Maintain awareness of other AWS services which natively support sharing. For example, the AWS Well-Architected Tool allows sharing a workload with other AWS accounts.When using service such as Amazon S3, it is recommended to disable ACLs for your Amazon S3 bucket and use IAM policies to define access control. For restricting access to an Amazon S3 origin from Amazon CloudFront, migrate from origin access identity (OAI) to origin access control (OAC) which supports additional features including server-side encryption with AWS KMS.Related documents:Bucket owner granting cross-account permission to objects it does not ownHow to use Trust Policies with IAMBuilding Data Perimeter on AWSHow to use an external ID when granting a third party access to your AWS resourcesRelated videos:Granular Access with AWS Resource Access ManagerSecuring your data perimeter with VPC endpointsEstablishing a data perimeter on AWS\n* Integrate access controls with operator and application lifecycle and your centralized federation provider. For example, remove a user\u2019s access when they leave the organization or change roles.As you manage workloads using separate accounts, there will be cases where you need to share resources between those accounts. We recommend that you share resources using AWS Resource Access Manager (AWS RAM). This service enables you to easily and securely share AWS resources within your AWS Organizations and Organizational Units. Using AWS RAM, access to shared resources is automatically granted or revoked as accounts are moved in and out of the Organization or Organization Unit with which they are shared. This helps ensure that resources are only shared with the accounts that you intend.Level of risk exposed if this best practice is not established: LowImplement a user access lifecycle policy for new users joining, job function changes, and users leaving so that only current users have access.Related documents:Attribute-based access control (ABAC)Grant least privilegeIAM Access AnalyzerRemove unnecessary credentialsWorking with PoliciesRelated videos:Become an IAM Policy Master in 60 Minutes or LessSeparation of Duties, Least Privilege, Delegation, and CI/CD\n\nKnowledge Area: Access controls and management across multiple accounts\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What action should an organization take to ensure access to shared resources is only granted to the accounts that are intended?\",\n\"answer_choices\": [\n    {\"answer\": \"Use the default IAM trust policy when granting third-party cross-account access\",\"is_correct\": \"False\",\"explanation\": \"Using the default IAM trust policy when granting third-party cross-account access is an anti-pattern and should be avoided. The organization should use AWS Resource Access Manager (RAM) to share resources between accounts.\"},\n    {\"answer\": \"Integrate access controls with operator and application lifecycle and the centralized federation provider\",\"is_correct\": \"True\",\"explanation\": \"Integrating access controls with operator and application lifecycle and the centralized federation provider is the best practice for managing access to shared resources across multiple accounts. The organization should use AWS RAM to share resources between accounts.\"},\n    {\"answer\": \"Put measures in place to protect against unintended access\",\"is_correct\": \"False\",\"explanation\": \"Put measures in place should be done as part of the process for managing access to shared resources, but it is not sufficient on its own. The organization should use AWS RAM to share resources between accounts.\"},\n    {\"answer\": \"Create an IAM cross-account role with only the privileges needed by the third party\",\"is_correct\": \"False\",\"explanation\": \"Creating an IAM cross-account role with only the privileges needed by the third party is part of the process for granting third-party cross-account access, but it is not sufficient on its own. The organization should use AWS RAM to share resources between accounts.\"}\n]}", "docs": ["('SEC03-BP05 Define permission guardrails for your organization - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_define_guardrails.html')", "('SEC03-BP08 Share resources securely - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_share_securely.html')", "('SEC03-BP06 Manage access based on lifecycle - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_lifecycle.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Knowledge of", "item": "AWS federated access and identity services (for example, AWS Identity and Access Management [IAM], AWS Single Sign-On [AWS SSO])", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* For workforce identities, rely on an identity provider that enables you to manage identities in a centralized place. This makes it easier to manage access across multiple applications and services, because you are creating, managing, and revoking access from a single location. For example, if someone leaves your organization, you can revoke access for all applications and services (including AWS) from one location. This reduces the need for multiple credentials and provides an opportunity to integrate with existing human resources (HR) processes.For federation with individual AWS accounts, you can use centralized identities for AWS with a SAML 2.0-based provider with AWS Identity and Access Management. You can use any provider\u2014 whether hosted by you in AWS, external to AWS, or supplied by the AWS Partner\u2014that is compatible with the SAML 2.0 protocol. You can use federation between your AWS account and your chosen provider to grant a user or application access to call AWS API operations by using a SAML assertion to get temporary security credentials. Web-based single sign-on is also supported, allowing users to sign in to the AWS Management Console from your sign in website.For federation to multiple accounts in your AWS Organizations, you can configure your identity source in AWS IAM Identity Center (successor to AWS Single Sign-On) (IAM Identity Center), and specify where your users and groups are stored. Once configured, your identity provider is your source of truth, and information can be synchronized using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. You can then look up users or groups and grant them IAM Identity Center access to AWS accounts, cloud applications, or both.IAM Identity Center integrates with AWS Organizations, which enables you to configure your identity provider once and then grant access to existing and new accounts managed in your organization. IAM Identity Center provides you with a default store, which you can use to manage your users and groups. If you choose to use the IAM Identity Center store, create your users and groups and assign their level of access to your AWS accounts and applications, keeping in mind the best practice of least privilege. Alternatively, you can choose to Connect to Your External Identity Provider using SAML 2.0, or Connect to Your Microsoft AD Directory using AWS Directory Service. Once configured, you can sign into the AWS Management Console, or the AWS mobile app, by authenticating through your central identity provider.For managing end-users or consumers of your workloads, such as a mobile app, you can use Amazon Cognito. It provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party, such as Amazon, Apple, Facebook, or Google.Level of risk exposed if this best practice is not established: HighCentralize administrative access: Create an Identity and Access Management (IAM) identity provider entity to establish a trusted relationship between your AWS account and your identity provider (IdP). IAM supports IdPs that are compatible with OpenID Connect (OIDC) or SAML 2.0 (Security Assertion Markup Language 2.0).Identity Providers and FederationCentralize application access: Consider Amazon Cognito for centralizing application access. It lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.Remove old IAM users and groups: After you start using an identity provider (IdP), remove IAM users and groups that are no longer required.Finding unused credentialsDeleting an IAM groupRelated documents:IAM Best PracticesSecurity Partner Solutions: Access and Access ControlTemporary Security CredentialsThe AWS Account Root UserRelated videos:Best Practices for Managing, Retrieving, and Rotating Secrets at ScaleManaging user permissions at scale with AWS IAM Identity Center (successor to AWS Single Sign-On)Mastering identity at every layer of the cake\n\nKnowledge Area: AWS federated access and identity services (for example, AWS Identity and Access Management [IAM], AWS Single Sign-On [AWS SSO])\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best practice for managing user identities and access in the given scenario?\", \n\"answer_choices\": [\n{\"answer\": \"Use IAM Identity Center to configure the identity source and set up federation with individual AWS accounts.\",\"is_correct\": \"True\",\"explanation\": \"IAM Identity Center integrates with AWS Organizations and enables you to configure your identity provider once and then grant access to existing and new accounts managed in your organization.\"},\n{\"answer\": \"Set up a web-based single sign-on for the users to access the AWS Management Console.\",\"is_correct\": \"False\",\"explanation\": \"Web-based single sign-on is supported, but it is not the best practice for managing user identities and access in the given scenario.\"},\n{\"answer\": \"Use Amazon Cognito for end-user or consumer management.\",\"is_correct\": \"False\",\"explanation\": \"Amazon Cognito is useful for managing end-users or consumers of your workloads, such as a mobile app. However, it is not the best practice for managing user identities and access in the given scenario.\"},\n{\"answer\": \"Set up an identity provider that enables centralized management of identities.\",\"is_correct\": \"True\",\"explanation\": \"Relying on an identity provider that enables you to manage identities in a centralized place is the best practice for managing user identities and access in the given scenario. This makes it easier to manage access across multiple applications and services.\"}\n]}", "docs": ["('SEC02-BP04 Rely on a centralized identity provider - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_identities_identity_provider.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Knowledge of", "item": "AWS global infrastructure (for example, Availability Zones, AWS Regions)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* Use the cloud location options available to reduce network latency or improve throughput. Use AWS Regions, Availability Zones, placement groups, and edge locations such as AWS Outposts, AWS Local Zones, and AWS Wavelength, to reduce network latency or improve throughput.The AWS Cloud infrastructure is built around Regions and Availability Zones. A Region is a physical location in the world having multiple Availability Zones.Availability Zones consist of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. These Availability Zones offer you the ability to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data centerChoose the appropriate Region or Regions for your deployment based on the following key elements:Where your users are located: Choosing a Region close to your workload\u2019s users ensures lower latency when they use the workload.Where your data is located: For data-heavy applications, the major bottleneck in latency is data transfer. Application code should execute as close to the data as possible.Other constraints: Consider constraints such as security and compliance.Amazon EC2 provides placement groups for networking. A placement group is a logical grouping of instances to decrease latency or increase reliability. Using placement groups with supported instance types and an Elastic Network Adapter (ENA) enables workloads to participate in a low-latency, 25 Gbps network. Placement groups are recommended for workloads that benefit from low network latency, high network throughput, or both. Using placement groups has the benefit of lowering jitter in network communications.Latency-sensitive services are delivered at the edge using a global network of edge locations. These edge locations commonly provide services such as content delivery network (CDN) and domain name system (DNS). By having these services at the edge, workloads can respond with low latency to requests for content or DNS resolution. These services also provide geographic services such as geo targeting of content (providing different content based on the end users\u2019 location), or latency-based routing to direct end users to the nearest Region (minimum latency).Amazon CloudFront is a global CDN that can be used to accelerate both static content such as images, scripts, and videos, as well as dynamic content such as APIs or web applications. It relies on a global network of edge locations that will cache the content and provide high-performance network connectivity to your users. CloudFront also accelerates many other features such as content uploading and dynamic applications, making it a performance addition to all applications serving traffic over the internet. Lambda@Edge is a feature of Amazon CloudFront that will let you run code closer to users of your workload, which improves performance and reduces latency.Amazon Route 53 is a highly available and scalable cloud DNS web service. It\u2019s designed to give developers and businesses an extremely reliable and cost-effective way to route end users to internet applications by translating names, like www.example.com, into numeric IP addresses, like 192.168.2.1, that computers use to connect to each other. Route 53 is fully compliant with IPv6.AWS Outposts is designed for workloads that need to remain on-premises due to latency requirements, where you want that workload to run seamlessly with the rest of your other workloads in AWS. AWS Outposts are fully managed and configurable compute and storage racks built with AWS-designed hardware that allow you to run compute and storage on-premises, while seamlessly connecting to the broad array of AWS services in in the cloud.AWS Local Zones is designed to run workloads that require single-digit millisecond latency, like video rendering and graphics intensive, virtual desktop applications. Local Zones allow you to gain all the benefits of having compute and storage resources closer to end-users.AWS Wavelength is designed to deliver ultra-low latency applications to 5G devices by extending AWS infrastructure, services, APIs, and tools to 5G networks. Wavelength embeds storage and compute inside telco providers 5G networks to help your 5G workload if it requires single-digit millisecond latency, such as IoT devices, game streaming, autonomous vehicles, and live media production.Use edge services to reduce latency and to enable content caching. Ensure that you have configured cache control correctly for both DNS and HTTP/HTTPS to gain the most benefit from these approaches.Common anti-patterns:You consolidate all workload resources into one geographic location.You chose the closest region to your location but not to the workload end user.Benefits of establishing this best practice: You must ensure that your network is available wherever you want to reach customers. Using the AWS private global network ensures that your customers get the lowest latency experience by deploying workloads into the locations nearest them.Level of risk exposed if this best practice is not established: MediumReduce latency by selecting the correct locations: Identify where your users and data are located. Take advantage of AWS Regions, Availability Zones, placement groups, and edge locations to reduce latency.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n\nKnowledge Area: AWS global infrastructure (for example, Availability Zones, AWS Regions)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best way to reduce network latency and improve throughput with AWS global infrastructure?\", \n\"answer_choices\": [\n    {\"answer\": \"Deploy workloads into the closest AWS region to your location.\",\"is_correct\": \"false\",\"explanation\": \"This is not the best way as it does not take into account where the user is located, which is a key factor in reducing network latency.\"},\n    {\"answer\": \"Deploy workloads into the AWS region closest to the user.\",\"is_correct\": \"true\",\"explanation\": \"This is the best way as it takes into account where the user is located, which is a key factor in reducing network latency.\"},\n    {\"answer\": \"Use AWS Outposts to run workloads on-premises.\",\"is_correct\": \"false\",\"explanation\": \"This is not the best way as it is not designed to reduce network latency.\"},\n    {\"answer\": \"Use AWS Route 53 to route end users to internet applications.\",\"is_correct\": \"false\",\"explanation\": \"This is not the best way as it is not designed to reduce network latency.\"}\n]}", "docs": ["('PERF05-BP06 Choose your workload\u2019s location based on network requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_location.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Knowledge of", "item": "AWS security best practices (for example, the principle of least privilege)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* Grant only the access that identities require by allowing access to specific actions on specific AWS resources under specific conditions. Rely on groups and identity attributes to dynamically set permissions at scale, rather than defining permissions for individual users. For example, you can allow a group of developers access to manage only resources for their project. This way, when a developer is removed from the group, access for the developer is revoked everywhere that group was used for access control, without requiring any changes to the access policies.Common anti-patterns:Defaulting to granting users administrator permissions.Using the root account for day-to-day activities.Level of risk exposed if this best practice is not established: HighEstablishing a principle of least privilege ensures that identities are only permitted to perform the most minimal set of functions necessary to fulfill a specific task, while balancing usability and efficiency. Operating on this principle limits unintended access and helps ensure that you can audit who has access to which resources. In AWS, identities have no permissions by default except for the root user. The credentials for the root user should be tightly controlled and only be used for a few specific tasks.You use policies to explicitly grant permissions attached to IAM or resource entities, such as an IAM role used by federated identities or machines, or resources (for example, S3 buckets). When you create and attach a policy, you can specify the service actions, resources, and conditions that must be true for AWS to allow access. AWS supports a variety of conditions to help you scope down access. For example, using the PrincipalOrgIDcondition key, the identifier of the AWS Organizations is verified so access can be granted within your AWS Organization.You can also control requests that AWS services make on your behalf, such as AWS CloudFormation creating an AWS Lambda function by using the CalledVia condition key. You should layer different policy types to effectively limit the overall permissions within an account. For example, you can allow your application teams to create their own IAM policies, but use a Permission Boundary to limit the maximum permissions they can grant.There are several AWS capabilities to help you scale permission management and adhere to the principle of least privilege. Attribute Based Access control allows you to limit permissions based on the tag of a resource, for making authorization decisions based on the tags applied to the resource and the calling IAM principal. This enables you to combine your tagging and permissions policy to achieve fine-grained resource access without needing many custom policies.Another way to accelerate creating a least privilege policy, is to base your policy on CloudTrail permissions after an activity runs. AWS Identity and Access Management Access Analyzer (IAM Access Analyzer) can automatically generate an IAM policy based on activity. You can also use IAM Access Analyzer at the Organization or individual account level to track the last accessed information for a particular policy.Establish a cadence of reviewing these details and removing unneeded permissions. You should establish permissions guardrails within your AWS Organization to control the maximum permissions within any member account. Services such as AWS Control Tower have prescriptive managed preventative controls and allow you to define your own controls.Related documents:Permissions boundaries for IAM entitiesTechniques for writing least privilege IAM policiesIAM Access Analyzer makes it easier to implement least privilege permissions by generating IAM policies based on access activityDelegate permission management to developers by using IAM permissions boundariesRefining Permissions using last accessed informationIAM policy types and when to use themTesting IAM policies with the IAM policy simulatorGuardrails in AWS Control TowerZero Trust architectures: An AWS perspectiveHow to implement the principle of least privilege with CloudFormation StackSetsRelated videos:Next-generation permissions managementZero Trust: An AWS perspectiveHow can I use permissions boundaries to limit IAM users and roles to prevent privilege escalation?Related examples:Lab: IAM permissions boundaries delegating role creation\n* There are seven design principles for security in the cloud:Implement a strong identity foundation: Implement the principle of least privilege and enforce separation of duties with appropriate authorization for each interaction with your AWS resources. Centralize identity management, and aim to eliminate reliance on long-term static credentials.Enable traceability: Monitor, alert, and audit actions and changes to your environment in real time. Integrate log and metric collection with systems to automatically investigate and take action.Apply security at all layers: Apply a defense in depth approach with multiple security controls. Apply to all layers (for example, edge of network, VPC, load balancing, every instance and compute service, operating system, application, and code).Automate security best practices: Automated software-based security mechanisms improve your ability to securely scale more rapidly and cost-effectively. Create secure architectures, including the implementation of controls that are defined and managed as code in version-controlled templates.Protect data in transit and at rest: Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate.Keep people away from data: Use mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data. This reduces the risk of mishandling or modification and human error when handling sensitive data.Prepare for security events: Prepare for an incident by having incident management and investigation policy and processes that align to your organizational requirements. Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery.\n\nKnowledge Area: AWS security best practices (for example, the principle of least privilege)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice for minimizing the risk of privilege escalation when managing IAM users and roles?\", \n\"answer_choices\": [\n    {\"answer\": \"Grant users administrator permissions\",\"is_correct\": \"False\",\"explanation\": \"Defaulting to granting users administrator permissions is an anti-pattern and not best practice. \"},\n    {\"answer\": \"Use the root account for day-to-day activities\",\"is_correct\": \"False\",\"explanation\": \"The credentials for the root user should be tightly controlled and only be used for a few specific tasks. \"},\n    {\"answer\": \"Delegate permission management to developers by using IAM permissions boundaries\",\"is_correct\": \"True\",\"explanation\": \"Using IAM permissions boundaries, you can limit the maximum permissions that developers can grant. This helps ensure that you are adhering to the principle of least privilege. \"},\n    {\"answer\": \"Write least privilege IAM policies\",\"is_correct\": \"False\",\"explanation\": \"Least privilege IAM policies help ensure that identities are only permitted to perform the most minimal set of functions necessary to fulfill their task, while balancing usability and efficiency. However, they are not the best practice for minimizing risk of privilege escalation. \"}\n]}", "docs": ["('SEC03-BP02 Grant least privilege access - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_least_privileges.html')", "('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-design.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Knowledge of", "item": "The AWS shared responsibility model", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* There are six best practice areas for security in the cloud:SecurityIdentity and Access ManagementDetectionInfrastructure ProtectionData ProtectionIncident ResponseBefore you architect any workload, you need to put in place practices that influence security. You will want to control who can do what. In addition, you want to be able to identify security incidents, protect your systems and services, and maintain the confidentiality and integrity of data through data protection. You should have a well-defined and practiced process for responding to security incidents. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.The AWS Shared Responsibility Model enables organizations that adopt the cloud to achieve their security and compliance goals. Because AWS physically secures the infrastructure that supports our cloud services, as an AWS customer you can focus on using services to accomplish your goals. The AWS Cloud also provides greater access to security data and an automated approach to responding to security events.\n* Share best practices across teams to increase awareness and maximize the benefits of development efforts.On AWS, application, compute, infrastructure, and operations can be defined and managed using code methodologies. This allows for easy release, sharing, and adoption.Many AWS services and resources are designed to be shared across accounts, enabling you to share created assets and learnings across your teams. For example, you can share CodeCommit repositories, Lambda functions, Amazon S3 buckets, and AMIs to specific accounts.When you publish new resources or updates, use Amazon SNS to provide cross account notifications. Subscribers can use Lambda to get new versions.If shared standards are enforced in your organization, it\u2019s critical that mechanisms exist to request additions, changes, and exceptions to standards in support of teams\u2019 activities. Without this option, standards become a constraint on innovation.Common anti-patterns:You have created your own user authentication mechanism, as have each of the other development teams in your organization. Your users have to maintain a separate set of credentials for each part of the system they want to access.You have created your own user authentication mechanism, as have each of the other development teams in your organization. Your organization is given a new compliance requirement that must be met. Every individual development team must now invest the resources to implement the new requirement.You have created your own screen layout, as have each of the other development teams in your organization. Your users are complaining about the difficulty of navigating the inconsistent interfaces.Benefits of establishing this best practice: Use shared standards to support the adoption of best practices and to maximizes the benefits of development efforts where standards satisfy requirements for multiple applications or organizations.Level of risk exposed if this best practice is not established: MediumShare design standards: Share existing best practices, design standards, checklists, operating procedures, and guidance and governance requirements across teams to reduce complexity and maximize the benefits from development efforts. Ensure that procedures exist to request changes, additions, and exceptions to design standards to support continual improvement and innovation. Ensure that teams are aware of published content so that they can take advantage of content, and limit rework and wasted effort.Delegating access to your AWS environmentShare an AWS CodeCommit repositoryEasy authorization of AWS Lambda functionsSharing an AMI with specific AWS accountsSpeed template sharing with an AWS CloudFormation designer URLUsing AWS Lambda with Amazon SNSRelated documents:Easy authorization of AWS Lambda functionsShare an AWS CodeCommit repositorySharing an AMI with specific AWS accountsSpeed template sharing with an AWS CloudFormation designer URLUsing AWS Lambda with Amazon SNSRelated videos:Delegating access to your AWS environment\n* Managed services shift responsibility for maintaining high-average utilization, and sustainability optimization of the deployed hardware to AWS. Use managed services to distribute the sustainability impact of the service across all tenants of the service, reducing your individual contribution.Level of risk exposed if this best practice is not established: LowMigrate from self-hosted services to managed services. For example, use managed Amazon Relational Database Service (Amazon RDS) instances instead of maintaining your own Amazon RDS instances on Amazon Elastic Compute Cloud (Amazon EC2), or use managed container services, such as AWS Fargate, instead of implementing your own container infrastructure.Related documents:AWS FargateAmazon DocumentDBAmazon Elastic Kubernetes Service (EKS)Amazon Managed Streaming for Apache Kafka (Amazon MSK)Amazon RedshiftAmazon Relational Database Service (RDS)\n\nKnowledge Area: The AWS shared responsibility model\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"What is an example of best practice, as defined in the AWS Shared Responsibility Model, that the retail company should use to ensure their e-commerce platform is secure and compliant during peak shopping seasons?\",\n \"answer_choices\": [\n {\"answer\": \"Migrate from self-hosted services to managed services.\", \"is_correct\": \"false\",\"explanation\": \"This best practice applies to sustainability optimization, not necessarily ensuring the e-commerce platform is secure and compliant.\"}, \n{\"answer\": \"Use shared standards and mechanisms to request additions, changes and exceptions to standards.\", \"is_correct\": \"true\",\"explanation\": \"This best practice ensures that standards are enforced and requests for additions, changes and exceptions are managed, contributing to both security and compliance.\"},\n{\"answer\": \"Share design standards and best practices across teams.\", \"is_correct\": \"false\",\"explanation\": \"This best practice increases awareness and maximizes the benefits of development efforts, but does not necessarily contribute to security and compliance.\"},\n{\"answer\": \"Delegate access to the AWS environment.\", \"is_correct\": \"false\",\"explanation\": \"This best practice is important for managing access, but does not necessarily contribute to security and compliance.\"}\n]}", "docs": ["('Definition - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-def.html')", "('OPS05-BP06 Share design standards - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_dev_integ_share_design_stds.html')", "('SUS05-BP03 Use managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a4.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Skills in", "item": "Applying AWS security best practices to IAM users and root users (for example, multi-factor authentication [MFA])", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* Enforce minimum password length, and educate your users to avoid common or reused passwords. Enforce multi-factor authentication (MFA) with software or hardware mechanisms to provide an additional layer of verification. For example, when using IAM Identity Center as the identity source, configure the \u201ccontext-aware\u201d or \u201calways-on\u201d setting for MFA, and allow users to enroll their own MFA devices to accelerate adoption. When using an external identity provider (IdP), configure your IdP for MFA.Level of risk exposed if this best practice is not established: HighCreate an AWS Identity and Access Management (IAM) policy to enforce MFA sign-in: Create a customer-managed IAM policy that prohibits all IAM actions except for the ones that allow a user to assume roles, change their own credentials, and manage their MFA devices on the My Security Credentials page.Enable MFA in your identity provider: Enable MFA in the identity provider or single sign-on service, such as AWS IAM Identity Center (successor to AWS Single Sign-On), that you use.Configure a strong password policy: Configure a strong password policy in IAM and federated identity systems to help protect against brute-force attacks.Rotate credentials regularly: Ensure administrators of your workload change their passwords and access keys (if used) regularly.Related documents:Getting Started with AWS Secrets ManagerIAM Best PracticesIdentity Providers and FederationThe AWS Account Root UserTemporary Security CredentialsSecurity Partner Solutions: Access and Access ControlRelated videos:Best Practices for Managing, Retrieving, and Rotating Secrets at ScaleManaging user permissions at scale with IAM Identity CenterMastering identity at every layer of the cake\n* There are a number of aspects to securing your AWS accounts, including the securing of, and not using the root user, and keeping your contact information up-to-date. You can use AWS Organizations to centrally manage and govern your accounts as you grow and scale your workloads in AWS. AWS Organizations helps you manage accounts, set controls, and configure services across your accounts.Level of risk exposed if this best practice is not established: HighUse AWS Organizations: Use AWS Organizations to centrally enforce policy-based management for multiple AWS accounts.Getting started with AWS OrganizationsHow to use service control policies to set permission guardrails across accounts in your AWS OrganizationLimit use of the AWS root user: Only use the root user to perform tasks that specifically require it.AWS Tasks That Require AWS Account Root User CredentialsEnable multi-factor-authentication (MFA) for the root user: Enable MFA on the AWS account root user, if AWS Organizations is not managing root users for you.Root userPeriodically change the root user password: Changing the root user password reduces the risk that a saved password can be used. This is especially important if you are not using AWS Organizations and anyone has physical access.Changing the AWS account root user passwordEnable notification when the AWS account root user is used: Being notified automatically reduces risk.How to receive notifications when your AWS account's root access keys are usedRestrict access to newly added Regions: For new AWS Regions, IAM resources, such as users and roles, will only be propagated to the Regions that you enable.Setting permissions to enable accounts for upcoming AWS RegionsConsider AWS CloudFormation StackSets: CloudFormation StackSets can be used to deploy resources including IAM policies, roles, and groups into different AWS accounts and Regions from an approved template.Use CloudFormation StackSetsRelated documents:AWS Control TowerAWS Security Audit GuidelinesIAM Best PracticesSecurity BulletinsRelated videos:Enable AWS adoption at scale with automation and governanceSecurity Best Practices the Well-Architected WayRelated examples:Lab: AWS account and root user\n\nKnowledge Area: Applying AWS security best practices to IAM users and root users (for example, multi-factor authentication [MFA])\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following best practices can be used to secure IAM users and root users in AWS?\", \n\"answer_choices\": [{\"answer\": \"Ensure administrators of your workload change their passwords and access keys regularly.\",\"is_correct\": \"False\", \"explanation\": \"This is only one part of the best practices for IAM users and root users - other best practices include enabling MFA, creating an IAM policy to enforce MFA sign-in, enabling MFA in your identity provider, and configuring a strong password policy.\"},{\"answer\": \"Use AWS Organizations to centrally manage and govern your accounts.\",\"is_correct\": \"True\", \"explanation\": \"Using AWS Organizations to centrally manage and govern your accounts is a best practice for securing IAM users and root users.\"},{\"answer\": \"Only use the root user to perform tasks that specifically require it.\",\"is_correct\": \"True\", \"explanation\": \"Limiting use of the AWS root user is a best practice for securing IAM users and root users.\"},{\"answer\": \"Configure a strong password policy in IAM and federated identity systems.\",\"is_correct\": \"True\", \"explanation\": \"Configuring a strong password policy in IAM and federated identity systems is a best practice for securing IAM users and root users.\"}]}", "docs": ["('SEC02-BP01 Use strong sign-in mechanisms - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_identities_enforce_mechanisms.html')", "('SEC01-BP02 Secure AWS account - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_securely_operate_aws_account.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Skills in", "item": "Designing a flexible authorization model that includes IAM users, groups, roles, and policies", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* Implement groups and roles that align to your policies and control who can create, modify, or decommission instances and resources in each group. For example, implement development, test, and production groups. This applies to AWS services and third-party solutions.Level of risk exposed if this best practice is not established: LowAfter you develop policies, you can create logical groups and roles of users within your organization. This allows you to assign permissions and control usage. Begin with high-level groupings of people. Typically this aligns with organizational units and job roles (for example, systems administrator in the IT Department, or financial controller). The groups join people that do similar tasks and need similar access. Roles define what a group must do. For example, a systems administrator in IT requires access to create all resources, but an analytics team member only needs to create analytics resources.Implementation stepsImplement groups: Using the groups of users defined in your organizational policies, implement the corresponding groups, if necessary. Refer to the security pillar for best practices on users, groups, and authentication.Implement roles and policies: Using the actions defined in your organizational policies, create the required roles and access policies. Refer to the security pillar for best practices on roles and policies.Related documents:AWS managed policies for job functionsAWS multiple account billing strategyControl access to AWS Regions using IAM policiesWell-Architected Security PillarRelated examples:Well-Architected Lab Basic Identity and Access\n* Each component or resource of your workload needs to be accessed by administrators, end users, or other components. Have a clear definition of who or what should have access to each component, choose the appropriate identity type and method of authentication and authorization.Common anti-patterns:Hard-coding or storing secrets in your application.Granting custom permissions for each user.Using long-lived credentials.Level of risk exposed if this best practice is not established: HighEach component or resource of your workload needs to be accessed by administrators, end users, or other components. Have a clear definition of who or what should have access to each component, choose the appropriate identity type and method of authentication and authorization.Regular access to AWS accounts within the organization should be provided using federated access or a centralized identity provider. You should also centralize your identity management and ensure that there is an established practice to integrate AWS access to your employee access lifecycle. For example, when an employee changes to a job role with a different access level, their group membership should also change to reflect their new access requirements.When defining access requirements for non-human identities, determine which applications and components need access and how permissions are granted. Using IAM roles built with the least privilege access model is a recommended approach. AWS Managed policies provide predefined IAM policies that cover most common use cases.AWS services, such as AWS Secrets Manager and AWS Systems Manager Parameter Store, can help decouple secrets from the application or workload securely in cases where it's not feasible to use IAM roles. In Secrets Manager, you can establish automatic rotation for your credentials. You can use Systems Manager to reference parameters in your scripts, commands, SSM documents, configuration, and automation workflows by using the unique name that you specified when you created the parameter.You can use AWS Identity and Access Management Roles Anywhere to obtain temporary security credentials in IAM for workloads that run outside of AWS. Your workloads can use the same IAM policies and IAM roles that you use with AWS applications to access AWS resources.Where possible, prefer short-term temporary credentials over long-term static credentials. For scenarios in which you need IAM users with programmatic access and long-term credentials, use access key last used information to rotate and remove access keys.Related documents:Attribute-based access control (ABAC)AWS IAM Identity Center (successor to AWS Single Sign-On)IAM Roles AnywhereAWS Managed policies for IAM Identity CenterAWS IAM policy conditionsIAM use casesRemove unnecessary credentialsWorking with PoliciesHow to control access to AWS resources based on AWS account, OU, or organizationIdentify, arrange, and manage secrets easily using enhanced search in AWS Secrets ManagerRelated videos:Become an IAM Policy Master in 60 Minutes or LessSeparation of Duties, Least Privilege, Delegation, and CI/CDStreamlining identity and access management for innovation\n\nKnowledge Area: Designing a flexible authorization model that includes IAM users, groups, roles, and policies\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best approach for the transportation company to use when granting access to AWS resources for their connected vehicles?\", \n\"answer_choices\": [\n    {\"answer\": \"Granting custom permissions for each vehicle\",\"is_correct\": \"False\",\"explanation\": \"This is not recommended since it would be time consuming and difficult to maintain.\"}, \n    {\"answer\": \"Using long-lived credentials\",\"is_correct\": \"False\",\"explanation\": \"This is not recommended since it can increase the risk of unauthorized access.\"},\n    {\"answer\": \"Using IAM roles built with the least privilege access model\",\"is_correct\": \"True\",\"explanation\": \"This is the best approach since it allows for secure and granular control of access.\"},\n    {\"answer\": \"Storing secrets in the application\",\"is_correct\": \"False\",\"explanation\": \"This is not recommended since it is insecure and increases the risk of unauthorized access.\"}\n]}", "docs": ["('COST02-BP04 Implement groups and roles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_govern_usage_groups_roles.html')", "('SEC03-BP01 Define access requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_define.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Skills in", "item": "Designing a role-based access control strategy (for example, AWS Security Token Service [AWS STS], role switching, cross-account access)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Establish common controls that restrict access to all identities in your organization. For example, you can restrict access to specific AWS Regions, or prevent your operators from deleting common resources, such as an IAM role used for your central security team.Common anti-patterns:Running workloads in your Organizational administrator account.Running production and non-production workloads in the same account.Level of risk exposed if this best practice is not established: MediumAs you grow and manage additional workloads in AWS, you should separate these workloads using accounts and manage those accounts using AWS Organizations. We recommend that you establish common permission guardrails that restrict access to all identities in your organization. For example, you can restrict access to specific AWS Regions, or prevent your team from deleting common resources, such as an IAM role used by your central security team.You can get started by implementing example service control policies, such as preventing users from disabling key services. SCPs use the IAM policy language and enable you to establish controls that all IAM principals (users and roles) adhere to. You can restrict access to specific service actions, resources and based on specific condition to meet the access control needs of your organization. If necessary, you can define exceptions to your guardrails. For example, you can restrict service actions for all IAM entities in the account except for a specific administrator role.We recommend you avoid running workloads in your management account. The management account should be used to govern and deploy security guardrails that will affect member accounts. Some AWS services support the use of a delegated administrator account. When available, you should use this delegated account instead of the management account. You should strongly limit access to the Organizational administrator account.Using a multi-account strategy allows you to have greater flexibility in applying guardrails to your workloads. The AWS Security Reference Architecture gives prescriptive guidance on how to design your account structure. AWS services such as AWS Control Tower provide capabilities to centrally manage both preventative and detective controls across your organization. Define a clear purpose for each account or OU within your organization and limit controls in line with that purpose.Related documents:AWS OrganizationsService control policies (SCPs)Get more out of service control policies in a multi-account environmentAWS Security Reference Architecture (AWS SRA)Related videos:Enforce Preventive Guardrails using Service Control PoliciesBuilding governance at scale with AWS Control TowerAWS Identity and Access Management deep dive\n* Govern the consumption of shared resources across accounts or within your AWS Organizations. Monitor shared resources and review shared resource access.Common anti-patterns:Using the default IAM trust policy when granting third party cross-account access.Level of risk exposed if this best practice is not established: LowAs you manage your workloads using multiple AWS accounts, you may need to share resources between accounts. This will very often be cross-account sharing within an AWS Organizations. Several AWS services, such as AWS Security Hub, Amazon GuardDuty, and AWS Backup have cross-account features integrated with Organizations. You can use AWS Resource Access Manager to share other common resources, such as VPC Subnets or Transit Gateway attachments, AWS Network Firewall, or Amazon SageMaker pipelines. If you want to ensure that your account only shares resources within your Organizations, we recommend using Service Control Policies (SCPs) to prevent access to external principals.When sharing resources, you should put measures in place to protect against unintended access. We recommend combining identity-based controls and network controls to create a data perimeter for your organization. These controls should place strict limits on what resources can be shared and prevent sharing or exposing resources that should not be allowed. For example, as a part of your data perimeter you could use VPC endpoint policies and the aws:PrincipalOrgId condition to ensure the identities accessing your Amazon S3 buckets belong to your organization.In some cases, you may want to allow share resources outside of your Organizations or grant third parties access to your account. For example, a partner may provide a monitoring solution that needs to access resources within your account. In those cases, you should create an IAM cross-account role with only the privileges needed by the third party. You should also craft a trust policy using the external ID condition. When using an external ID, you should generate a unique ID for each third party. The unique ID should not be supplied by or controlled by the third party. If the third party no longer needs access to your environment, you should remove the role. You should also avoid providing long-term IAM credentials to a third-party in all cases. Maintain awareness of other AWS services which natively support sharing. For example, the AWS Well-Architected Tool allows sharing a workload with other AWS accounts.When using service such as Amazon S3, it is recommended to disable ACLs for your Amazon S3 bucket and use IAM policies to define access control. For restricting access to an Amazon S3 origin from Amazon CloudFront, migrate from origin access identity (OAI) to origin access control (OAC) which supports additional features including server-side encryption with AWS KMS.Related documents:Bucket owner granting cross-account permission to objects it does not ownHow to use Trust Policies with IAMBuilding Data Perimeter on AWSHow to use an external ID when granting a third party access to your AWS resourcesRelated videos:Granular Access with AWS Resource Access ManagerSecuring your data perimeter with VPC endpointsEstablishing a data perimeter on AWS\n\nKnowledge Area: Designing a role-based access control strategy (for example, AWS Security Token Service [AWS STS], role switching, cross-account access)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice for granting third-party cross-account access when sharing resources on AWS?\", \"answer_choices\": [{\"answer\": \"Running workloads in your Organizational administrator account\", \"is_correct\": \"False\", \"explanation\": \"Running workloads in the organizational administrator account should be avoided. The management account should be used to govern and deploy security guardrails that will affect member accounts.\"},{\"answer\": \"Creating an IAM role with only the privileges needed and crafting a trust policy using the external ID condition\", \"is_correct\": \"True\", \"explanation\": \"Creating an IAM role with only the privileges needed and crafting a trust policy using the external ID condition is the best practice for granting third-party cross-account access when sharing resources on AWS.\"},{\"answer\": \"Using the default IAM trust policy when granting third party cross-account access\", \"is_correct\": \"False\", \"explanation\": \"Using the default IAM trust policy when granting third party cross-account access is an anti-pattern and should be avoided.\"},{\"answer\": \"Using AWS Resource Access Manager to share common resources\", \"is_correct\": \"False\", \"explanation\": \"Using AWS Resource Access Manager to share common resources is a good practice when sharing resources, but it is not the best practice for granting third-party cross-account access.\"}]}", "docs": ["('SEC03-BP05 Define permission guardrails for your organization - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_define_guardrails.html')", "('SEC03-BP08 Share resources securely - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_share_securely.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Skills in", "item": "Designing a security strategy for multiple AWS accounts (for example, AWS Control Tower, service control policies [SCPs])", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* Establish common controls that restrict access to all identities in your organization. For example, you can restrict access to specific AWS Regions, or prevent your operators from deleting common resources, such as an IAM role used for your central security team.Common anti-patterns:Running workloads in your Organizational administrator account.Running production and non-production workloads in the same account.Level of risk exposed if this best practice is not established: MediumAs you grow and manage additional workloads in AWS, you should separate these workloads using accounts and manage those accounts using AWS Organizations. We recommend that you establish common permission guardrails that restrict access to all identities in your organization. For example, you can restrict access to specific AWS Regions, or prevent your team from deleting common resources, such as an IAM role used by your central security team.You can get started by implementing example service control policies, such as preventing users from disabling key services. SCPs use the IAM policy language and enable you to establish controls that all IAM principals (users and roles) adhere to. You can restrict access to specific service actions, resources and based on specific condition to meet the access control needs of your organization. If necessary, you can define exceptions to your guardrails. For example, you can restrict service actions for all IAM entities in the account except for a specific administrator role.We recommend you avoid running workloads in your management account. The management account should be used to govern and deploy security guardrails that will affect member accounts. Some AWS services support the use of a delegated administrator account. When available, you should use this delegated account instead of the management account. You should strongly limit access to the Organizational administrator account.Using a multi-account strategy allows you to have greater flexibility in applying guardrails to your workloads. The AWS Security Reference Architecture gives prescriptive guidance on how to design your account structure. AWS services such as AWS Control Tower provide capabilities to centrally manage both preventative and detective controls across your organization. Define a clear purpose for each account or OU within your organization and limit controls in line with that purpose.Related documents:AWS OrganizationsService control policies (SCPs)Get more out of service control policies in a multi-account environmentAWS Security Reference Architecture (AWS SRA)Related videos:Enforce Preventive Guardrails using Service Control PoliciesBuilding governance at scale with AWS Control TowerAWS Identity and Access Management deep dive\n* Start with security and infrastructure in mind to enable your organization to set common guardrails as your workloads grow. This approach provides boundaries and controls between workloads. Account-level separation is strongly recommended for isolating production environments from development and test environments, or providing a strong logical boundary between workloads that process data of different sensitivity levels, as defined by external compliance requirements (such as PCI-DSS or HIPAA), and workloads that don\u2019t.Level of risk exposed if this best practice is not established: HighUse AWS Organizations: Use AWS Organizations to centrally enforce policy-based management for multiple AWS accounts.Getting started with AWS OrganizationsHow to use service control policies to set permission guardrails across accounts in your AWS OrganizationConsider AWS Control Tower: AWS Control Tower provides an easy way to set up and govern a new, secure, multi-account AWS environment based on best practices.AWS Control TowerRelated documents:IAM Best PracticesSecurity BulletinsAWS Security Audit GuidelinesRelated videos:Managing Multi-Account AWS Environments Using AWS OrganizationsSecurity Best Practices the Well-Architected WayUsing AWS Control Tower to Govern Multi-Account AWS Environments\n* Govern the consumption of shared resources across accounts or within your AWS Organizations. Monitor shared resources and review shared resource access.Common anti-patterns:Using the default IAM trust policy when granting third party cross-account access.Level of risk exposed if this best practice is not established: LowAs you manage your workloads using multiple AWS accounts, you may need to share resources between accounts. This will very often be cross-account sharing within an AWS Organizations. Several AWS services, such as AWS Security Hub, Amazon GuardDuty, and AWS Backup have cross-account features integrated with Organizations. You can use AWS Resource Access Manager to share other common resources, such as VPC Subnets or Transit Gateway attachments, AWS Network Firewall, or Amazon SageMaker pipelines. If you want to ensure that your account only shares resources within your Organizations, we recommend using Service Control Policies (SCPs) to prevent access to external principals.When sharing resources, you should put measures in place to protect against unintended access. We recommend combining identity-based controls and network controls to create a data perimeter for your organization. These controls should place strict limits on what resources can be shared and prevent sharing or exposing resources that should not be allowed. For example, as a part of your data perimeter you could use VPC endpoint policies and the aws:PrincipalOrgId condition to ensure the identities accessing your Amazon S3 buckets belong to your organization.In some cases, you may want to allow share resources outside of your Organizations or grant third parties access to your account. For example, a partner may provide a monitoring solution that needs to access resources within your account. In those cases, you should create an IAM cross-account role with only the privileges needed by the third party. You should also craft a trust policy using the external ID condition. When using an external ID, you should generate a unique ID for each third party. The unique ID should not be supplied by or controlled by the third party. If the third party no longer needs access to your environment, you should remove the role. You should also avoid providing long-term IAM credentials to a third-party in all cases. Maintain awareness of other AWS services which natively support sharing. For example, the AWS Well-Architected Tool allows sharing a workload with other AWS accounts.When using service such as Amazon S3, it is recommended to disable ACLs for your Amazon S3 bucket and use IAM policies to define access control. For restricting access to an Amazon S3 origin from Amazon CloudFront, migrate from origin access identity (OAI) to origin access control (OAC) which supports additional features including server-side encryption with AWS KMS.Related documents:Bucket owner granting cross-account permission to objects it does not ownHow to use Trust Policies with IAMBuilding Data Perimeter on AWSHow to use an external ID when granting a third party access to your AWS resourcesRelated videos:Granular Access with AWS Resource Access ManagerSecuring your data perimeter with VPC endpointsEstablishing a data perimeter on AWS\n\nKnowledge Area: Designing a security strategy for multiple AWS accounts (for example, AWS Control Tower, service control policies [SCPs])\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following best practices should be used to establish a secure multi-account strategy in AWS?\", \n\"answer_choices\": [\n    {\"answer\": \"Set up AWS Control Tower and apply service control policies across accounts\", \n    \"is_correct\": \"true\", \n    \"explanation\": \"AWS Control Tower provides an easy way to set up and govern a new, secure, multi-account AWS environment based on best practices. Service control policies allow you to set permission guardrails across accounts in your AWS Organization.\"},\n    {\"answer\": \"Run workloads in the Organizational administrator account\", \n    \"is_correct\": \"false\", \n    \"explanation\": \"You should avoid running workloads in your management account. The management account should be used to govern and deploy security guardrails that will affect member accounts. Some AWS services support the use of a delegated administrator account which should be used instead of the management account.\"},\n    {\"answer\": \"Use identity-based controls and network controls to create a data perimeter\", \n    \"is_correct\": \"true\", \n    \"explanation\": \"When sharing resources, you should put measures in place to protect against unintended access. We recommend combining identity-based controls and network controls to create a data perimeter for your organization.\"},\n    {\"answer\": \"Allow third party access to resources outside of organizations\", \n    \"is_correct\": \"false\", \n    \"explanation\": \"When allowing third party access to your account, you should create an IAM cross-account role with only the privileges needed by the third party and craft a trust policy using the external ID condition.\"}\n    ]}", "docs": ["('SEC03-BP05 Define permission guardrails for your organization - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_define_guardrails.html')", "('SEC01-BP01 Separate workloads using accounts - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_securely_operate_multi_accounts.html')", "('SEC03-BP08 Share resources securely - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_share_securely.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 1: Design secure access to AWS resources", "focus": "Skills in", "item": "Determining when to federate a directory service with IAM roles", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Require identities to dynamically acquire temporary credentials. For workforce identities, use AWS IAM Identity Center (successor to AWS Single Sign-On), or federation with AWS Identity and Access Management (IAM) roles to access AWS accounts. For machine identities, such as Amazon Elastic Compute Cloud(Amazon EC2) instances or AWS Lambda functions, require the use of IAM roles instead of IAM users with long-term access keys.For human identities using the AWS Management Console, require users to acquire temporary credentials and federate into AWS. You can do this using the AWS IAM Identity Center (successor to AWS Single Sign-On) user portal. For users requiring CLI access, ensure that they use AWS CLI v2, which supports direct integration with IAM Identity Center. Users can create CLI profiles that are linked to IAM Identity Center accounts and roles. The CLI automatically retrieves AWS credentials from IAM Identity Center and refreshes them on your behalf. This eliminates the need to copy and paste temporary AWS credentials from the IAM Identity Center console. For SDK, users should rely on AWS Security Token Service (AWS STS) to assume roles to receive temporary credentials. In certain cases, temporary credentials might not be practical. You should be aware of the risks of storing access keys, rotate these often, and require multi-factor authentication (MFA) as a condition when possible. Use last accessed information to determine when to rotate or remove access keys.For cases where you need to grant consumers access to your AWS resources, use Amazon Cognito identity pools and assign them a set of temporary, limited privilege credentials to access your AWS resources. The permissions for each user are controlled through IAM roles that you create. You can define rules to choose the role for each user based on claims in the user's ID token. You can define a default role for authenticated users. You can also define a separate IAM role with limited permissions for guest users who are not authenticated.For machine identities, you should rely on IAM roles to grant access to AWS. For Amazon Elastic Compute Cloud(Amazon EC2) instances, you can use roles for Amazon EC2. You can attach an IAM role to your Amazon EC2 instance to enable your applications running on Amazon EC2 to use temporary security credentials that AWS creates, distributes, and rotates automatically through the Instance Metadata Service (IMDS). The latest version of IMDS helps protect against vulnerabilities that expose the temporary credentials and should be implemented. For accessing Amazon EC2 instances using keys or passwords, AWS Systems Manager is a more secure way to access and manage your instances using a pre- installed agent without the stored secret. Additionally, other AWS services, such as AWS Lambda, enable you to configure an IAM service role to grant the service permissions to perform AWS actions using temporary credentials. In situations where you cannot use temporary credentials, use programmatic tools, such as AWS Secrets Manager, to automate credential rotation and management.Audit and rotate credentials periodically: Periodic validation, preferably through an automated tool, is necessary to verify that the correct controls are enforced. For human identities, you should require users to change their passwords periodically and retire access keys in favor of temporary credentials. As you are moving from IAM users to centralized identities, you can generate a credential report to audit your IAM users. We also recommend that you enforce MFA settings in your identity provider. You can set up AWS Config Rules to monitor these settings. For machine identities, you should rely on temporary credentials using IAM roles. For situations where this is not possible, frequent auditing and rotating access keys is necessary.Store and use secrets securely: For credentials that are not IAM-related and cannot take advantage of temporary credentials, such as database logins, use a service that is designed to handle management of secrets, such as Secrets Manager. Secrets Manager makes it easy to manage, rotate, and securely store encrypted secrets using supported services. Calls to access the secrets are logged in AWS CloudTrail for auditing purposes, and IAM permissions can grant least-privilege access to them.Level of risk exposed if this best practice is not established: HighImplement least privilege policies: Assign access policies with least privilege to IAM groups and roles to reflect the user's role or function that you have defined.Grant least privilegeRemove unnecessary permissions: Implement least privilege by removing permissions that are unnecessary.Reducing policy scope by viewing user activityView role accessConsider permissions boundaries: A permissions boundary is an advanced feature for using a managed policy that sets the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.Lab: IAM permissions boundaries delegating role creationConsider resource tags for permissions: You can use tags to control access to your AWS resources that support tagging. You can also tag IAM users and roles to control what they can access.Lab: IAM tag based access control for EC2Attribute-based access control (ABAC)Related documents:Getting Started with AWS Secrets ManagerIAM Best PracticesIdentity Providers and FederationSecurity Partner Solutions: Access and Access ControlTemporary Security CredentialsThe AWS Account Root UserRelated videos:Best Practices for Managing, Retrieving, and Rotating Secrets at ScaleManaging user permissions at scale with AWS IAM Identity Center (successor to AWS Single Sign-On)Mastering identity at every layer of the cake\n\nKnowledge Area: Determining when to federate a directory service with IAM roles\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best practice for granting consumers access to a healthcare company's AWS resources, while storing and processing sensitive patient information in the cloud?\",\n\"answer_choices\": [{\"answer\": \"Federate with AWS IAM roles to access AWS accounts\",\"is_correct\": \"false\",\"explanation\": \"Federation with AWS IAM roles should be used for workforce identities, not for granting consumers access to AWS resources.\"},{\"answer\": \"Use Amazon Cognito identity pools and assign them a set of temporary, limited privilege credentials to access AWS resources\",\"is_correct\": \"true\",\"explanation\": \"Using Amazon Cognito identity pools and assigning them a set of temporary, limited privilege credentials to access AWS resources is the best practice for granting consumers access to a healthcare company's AWS resources while storing and processing sensitive patient information in the cloud.\"},{\"answer\": \"Grant consumers access to AWS resources using IAM users with long-term access keys\",\"is_correct\": \"false\",\"explanation\": \"IAM users with long-term access keys should not be used for granting consumers access to AWS resources, as this increases the risk for vulnerabilities.\"},{\"answer\": \"Grant consumers access to AWS resources using AWS Single Sign-On\",\"is_correct\": \"false\",\"explanation\": \"AWS Single Sign-On is the predecessor to AWS IAM Identity Center, and should not be used for granting consumers access to AWS resources.\"}]}", "docs": ["('SEC02-BP02 Use temporary credentials - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_identities_unique.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Knowledge of", "item": "Application configuration and credentials security", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* Establish secure baselines and templates for security mechanisms that are tested and validated as part of your build, pipelines, and processes. Use tools and automation to test and validate all security controls continuously. For example, scan items such as machine images and infrastructure-as-code templates for security vulnerabilities, irregularities, and drift from an established baseline at each stage. AWS CloudFormation Guard can help you verify that CloudFormation templates are safe, save you time, and reduce the risk of configuration error.Reducing the number of security misconfigurations introduced into a production environment is critical\u2014the more quality control and reduction of defects you can perform in the build process, the better. Design continuous integration and continuous deployment (CI/CD) pipelines to test for security issues whenever possible. CI/CD pipelines offer the opportunity to enhance security at each stage of build and delivery. CI/CD security tooling must also be kept updated to mitigate evolving threats.Track changes to your workload configuration to help with compliance auditing, change management, and investigations that may apply to you. You can use AWS Config to record and evaluate your AWS and third-party resources. It allows you to continuously audit and assess the overall compliance with rules and conformance packs, which are collections of rules with remediation actions.Change tracking should include planned changes, which are part of your organization\u2019s change control process (sometimes referred to as MACD\u2014Move, Add, Change, Delete), unplanned changes, and unexpected changes, such as incidents. Changes might occur on the infrastructure, but they might also be related to other categories, such as changes in code repositories, machine images and application inventory changes, process and policy changes, or documentation changes.Level of risk exposed if this best practice is not established: MediumAutomate configuration management: Enforce and validate secure configurations automatically by using a configuration management service or tool.AWS Systems ManagerAWS CloudFormationSet Up a CI/CD Pipeline on AWSRelated documents:How to use service control policies to set permission guardrails across accounts in your AWS OrganizationRelated videos:Managing Multi-Account AWS Environments Using AWS OrganizationsSecurity Best Practices the Well-Architected Way\n* There are seven design principles for security in the cloud:Implement a strong identity foundation: Implement the principle of least privilege and enforce separation of duties with appropriate authorization for each interaction with your AWS resources. Centralize identity management, and aim to eliminate reliance on long-term static credentials.Enable traceability: Monitor, alert, and audit actions and changes to your environment in real time. Integrate log and metric collection with systems to automatically investigate and take action.Apply security at all layers: Apply a defense in depth approach with multiple security controls. Apply to all layers (for example, edge of network, VPC, load balancing, every instance and compute service, operating system, application, and code).Automate security best practices: Automated software-based security mechanisms improve your ability to securely scale more rapidly and cost-effectively. Create secure architectures, including the implementation of controls that are defined and managed as code in version-controlled templates.Protect data in transit and at rest: Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate.Keep people away from data: Use mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data. This reduces the risk of mishandling or modification and human error when handling sensitive data.Prepare for security events: Prepare for an incident by having incident management and investigation policy and processes that align to your organizational requirements. Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery.\n* Each component or resource of your workload needs to be accessed by administrators, end users, or other components. Have a clear definition of who or what should have access to each component, choose the appropriate identity type and method of authentication and authorization.Common anti-patterns:Hard-coding or storing secrets in your application.Granting custom permissions for each user.Using long-lived credentials.Level of risk exposed if this best practice is not established: HighEach component or resource of your workload needs to be accessed by administrators, end users, or other components. Have a clear definition of who or what should have access to each component, choose the appropriate identity type and method of authentication and authorization.Regular access to AWS accounts within the organization should be provided using federated access or a centralized identity provider. You should also centralize your identity management and ensure that there is an established practice to integrate AWS access to your employee access lifecycle. For example, when an employee changes to a job role with a different access level, their group membership should also change to reflect their new access requirements.When defining access requirements for non-human identities, determine which applications and components need access and how permissions are granted. Using IAM roles built with the least privilege access model is a recommended approach. AWS Managed policies provide predefined IAM policies that cover most common use cases.AWS services, such as AWS Secrets Manager and AWS Systems Manager Parameter Store, can help decouple secrets from the application or workload securely in cases where it's not feasible to use IAM roles. In Secrets Manager, you can establish automatic rotation for your credentials. You can use Systems Manager to reference parameters in your scripts, commands, SSM documents, configuration, and automation workflows by using the unique name that you specified when you created the parameter.You can use AWS Identity and Access Management Roles Anywhere to obtain temporary security credentials in IAM for workloads that run outside of AWS. Your workloads can use the same IAM policies and IAM roles that you use with AWS applications to access AWS resources.Where possible, prefer short-term temporary credentials over long-term static credentials. For scenarios in which you need IAM users with programmatic access and long-term credentials, use access key last used information to rotate and remove access keys.Related documents:Attribute-based access control (ABAC)AWS IAM Identity Center (successor to AWS Single Sign-On)IAM Roles AnywhereAWS Managed policies for IAM Identity CenterAWS IAM policy conditionsIAM use casesRemove unnecessary credentialsWorking with PoliciesHow to control access to AWS resources based on AWS account, OU, or organizationIdentify, arrange, and manage secrets easily using enhanced search in AWS Secrets ManagerRelated videos:Become an IAM Policy Master in 60 Minutes or LessSeparation of Duties, Least Privilege, Delegation, and CI/CDStreamlining identity and access management for innovation\n\nKnowledge Area: Application configuration and credentials security\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best approach to secure configuration and credentials for the manufacturing company's workload in the AWS environment?\", \"answer_choices\": [{\"answer\": \"Use AWS Managed Policies to provide predefined IAM policies for access to each component\",\"is_correct\": \"True\",\"explanation\": \"AWS Managed policies provide predefined IAM policies that cover most common use cases and can help manage access to each component of the workload.\"},{\"answer\": \"Hard-code or store secrets in the application\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern and should be avoided as it exposes the workload to high levels of risk.\"},{\"answer\": \"Use long-term, static credentials\",\"is_correct\": \"False\", \"explanation\": \"This is an anti-pattern and should be avoided as it exposes the workload to high levels of risk.\"},{\"answer\": \"Use IAM roles with least privilege access\",\"is_correct\": \"True\",\"explanation\": \"Using IAM roles with least privilege access can help ensure that only the necessary components and resources of the workload have access and can prevent unnecessary access to the workload.\"}]}", "docs": ["('SEC01-BP06 Automate testing and validation of security controls in pipelines - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_securely_operate_test_validate_pipeline.html')", "('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-design.html')", "('SEC03-BP01 Define access requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_define.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Knowledge of", "item": "AWS service endpoints", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies.Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public endpoints. You might also choose to evaluate AWS Marketplace software appliances for load balancing and proxying.Consumers of the service your workload provides, whether they are end-users or other services, make requests on these service endpoints. Several AWS resources are available to enable you to provide highly available endpoints.Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.Distributed Denial of Service (DDoS) attacks risk shutting out legitimate traffic and lowering availability for your users. AWS Shield provides automatic protection against these attacks at no extra cost for AWS service endpoints on your workload. You can augment these features with virtual appliances from APN Partners and the AWS Marketplace to meet your needs.Common anti-patterns:Using public internet addresses on instances or containers and managing the connectivity to them via DNS.Using Internet Protocol addresses instead of domain names for locating services.Providing content (web pages, static assets, media files) to a large geographic area and not using a content delivery network.Benefits of establishing this best practice: By implementing highly available services in your workload, you know that your workload will be available to your users.Level of risk exposed if this best practice is not established: HighEnsure that you have highly available connectivity for users of the workload Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public facing endpoints. You may also choose to evaluate AWS Marketplace software appliances for load-balancing and proxying.Ensure that you have a highly available connection to your users.Ensure that you are using a highly available DNS to manage the domain names of your application endpoints.If your users access your application via the internet, use service API operations to confirm the correct usage of Internet Gateways. Also confirm that the route tables entries for the subnets hosting your application endpoints are correct.DescribeInternetGatewaysDescribeRouteTablesEnsure that you are using a highly available reverse proxy or load balancer in front of your application.If your users access your application via your on-premises environment, ensure that your connectivity between AWS and your on-premises environment is highly available.Use Route 53 to manage your domain names.What is Amazon Route 53?Use a third-party DNS provider that meets your requirements.Use Elastic Load Balancing.What is Elastic Load Balancing?Use an AWS Marketplace appliance that meets your requirements.Related documents:APN Partner: partners that can help plan your networkingAWS Direct Connect Resiliency RecommendationsAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivityUsing the Direct Connect Resiliency Toolkit to get startedVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is AWS Global Accelerator?What Is Amazon VPC?What Is a Transit Gateway?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?Working with Direct Connect GatewaysRelated videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n* The workload has a recovery time objective (RTO) and recovery point objective (RPO).\n* Define a disaster recovery (DR) strategy that meets your workload's recovery objectives. Choose a strategy such as: backup and restore; standby (active/passive); or active/active.\n* AWS Architecture CenterAWS Cloud ComplianceAWS Well-Architected Partner programAWS Well-Architected ToolAWS Well-Architected homepageOperational Excellence Pillar whitepaperSecurity Pillar whitepaperReliability Pillar whitepaperPerformance Efficiency Pillar whitepaperCost Optimization Pillar whitepaperSustainability Pillar whitepaperThe Amazon Builders' Library\n\nKnowledge Area: AWS service endpoints\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"A company wants to store log files from their applications for analysis and compliance purposes. What AWS service provides highly available public facing endpoints?\",\n\"answer_choices\": [{\"answer\": \"Amazon Route 53\",\"is_correct\": \"True\",\"explanation\": \"Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.\"},{\"answer\": \"AWS Global Accelerator\",\"is_correct\": \"True\",\"explanation\": \"AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.\"},{\"answer\": \"Amazon CloudFront\",\"is_correct\": \"True\",\"explanation\": \"Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users.\"},{\"answer\": \"Amazon API Gateway\",\"is_correct\": \"True\",\"explanation\": \"Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.\"}]}", "docs": ["('REL02-BP01 Use highly available network connectivity for your workload public endpoints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ha_conn_users.html')", "('REL13-BP01 Define recovery objectives for downtime and data loss - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_objective_defined_recovery.html')", "('REL13-BP02 Use defined recovery strategies to meet the recovery objectives - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_disaster_recovery.html')", "('Further reading - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/further-reading.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Knowledge of", "item": "Control ports, protocols, and network traffic on AWS", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* Inspect and filter your traffic at each layer. You can inspect your VPC configurations for potential unintended access using VPC Network Access Analyzer. You can specify your network access requirements and identify potential network paths that do not meet them. For components transacting over HTTP-based protocols, a web application firewall can help protect from common attacks. AWS WAF is a web application firewall that lets you monitor and block HTTP(s) requests that match your configurable rules that are forwarded to an Amazon API Gateway API, Amazon CloudFront, or an Application Load Balancer. To get started with AWS WAF, you can use AWS Managed Rules in combination with your own, or use existing partner integrations.For managing AWS WAF, AWS Shield Advanced protections, and Amazon VPC security groups across AWS Organizations, you can use AWS Firewall Manager. It allows you to centrally configure and manage firewall rules across your accounts and applications, making it easier to scale enforcement of common rules. It also enables you to rapidly respond to attacks, using AWS Shield Advanced, or solutions that can automatically block unwanted requests to your web applications. Firewall Manager also works with AWS Network Firewall. AWS Network Firewall is a managed service that uses a rules engine to give you fine-grained control over both stateful and stateless network traffic. It supports the Suricata compatible open source intrusion prevention system (IPS) specifications for rules to help protect your workload.Level of risk exposed if this best practice is not established: LowConfigure Amazon GuardDuty: GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. Enable GuardDuty and configure automated alerts.Amazon GuardDutyLab: Automated Deployment of Detective ControlsConfigure virtual private cloud (VPC) Flow Logs: VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs and Amazon Simple Storage Service (Amazon S3). After you've created a flow log, you can retrieve and view its data in the chosen destination.Consider VPC traffic mirroring: Traffic mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of Amazon Elastic Compute Cloud (Amazon EC2) instances and then send it to out-of-band security and monitoring appliances for content inspection, threat monitoring, and troubleshooting.VPC traffic mirroringRelated documents:AWS Firewall ManagerAmazon InspectorAmazon VPC SecurityGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n* When architecting your network topology, you should examine the connectivity requirements of each component. For example, if a component requires internet accessibility (inbound and outbound), connectivity to VPCs, edge services, and external data centers.A VPC allows you to define your network topology that spans an AWS Region with a private IPv4 address range that you set, or an IPv6 address range AWS selects. You should apply multiple controls with a defense in depth approach for both inbound and outbound traffic, including the use of security groups (stateful inspection firewall), Network ACLs, subnets, and route tables. Within a VPC, you can create subnets in an Availability Zone. Each subnet can have an associated route table that defines routing rules for managing the paths that traffic takes within the subnet. You can define an internet routable subnet by having a route that goes to an internet or NAT gateway attached to the VPC, or through another VPC.When an instance, Amazon Relational Database Service(Amazon RDS) database, or other service is launched within a VPC, it has its own security group per network interface. This firewall is outside the operating system layer and can be used to define rules for allowed inbound and outbound traffic. You can also define relationships between security groups. For example, instances within a database tier security group only accept traffic from instances within the application tier, by reference to the security groups applied to the instances involved. Unless you are using non-TCP protocols, it shouldn\u2019t be necessary to have an Amazon Elastic Compute Cloud(Amazon EC2) instance directly accessible by the internet (even with ports restricted by security groups) without a load balancer, or CloudFront. This helps protect it from unintended access through an operating system or application issue. A subnet can also have a network ACL attached to it, which acts as a stateless firewall. You should configure the network ACL to narrow the scope of traffic allowed between layers, note that you need to define both inbound and outbound rules.Some AWS services require components to access the internet for making API calls, where AWS API endpoints are located. Other AWS services use VPC endpoints within your Amazon VPCs. Many AWS services, including Amazon S3 and Amazon DynamoDB, support VPC endpoints, and this technology has been generalized in AWS PrivateLink. We recommend you use this approach to access AWS services, third-party services, and your own services hosted in other VPCs securely. All network traffic on AWS PrivateLink stays on the global AWS backbone and never traverses the internet. Connectivity can only be initiated by the consumer of the service, and not by the provider of the service. Using AWS PrivateLink for external service access allows you to create air-gapped VPCs with no internet access and helps protect your VPCs from external threat vectors. Third-party services can use AWS PrivateLink to allow their customers to connect to the services from their VPCs over private IP addresses. For VPC assets that need to make outbound connections to the internet, these can be made outbound only (one-way) through an AWS managed NAT gateway, outbound only internet gateway, or web proxies that you create and manage.Level of risk exposed if this best practice is not established: HighControl network traffic in a VPC: Implement VPC best practices to control traffic.Amazon VPC securityVPC endpointsAmazon VPC security groupNetwork ACLsControl traffic at the edge: Implement edge services, such as Amazon CloudFront, to provide an additional layer of protection and other features.Amazon CloudFront use casesAWS Global AcceleratorAWS Web Application Firewall (AWS WAF)Amazon Route 53Amazon VPC Ingress RoutingControl private network traffic: Implement services that protect your private traffic for your workload.Amazon VPC PeeringAmazon VPC Endpoint Services (AWS PrivateLink)Amazon VPC Transit GatewayAWS Direct ConnectAWS Site-to-Site VPNAWS Client VPNAmazon S3 Access PointsRelated documents:AWS Firewall ManagerAmazon InspectorGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n\nKnowledge Area: Control ports, protocols, and network traffic on AWS\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following best practices should you implement to control network traffic in a VPC?\", \n\"answer_choices\": [{\"answer\": \"Configure Amazon GuardDuty\",\"is_correct\": \"False\", \"explanation\": \"Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.\"},{\"answer\": \"Implement VPC endpoints\",\"is_correct\": \"True\", \"explanation\": \"VPC endpoints allow components to access AWS services without traversing the internet or leaving the Amazon VPC. You should use this approach to access AWS services, third-party services, and your own services hosted in other VPCs securely.\"}, {\"answer\": \"Configure AWS Firewall Manager\",\"is_correct\": \"False\", \"explanation\": \"AWS Firewall Manager allows you to centrally configure and manage firewall rules across your accounts and applications, making it easier to scale enforcement of common rules. It does not help to control network traffic.\"},{\"answer\": \"Configure virtual private cloud (VPC) Flow Logs\",\"is_correct\": \"True\", \"explanation\": \"VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs and Amazon Simple Storage Service (Amazon S3) and can be used to control network traffic.\"}]}", "docs": ["('SEC05-BP04 Implement inspection and protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_inspection.html')", "('SEC05-BP02 Control traffic at all layers - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_layered.html')"]}, {"scenario": "A research institution wants to perform data analysis on a large dataset using AWS compute resources.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Knowledge of", "item": "Secure application access", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA research institution wants to perform data analysis on a large dataset using AWS compute resources.`\"\n\nContext:\n\n* Keep all users away from directly accessing sensitive data and systems under normal operational circumstances. For example, use a change management workflow to manage Amazon Elastic Compute Cloud (Amazon EC2) instances using tools instead of allowing direct access or a bastion host. This can be achieved using AWS Systems Manager Automation, which uses automation documents that contain steps you use to perform tasks. These documents can be stored in source control, be peer reviewed before running, and tested thoroughly to minimize risk compared to shell access. Business users could have a dashboard instead of direct access to a data store to run queries. Where CI/CD pipelines are not used, determine which controls and processes are required to adequately provide a normally disabled break-glass access mechanism.Level of risk exposed if this best practice is not established: LowImplement mechanisms to keep people away from data: Mechanisms include using dashboards, such as Amazon QuickSight, to display data to users instead of directly querying.Amazon QuickSightAutomate configuration management: Perform actions at a distance, enforce and validate secure configurations automatically by using a configuration management service or tool. Avoid use of bastion hosts or directly accessing EC2 instances.AWS Systems ManagerAWS CloudFormationCI/CD Pipeline for AWS CloudFormation templates on AWSRelated documents:AWS KMS Cryptographic Details WhitepaperRelated videos:How Encryption Works in AWSSecuring Your Block Storage on AWS\n* Protect your data in transit by implementing multiple controls to reduce the risk of unauthorized access or loss.Best practices\n* Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access or mishandling.Best practices\n* Enforce access control with least privileges and mechanisms, including backups, isolation, and versioning, to help protect your data at rest. Prevent operators from granting public access to your data.Different controls including access (using least privilege), backups (see Reliability whitepaper), isolation, and versioning can all help protect your data at rest. Access to your data should be audited using detective mechanisms covered earlier in this paper including CloudTrail, and service level log, such as Amazon Simple Storage Service (Amazon S3) access logs. You should inventory what data is publicly accessible, and plan for how you can reduce the amount of data available over time. Amazon S3 Glacier Vault Lock and Amazon S3 Object Lock are capabilities providing mandatory access control\u2014once a vault policy is locked with the compliance option, not even the root user can change it until the lock expires. The mechanism meets the Books and Records Management requirements of the SEC, CFTC, and FINRA. For more details, see this whitepaper.Level of risk exposed if this best practice is not established: LowEnforce access control: Enforce access control with least privileges, including access to encryption keys.Introduction to Managing Access Permissions to Your Amazon S3 ResourcesSeparate data based on different classification levels: Use different AWS accounts for data classification levels managed by AWS Organizations.AWS OrganizationsReview AWS KMS policies: Review the level of access granted in AWS KMS policies.Overview of managing access to your AWS KMS resourcesReview Amazon S3 bucket and object permissions: Regularly review the level of access granted in Amazon S3 bucket policies. Best practice is to not have publicly readable or writeable buckets. Consider using AWS Config to detect buckets that are publicly available, and Amazon CloudFront to serve content from Amazon S3.AWS Config RulesAmazon S3 + Amazon CloudFront: A Match Made in the CloudEnable Amazon S3 versioning and object lock.Using versioningLocking Objects Using Amazon S3 Object LockUse Amazon S3 Inventory: Amazon S3 inventory is one of the tools you can use to audit and report on the replication and encryption status of your objects.Amazon S3 InventoryReview Amazon EBS and AMI sharing permissions: Sharing permissions can allow images and volumes to be shared to AWS accounts external to your workload.Sharing an Amazon EBS SnapshotShared AMIsRelated documents:AWS KMS Cryptographic Details WhitepaperRelated videos:Securing Your Block Storage on AWS\n* There are seven design principles for security in the cloud:Implement a strong identity foundation: Implement the principle of least privilege and enforce separation of duties with appropriate authorization for each interaction with your AWS resources. Centralize identity management, and aim to eliminate reliance on long-term static credentials.Enable traceability: Monitor, alert, and audit actions and changes to your environment in real time. Integrate log and metric collection with systems to automatically investigate and take action.Apply security at all layers: Apply a defense in depth approach with multiple security controls. Apply to all layers (for example, edge of network, VPC, load balancing, every instance and compute service, operating system, application, and code).Automate security best practices: Automated software-based security mechanisms improve your ability to securely scale more rapidly and cost-effectively. Create secure architectures, including the implementation of controls that are defined and managed as code in version-controlled templates.Protect data in transit and at rest: Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate.Keep people away from data: Use mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data. This reduces the risk of mishandling or modification and human error when handling sensitive data.Prepare for security events: Prepare for an incident by having incident management and investigation policy and processes that align to your organizational requirements. Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery.\n\nKnowledge Area: Secure application access\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best practice for keeping users away from directly accessing sensitive data and systems in this scenario?\", \n\"answer_choices\": [\n{\"answer\": \"Use a change management workflow to manage Amazon Elastic Compute Cloud (Amazon EC2) instances using tools instead of allowing direct access or a bastion host\",\"is_correct\": \"true\",\"explanation\": \"This can be achieved using AWS Systems Manager Automation, which uses automation documents that contain steps you use to perform tasks.\"},\n{\"answer\": \"Allow users to directly access and query data stores to run their queries\",\"is_correct\": \"false\",\"explanation\": \"This should be avoided, as it exposes the data to potential unauthorized access. Dashboards, such as Amazon QuickSight, should be used to display data to users instead.\"},\n{\"answer\": \"Enable Amazon S3 versioning and object lock\",\"is_correct\": \"false\",\"explanation\": \"This is a good practice for protecting data at rest, but it does not keep users away from directly accessing data.\"},\n{\"answer\": \"Use different AWS accounts for data classification levels managed by AWS Organizations\",\"is_correct\": \"false\",\"explanation\": \"Using different AWS accounts is a good practice for separating data based on different classification levels, but it does not keep users away from directly accessing data. A change management workflow should be used to manage Amazon EC2 instances instead.\"}\n]}", "docs": ["('SEC08-BP05 Use mechanisms to keep people away from data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_use_people_away.html')", "('SEC 9\\xa0 How do you protect your data in transit? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-09.html')", "('SEC 8\\xa0 How do you protect your data at rest? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-08.html')", "('SEC08-BP04 Enforce access control - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_access_control.html')", "('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-design.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Knowledge of", "item": "Security services with appropriate use cases (for example, Amazon Cognito, Amazon GuardDuty, Amazon Macie)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* Use tools such as Amazon GuardDuty to automatically detect suspicious activity or attempts to move data outside of defined boundaries. For example, GuardDuty can detect Amazon Simple Storage Service (Amazon S3) read activity that is unusual with the Exfiltration:S3/AnomalousBehavior finding. In addition to GuardDuty, Amazon VPC Flow Logs, which capture network traffic information, can be used with Amazon EventBridge to trigger detection of abnormal connections\u2013both successful and denied. Amazon S3 Access Analyzer can help assess what data is accessible to who in your Amazon S3 buckets.Level of risk exposed if this best practice is not established: MediumAutomate detection of unintended data access: Use a tool or detection mechanism to automatically detect attempts to move data outside of defined boundaries, for example, to detect a database system that is copying data to an unrecognized host.VPC Flow LogsConsider Amazon Macie: Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.Amazon MacieRelated documents:VPC Flow LogsAmazon Macie\n* Automating the identification and classification of data can help you implement the correct controls. Using automation for this instead of direct access from a person reduces the risk of human error and exposure. You should evaluate using a tool, such as Amazon Macie, that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Amazon Macie recognizes sensitive data, such as personally identifiable information (PII) or intellectual property, and provides you with dashboards and alerts that give visibility into how this data is being accessed or moved.Level of risk exposed if this best practice is not established: MediumUse Amazon Simple Storage Service (Amazon S3) Inventory: Amazon S3 inventory is one of the tools you can use to audit and report on the replication and encryption status of your objects.Amazon S3 InventoryConsider Amazon Macie: Amazon Macie uses machine learning to automatically discover and classify data stored in Amazon S3.Amazon MacieRelated documents:Amazon MacieAmazon S3 InventoryData Classification WhitepaperGetting started with Amazon MacieRelated videos:Introducing the New Amazon Macie\n* Ensure that security personnel have the right tools pre-deployed into AWS to reduce the time for investigation through to recovery.To automate security engineering and operations functions, you can use a comprehensive set of APIs and tools from AWS. You can fully automate identity management, network security, data protection, and monitoring capabilities and deliver them using popular software development methods that you already have in place. When you build security automation, your system can monitor, review, and initiate a response, rather than having people monitor your security position and manually react to events. An effective way to automatically provide searchable and relevant log data across AWS services to your incident responders is to enable Amazon Detective.If your incident response teams continue to respond to alerts in the same way, they risk alert fatigue. Over time, the team can become desensitized to alerts and can either make mistakes handling ordinary situations or miss unusual alerts. Automation helps avoid alert fatigue by using functions that process the repetitive and ordinary alerts, leaving humans to handle the sensitive and unique incidents. Integrating anomaly detection systems, such as Amazon GuardDuty, AWS CloudTrail Insights, and Amazon CloudWatch Anomaly Detection, can reduce the burden of common threshold-based alerts.You can improve manual processes by programmatically automating steps in the process. After you define the remediation pattern to an event, you can decompose that pattern into actionable logic, and write the code to perform that logic. Responders can then execute that code to remediate the issue. Over time, you can automate more and more steps, and ultimately automatically handle whole classes of common incidents.For tools that execute within the operating system of your Amazon Elastic Compute Cloud (Amazon EC2) instance, you should evaluate using the AWS Systems Manager Run Command, which enables you to remotely and securely administrate instances using an agent that you install on your Amazon EC2 instance operating system. It requires the Systems Manager Agent (SSM Agent), which is installed by default on many Amazon Machine Images (AMIs). Be aware, though, that once an instance has been compromised, no responses from tools or agents running on it should be considered trustworthy.Level of risk exposed if this best practice is not established: LowPre-deploy tools: Ensure that security personnel have the right tools pre-deployed in AWS so that an appropriate response can be made to an incident.Lab: Incident response with AWS Management Console and CLIIncident Response Playbook with Jupyter - AWS IAMAWS Security AutomationImplement resource tagging: Tag resources with information, such as a code for the resource under investigation, so that you can identify resources during an incident.AWS Tagging StrategiesRelated documents:AWS Incident Response GuideRelated videos:DIY guide to runbooks, incident reports, and incident response\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n\nKnowledge Area: Security services with appropriate use cases (for example, Amazon Cognito, Amazon GuardDuty, Amazon Macie)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which service should the transportation company use to detect attempts to move data outside of defined boundaries in AWS?\", \n\"answer_choices\": [\n    {\"answer\": \"Amazon S3 Access Analyzer\", \"is_correct\": \"false\", \"explanation\": \"Amazon S3 Access Analyzer is used to assess what data is accessible to who in your Amazon S3 buckets.\"},\n    {\"answer\": \"Amazon Macie\", \"is_correct\": \"false\", \"explanation\": \"Amazon Macie is used to discover and protect sensitive data in AWS.\"},\n    {\"answer\": \"Amazon VPC Flow Logs\", \"is_correct\": \"true\", \"explanation\": \"VPC Flow Logs can be used with Amazon EventBridge to detect abnormal connections, both successful and denied.\"},\n    {\"answer\": \"Amazon GuardDuty\", \"is_correct\": \"false\", \"explanation\": \"Amazon GuardDuty can detect Amazon Simple Storage Service (Amazon S3) read activity that is unusual with the Exfiltration:S3/AnomalousBehavior finding.\"}\n]}", "docs": ["('SEC09-BP03 Automate detection of unintended data access - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_transit_auto_unintended_access.html')", "('SEC07-BP03 Automate identification and classification - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_data_classification_auto_classification.html')", "('SEC10-BP06 Pre-deploy tools - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_incident_response_pre_deploy_tools.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')"]}, {"scenario": "A telecommunications company wants to use AWS to host and manage their network infrastructure.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Knowledge of", "item": "Threat vectors external to AWS (for example, DDoS, SQL injection)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA telecommunications company wants to use AWS to host and manage their network infrastructure.`\"\n\nContext:\n\n* To help you define and implement appropriate controls, recognize attack vectors by staying up to date with the latest security threats. Consume AWS Managed Services to make it easier to receive notification of unexpected or unusual behavior in your AWS accounts. Investigate using AWS Partner tools or third-party threat information feeds as part of your security information flow. The Common Vulnerabilities and Exposures (CVE) List list contains publicly disclosed cyber security vulnerabilities that you can use to stay up to date.Level of risk exposed if this best practice is not established: HighSubscribe to threat intelligence sources: Regularly review threat intelligence information from multiple sources that are relevant to the technologies used in your workload.Common Vulnerabilities and Exposures ListConsider AWS Shield Advanced service: It provides near real-time visibility into intelligence sources, if your workload is internet accessible.Related documents:AWS Security Audit GuidelinesAWS ShieldSecurity BulletinsRelated videos:Security Best Practices the Well-Architected Way\n* Compute resources in your workload require multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more.Best practices\n* Keep all users away from directly accessing sensitive data and systems under normal operational circumstances. For example, use a change management workflow to manage Amazon Elastic Compute Cloud (Amazon EC2) instances using tools instead of allowing direct access or a bastion host. This can be achieved using AWS Systems Manager Automation, which uses automation documents that contain steps you use to perform tasks. These documents can be stored in source control, be peer reviewed before running, and tested thoroughly to minimize risk compared to shell access. Business users could have a dashboard instead of direct access to a data store to run queries. Where CI/CD pipelines are not used, determine which controls and processes are required to adequately provide a normally disabled break-glass access mechanism.Level of risk exposed if this best practice is not established: LowImplement mechanisms to keep people away from data: Mechanisms include using dashboards, such as Amazon QuickSight, to display data to users instead of directly querying.Amazon QuickSightAutomate configuration management: Perform actions at a distance, enforce and validate secure configurations automatically by using a configuration management service or tool. Avoid use of bastion hosts or directly accessing EC2 instances.AWS Systems ManagerAWS CloudFormationCI/CD Pipeline for AWS CloudFormation templates on AWSRelated documents:AWS KMS Cryptographic Details WhitepaperRelated videos:How Encryption Works in AWSSecuring Your Block Storage on AWS\n* Use tools such as Amazon GuardDuty to automatically detect suspicious activity or attempts to move data outside of defined boundaries. For example, GuardDuty can detect Amazon Simple Storage Service (Amazon S3) read activity that is unusual with the Exfiltration:S3/AnomalousBehavior finding. In addition to GuardDuty, Amazon VPC Flow Logs, which capture network traffic information, can be used with Amazon EventBridge to trigger detection of abnormal connections\u2013both successful and denied. Amazon S3 Access Analyzer can help assess what data is accessible to who in your Amazon S3 buckets.Level of risk exposed if this best practice is not established: MediumAutomate detection of unintended data access: Use a tool or detection mechanism to automatically detect attempts to move data outside of defined boundaries, for example, to detect a database system that is copying data to an unrecognized host.VPC Flow LogsConsider Amazon Macie: Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.Amazon MacieRelated documents:VPC Flow LogsAmazon Macie\n* Infrastructure protection encompasses control methodologies, such as defense in depth, necessary to meet best practices and organizational or regulatory obligations. Use of these methodologies is critical for successful, ongoing operations in either the cloud or on-premises.In AWS, you can implement stateful and stateless packet inspection, either by using AWS-native technologies or by using partner products and services available through the AWS Marketplace. You should use Amazon Virtual Private Cloud (Amazon VPC) to create a private, secured, and scalable environment in which you can define your topology\u2014including gateways, routing tables, and public and private subnets.The following questions focus on these considerations for security.Multiple layers of defense are advisable in any type of environment. In the case of infrastructure protection, many of the concepts and methods are valid across cloud and on-premises models. Enforcing boundary protection, monitoring points of ingress and egress, and comprehensive logging, monitoring, and alerting are all essential to an effective information security plan.AWS customers are able to tailor, or harden, the configuration of an Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS) container, or AWS Elastic Beanstalk instance, and persist this configuration to an immutable Amazon Machine Image (AMI). Then, whether triggered by Auto Scaling or launched manually, all new virtual servers (instances) launched with this AMI receive the hardened configuration.SEC 5: How do you protect your network resources?Any workload that has some form of network connectivity, whether it\u2019s the internet or a private network, requires multiple layers of defense to help protect from external and internal network-based threats.SEC 6: How do you protect your compute resources?Compute resources in your workload require multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more.\n* Design and configure your workload to emit information about the status (for example, reachability or response time) of resources it depends on. Examples of external dependencies can include, external databases, DNS, and network connectivity. Use this information to determine when a response is required.Common anti-patterns:You are unable to determine if the reason your application is unreachable is a DNS issue without manually performing a check to see if your DNS provider is working.Your shopping cart application is unable to complete transactions. You are unable to determine if it's a problem with your credit card processing provider without contacting them to verify.Benefits of establishing this best practice: Understanding the health of your dependencies enables you to respond if necessary.Level of risk exposed if this best practice is not established: MediumImplement dependency telemetry: Design and configure your workload to emit information about the state and status of systems it depends on. Some examples include: external databases, DNS, network connectivity, and external credit card processing services.Amazon CloudWatch Agent with AWS Systems Manager integration - unified metrics & log collection for Linux & WindowsCollect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch AgentRelated documents:Amazon CloudWatch Agent with AWS Systems Manager integration - unified metrics & log collection for Linux & WindowsCollect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch AgentRelated examples:Well-Architected Labs \u2013 Dependency Monitoring\n\nKnowledge Area: Threat vectors external to AWS (for example, DDoS, SQL injection)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best practice to help protect from external and internal threats for compute resources?\", \n\"answer_choices\": [\n{\"answer\": \"Use AWS Shield Advanced service\", \"is_correct\": \"False\", \"explanation\": \"AWS Shield Advanced service provides near real-time visibility into intelligence sources, if your workload is internet accessible. However, compute resources need multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more.\"}, \n{\"answer\": \"Automate configuration management with AWS Systems Manager and CloudFormation\", \"is_correct\": \"True\", \"explanation\": \"Automating configuration management with AWS Systems Manager and CloudFormation is a best practice to help protect from external and internal threats for compute resources. This can be achieved using AWS Systems Manager Automation, which uses automation documents that contain steps you use to perform tasks. These documents can be stored in source control, be peer reviewed before running, and tested thoroughly to minimize risk compared to shell access.\"}, \n{\"answer\": \"Use Amazon GuardDuty to detect suspicious activity or data exfiltration\", \"is_correct\": \"False\", \"explanation\": \"Using Amazon GuardDuty to detect suspicious activity or data exfiltration is a best practice to help protect from external and internal threats for compute resources. However, compute resources need multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more.\"}, \n{\"answer\": \"Keep all users away from directly accessing sensitive data and systems\", \"is_correct\": \"False\", \"explanation\": \"Keeping all users away from directly accessing sensitive data and systems is a best practice to help protect from external and internal threats for compute resources. However, compute resources need multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more.\"} \n]}", "docs": ["('SEC01-BP04 Keep up-to-date with security threats - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_securely_operate_updated_threats.html')", "('SEC 6\\xa0 How do you protect your compute resources? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-06.html')", "('SEC08-BP05 Use mechanisms to keep people away from data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_use_people_away.html')", "('SEC09-BP03 Automate detection of unintended data access - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_transit_auto_unintended_access.html')", "('Infrastructure protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-infrastructure.html')", "('OPS04-BP04 Implement dependency telemetry - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_telemetry_dependency_telemetry.html')"]}, {"scenario": "A social media platform wants to use AWS to store and process user-generated content.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Skills in", "item": "Designing VPC architectures with security components (for example, security groups, route tables, network ACLs, NAT gateways)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA social media platform wants to use AWS to store and process user-generated content.`\"\n\nContext:\n\n* When architecting your network topology, you should examine the connectivity requirements of each component. For example, if a component requires internet accessibility (inbound and outbound), connectivity to VPCs, edge services, and external data centers.A VPC allows you to define your network topology that spans an AWS Region with a private IPv4 address range that you set, or an IPv6 address range AWS selects. You should apply multiple controls with a defense in depth approach for both inbound and outbound traffic, including the use of security groups (stateful inspection firewall), Network ACLs, subnets, and route tables. Within a VPC, you can create subnets in an Availability Zone. Each subnet can have an associated route table that defines routing rules for managing the paths that traffic takes within the subnet. You can define an internet routable subnet by having a route that goes to an internet or NAT gateway attached to the VPC, or through another VPC.When an instance, Amazon Relational Database Service(Amazon RDS) database, or other service is launched within a VPC, it has its own security group per network interface. This firewall is outside the operating system layer and can be used to define rules for allowed inbound and outbound traffic. You can also define relationships between security groups. For example, instances within a database tier security group only accept traffic from instances within the application tier, by reference to the security groups applied to the instances involved. Unless you are using non-TCP protocols, it shouldn\u2019t be necessary to have an Amazon Elastic Compute Cloud(Amazon EC2) instance directly accessible by the internet (even with ports restricted by security groups) without a load balancer, or CloudFront. This helps protect it from unintended access through an operating system or application issue. A subnet can also have a network ACL attached to it, which acts as a stateless firewall. You should configure the network ACL to narrow the scope of traffic allowed between layers, note that you need to define both inbound and outbound rules.Some AWS services require components to access the internet for making API calls, where AWS API endpoints are located. Other AWS services use VPC endpoints within your Amazon VPCs. Many AWS services, including Amazon S3 and Amazon DynamoDB, support VPC endpoints, and this technology has been generalized in AWS PrivateLink. We recommend you use this approach to access AWS services, third-party services, and your own services hosted in other VPCs securely. All network traffic on AWS PrivateLink stays on the global AWS backbone and never traverses the internet. Connectivity can only be initiated by the consumer of the service, and not by the provider of the service. Using AWS PrivateLink for external service access allows you to create air-gapped VPCs with no internet access and helps protect your VPCs from external threat vectors. Third-party services can use AWS PrivateLink to allow their customers to connect to the services from their VPCs over private IP addresses. For VPC assets that need to make outbound connections to the internet, these can be made outbound only (one-way) through an AWS managed NAT gateway, outbound only internet gateway, or web proxies that you create and manage.Level of risk exposed if this best practice is not established: HighControl network traffic in a VPC: Implement VPC best practices to control traffic.Amazon VPC securityVPC endpointsAmazon VPC security groupNetwork ACLsControl traffic at the edge: Implement edge services, such as Amazon CloudFront, to provide an additional layer of protection and other features.Amazon CloudFront use casesAWS Global AcceleratorAWS Web Application Firewall (AWS WAF)Amazon Route 53Amazon VPC Ingress RoutingControl private network traffic: Implement services that protect your private traffic for your workload.Amazon VPC PeeringAmazon VPC Endpoint Services (AWS PrivateLink)Amazon VPC Transit GatewayAWS Direct ConnectAWS Site-to-Site VPNAWS Client VPNAmazon S3 Access PointsRelated documents:AWS Firewall ManagerAmazon InspectorGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n* Group components that share reachability requirements into layers. For example, a database cluster in a virtual private cloud (VPC) with no need for internet access should be placed in subnets with no route to or from the internet. In a serverless workload operating without a VPC, similar layering and segmentation with microservices can achieve the same goal.Components such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Relational Database Service (Amazon RDS) database clusters, and AWS Lambda functions that share reachability requirements can be segmented into layers formed by subnets. For example, an Amazon RDS database cluster in a VPC with no need for internet access should be placed in subnets with no route to or from the internet. This layered approach for the controls mitigates the impact of a single layer misconfiguration, which could allow unintended access. For Lambda, you can run your functions in your VPC to take advantage of VPC-based controls.For network connectivity that can include thousands of VPCs, AWS accounts, and on-premises networks, you should use AWS Transit Gateway. It acts as a hub that controls how traffic is routed among all the connected networks, which act like spokes. Traffic between an Amazon Virtual Private Cloud and AWS Transit Gateway remains on the AWS private network, which reduces external threat vectors such as distributed denial of service (DDoS) attacks and common exploits, such as SQL injection, cross-site scripting, cross-site request forgery, or abuse of broken authentication code. AWS Transit Gateway inter-region peering also encrypts inter-region traffic with no single point of failure or bandwidth bottleneck.Level of risk exposed if this best practice is not established: HighCreate subnets in VPC: Create subnets for each layer (in groups that include multiple Availability Zones), and associate route tables to control routing.VPCs and subnetsRoute tablesRelated documents:AWS Firewall ManagerAmazon InspectorAmazon VPC SecurityGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n\nKnowledge Area: Designing VPC architectures with security components (for example, security groups, route tables, network ACLs, NAT gateways)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When architecting a network topology for a social media platform using AWS to store and process user-generated content, which of the following is the best way to control traffic between two layers?\", \n\"answer_choices\": [{\"answer\": \"Use a NAT gateway to create a one-way outbound connection\",\"is_correct\": \"false\",\"explanation\": \"NAT gateways are used to allow outbound internet traffic from private subnets, but does not control traffic between two layers.\"},{\"answer\": \"Use a web proxy to control traffic between tiers\",\"is_correct\": \"false\",\"explanation\": \"Web proxies can be used to control traffic between tiers, but are not the best solution for controlling traffic between two layers.\"},{\"answer\": \"Use a VPC endpoint service\",\"is_correct\": \"true\",\"explanation\": \"VPC endpoint services allow for secure connectivity between VPCs and can be used to control traffic between two layers.\"},{\"answer\": \"Use a security group to control traffic between tiers\",\"is_correct\": \"false\",\"explanation\": \"Security groups are used to control the traffic into and out of a VPC, but should not be used to control traffic between two layers.\"}]}", "docs": ["('SEC05-BP02 Control traffic at all layers - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_layered.html')", "('SEC05-BP01 Create network layers - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_create_layers.html')"]}, {"scenario": "A research institution wants to perform data analysis on a large dataset using AWS compute resources.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Skills in", "item": "Determining network segmentation strategies (for example, using public subnets and private subnets)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA research institution wants to perform data analysis on a large dataset using AWS compute resources.`\"\n\nContext:\n\n* Group components that share reachability requirements into layers. For example, a database cluster in a virtual private cloud (VPC) with no need for internet access should be placed in subnets with no route to or from the internet. In a serverless workload operating without a VPC, similar layering and segmentation with microservices can achieve the same goal.Components such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Relational Database Service (Amazon RDS) database clusters, and AWS Lambda functions that share reachability requirements can be segmented into layers formed by subnets. For example, an Amazon RDS database cluster in a VPC with no need for internet access should be placed in subnets with no route to or from the internet. This layered approach for the controls mitigates the impact of a single layer misconfiguration, which could allow unintended access. For Lambda, you can run your functions in your VPC to take advantage of VPC-based controls.For network connectivity that can include thousands of VPCs, AWS accounts, and on-premises networks, you should use AWS Transit Gateway. It acts as a hub that controls how traffic is routed among all the connected networks, which act like spokes. Traffic between an Amazon Virtual Private Cloud and AWS Transit Gateway remains on the AWS private network, which reduces external threat vectors such as distributed denial of service (DDoS) attacks and common exploits, such as SQL injection, cross-site scripting, cross-site request forgery, or abuse of broken authentication code. AWS Transit Gateway inter-region peering also encrypts inter-region traffic with no single point of failure or bandwidth bottleneck.Level of risk exposed if this best practice is not established: HighCreate subnets in VPC: Create subnets for each layer (in groups that include multiple Availability Zones), and associate route tables to control routing.VPCs and subnetsRoute tablesRelated documents:AWS Firewall ManagerAmazon InspectorAmazon VPC SecurityGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n* Amazon VPC IP address ranges must be large enough to accommodate workload requirements, including factoring in future expansion and allocation of IP addresses to subnets across Availability Zones. This includes load balancers, EC2 instances, and container-based applications.When you plan your network topology, the first step is to define the IP address space itself. Private IP address ranges (following RFC 1918 guidelines) should be allocated for each VPC. Accommodate the following requirements as part of this process:Allow IP address space for more than one VPC per Region.Within a VPC, allow space for multiple subnets that span multiple Availability Zones.Always leave unused CIDR block space within a VPC for future expansion.Ensure that there is IP address space to meet the needs of any transient fleets of EC2 instances that you might use, such as Spot Fleets for machine learning, Amazon EMR clusters, or Amazon Redshift clusters.Note that the first four IP addresses and the last IP address in each subnet CIDR block are reserved and not available for your use.You should plan on deploying large VPC CIDR blocks. Note that the initial VPC CIDR block allocated to your VPC cannot be changed or deleted, but you can add additional non-overlapping CIDR blocks to the VPC. Subnet IPv4 CIDRs cannot be changed, however IPv6 CIDRs can. Keep in mind that deploying the largest VPC possible (/16) results in over 65,000 IP addresses. In the base 10.x.x.x IP address space alone, you could provision 255 such VPCs. You should therefore err on the side of being too large rather than too small to make it easier to manage your VPCs.Common anti-patterns:Creating small VPCs.Creating small subnets and then having to add subnets to configurations as you grow.Incorrectly estimating how many IP addresses a elastic load balancer can use.Deploying many high traffic load balancers into the same subnets.Benefits of establishing this best practice: This ensures that you can accommodate the growth of your workloads and continue to provide availability as you scale up.Level of risk exposed if this best practice is not established: MediumPlan your network to accommodate for growth, regulatory compliance, and integration with others. Growth can be underestimated, regulatory compliance can change, and acquisitions or private network connections can be difficult to implement without proper planning.Select relevant AWS accounts and Regions based on your service requirements, latency, regulatory, and disaster recovery (DR) requirements.Identify your needs for regional VPC deployments.Identify the size of the VPCs.Determine if you are going to deploy multi-VPC connectivity.What Is a Transit Gateway?Single Region Multi-VPC ConnectivityDetermine if you need segregated networking for regulatory requirements.Make VPCs as large as possible. The initial VPC CIDR block allocated to your VPC cannot be changed or deleted, but you can add additional non-overlapping CIDR blocks to the VPC. This however may fragment your address ranges.Related documents:APN Partner: partners that can help plan your networkingAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivitySingle Region Multi-VPC ConnectivityWhat Is Amazon VPC?Related videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n\nKnowledge Area: Determining network segmentation strategies (for example, using public subnets and private subnets)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is a best practice for creating a network segmentation strategy within AWS?\", \n\"answer_choices\": [\n{\"answer\": \"Create multiple subnets in the same Availability Zone and associate them with a route table.\",\"is_correct\": \"False\", \"explanation\": \"Subnets should span multiple Availability Zones.\"}, \n{\"answer\": \"Create subnets in VPC, associate route tables with each subnet, and plan for future expansion.\",\"is_correct\": \"True\", \"explanation\": \"This ensures that you can accommodate the growth of your workloads and continue to provide availability as you scale up.\"}, \n{\"answer\": \"Deploy large VPC CIDR blocks in the first four IP addresses in each subnet CIDR block.\",\"is_correct\": \"False\", \"explanation\": \"The first four IP addresses and the last IP address in each subnet CIDR block are reserved and not available for your use.\"}, \n{\"answer\": \"Deploy many high traffic load balancers into the same subnets.\",\"is_correct\": \"False\", \"explanation\": \"High traffic load balancers should be deployed into different subnets to maximize efficiency.\"}\n]}", "docs": ["('SEC05-BP01 Create network layers - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_create_layers.html')", "('REL02-BP03 Ensure IP subnet allocation accounts for expansion and availability - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ip_subnet_allocation.html')"]}, {"scenario": "A telecommunications company wants to use AWS to host and manage their network infrastructure.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Skills in", "item": "Integrating AWS services to secure applications (for example, AWS Shield, AWS WAF, AWS SSO, AWS Secrets Manager)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA telecommunications company wants to use AWS to host and manage their network infrastructure.`\"\n\nContext:\n\n* There are seven design principles for security in the cloud:Implement a strong identity foundation: Implement the principle of least privilege and enforce separation of duties with appropriate authorization for each interaction with your AWS resources. Centralize identity management, and aim to eliminate reliance on long-term static credentials.Enable traceability: Monitor, alert, and audit actions and changes to your environment in real time. Integrate log and metric collection with systems to automatically investigate and take action.Apply security at all layers: Apply a defense in depth approach with multiple security controls. Apply to all layers (for example, edge of network, VPC, load balancing, every instance and compute service, operating system, application, and code).Automate security best practices: Automated software-based security mechanisms improve your ability to securely scale more rapidly and cost-effectively. Create secure architectures, including the implementation of controls that are defined and managed as code in version-controlled templates.Protect data in transit and at rest: Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate.Keep people away from data: Use mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data. This reduces the risk of mishandling or modification and human error when handling sensitive data.Prepare for security events: Prepare for an incident by having incident management and investigation policy and processes that align to your organizational requirements. Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery.\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n* There are six best practice areas for security in the cloud:SecurityIdentity and Access ManagementDetectionInfrastructure ProtectionData ProtectionIncident ResponseBefore you architect any workload, you need to put in place practices that influence security. You will want to control who can do what. In addition, you want to be able to identify security incidents, protect your systems and services, and maintain the confidentiality and integrity of data through data protection. You should have a well-defined and practiced process for responding to security incidents. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.The AWS Shared Responsibility Model enables organizations that adopt the cloud to achieve their security and compliance goals. Because AWS physically secures the infrastructure that supports our cloud services, as an AWS customer you can focus on using services to accomplish your goals. The AWS Cloud also provides greater access to security data and an automated approach to responding to security events.\n* Automate your protective compute mechanisms including vulnerability management, reduction in attack surface, and management of resources. The automation will help you invest time in securing other aspects of your workload, and reduce the risk of human error.Level of risk exposed if this best practice is not established: MediumAutomate configuration management: Enforce and validate secure configurations automatically by using a configuration management service or tool.AWS Systems ManagerAWS CloudFormationLab: Automated deployment of VPCLab: Automated deployment of EC2 web applicationAutomate patching of Amazon Elastic Compute Cloud (Amazon EC2) instances: AWS Systems Manager Patch Manager automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications.AWS Systems Manager Patch ManagerCentralized multi-account and multi-Region patching with AWS Systems Manager AutomationImplement intrusion detection and prevention: Implement an intrusion detection and prevention tool to monitor and stop malicious activity on instances.Consider AWS Partner solutions: AWS Partners offer hundreds of industry-leading products that are equivalent, identical to, or integrate with existing controls in your on-premises environments. These products complement the existing AWS services to enable you to deploy a comprehensive security architecture and a more seamless experience across your cloud and on-premises environments.Infrastructure securityRelated documents:AWS CloudFormationAWS Systems ManagerAWS Systems Manager Patch ManagerCentralized multi-account and multi-region patching with AWS Systems Manager AutomationInfrastructure securityReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: Automated Deployment of Web Application FirewallLab: Automated deployment of Amazon EC2 web application\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n\nKnowledge Area: Integrating AWS services to secure applications (for example, AWS Shield, AWS WAF, AWS SSO, AWS Secrets Manager)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service should the telecommunications company implement to securely manage their network infrastructure?\",\n\"answer_choices\": [{\"answer\": \"AWS Shield\",\"is_correct\": \"True\",\"explanation\": \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. It protects against network and transport layer DDoS attacks, and can be used to secure HTTP and HTTPS applications.\"},{\"answer\": \"AWS WAF\",\"is_correct\": \"False\",\"explanation\": \"AWS WAF is a web application firewall service that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources.\"},{\"answer\": \"AWS Secrets Manager\",\"is_correct\": \"False\",\"explanation\": \"AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. It enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.\"},{\"answer\": \"AWS SSO\",\"is_correct\": \"False\",\"explanation\": \"AWS Single Sign-On (SSO) is a cloud-based identity and access management service that makes it easy to manage user access and user profiles in multiple AWS accounts and business applications.\"}]}", "docs": ["('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-design.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')", "('Definition - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-def.html')", "('SEC06-BP04 Automate compute protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_auto_protection.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 2: Design secure workloads and applications", "focus": "Skills in", "item": "Securing external network connections to and from the AWS Cloud (for example, VPN, AWS Direct Connect)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* Implement services to reduce data transfer. For example, using a content delivery network (CDN) such as Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using AWS Direct Connect instead of VPN for connectivity to AWS.Level of risk exposed if this best practice is not established: LowAmazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources. By using CloudFront, you can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network. It is ideal for small offices or business partners because it provides quick and easy connectivity, and it is a fully managed and elastic service.VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs. Gateway VPC endpoints have no hourly charges, and support Amazon Simple Storage Service(Amazon S3) and Amazon DynamoDB. Interface VPC endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost.Implementation stepsImplement services: Using the data transfer modeling, look at where the largest costs and highest volume flows are. Review the AWS services and assess whether there is a service that reduces or removes the transfer, specifically networking and content delivery. Also look for caching services where there is repeated access to data, or large amounts of data.Related documents:AWS Direct ConnectAWS Explore Our ProductsAWS caching solutionsAmazon CloudFrontDeliver content faster with Amazon CloudFront\n* When architecting your network topology, you should examine the connectivity requirements of each component. For example, if a component requires internet accessibility (inbound and outbound), connectivity to VPCs, edge services, and external data centers.A VPC allows you to define your network topology that spans an AWS Region with a private IPv4 address range that you set, or an IPv6 address range AWS selects. You should apply multiple controls with a defense in depth approach for both inbound and outbound traffic, including the use of security groups (stateful inspection firewall), Network ACLs, subnets, and route tables. Within a VPC, you can create subnets in an Availability Zone. Each subnet can have an associated route table that defines routing rules for managing the paths that traffic takes within the subnet. You can define an internet routable subnet by having a route that goes to an internet or NAT gateway attached to the VPC, or through another VPC.When an instance, Amazon Relational Database Service(Amazon RDS) database, or other service is launched within a VPC, it has its own security group per network interface. This firewall is outside the operating system layer and can be used to define rules for allowed inbound and outbound traffic. You can also define relationships between security groups. For example, instances within a database tier security group only accept traffic from instances within the application tier, by reference to the security groups applied to the instances involved. Unless you are using non-TCP protocols, it shouldn\u2019t be necessary to have an Amazon Elastic Compute Cloud(Amazon EC2) instance directly accessible by the internet (even with ports restricted by security groups) without a load balancer, or CloudFront. This helps protect it from unintended access through an operating system or application issue. A subnet can also have a network ACL attached to it, which acts as a stateless firewall. You should configure the network ACL to narrow the scope of traffic allowed between layers, note that you need to define both inbound and outbound rules.Some AWS services require components to access the internet for making API calls, where AWS API endpoints are located. Other AWS services use VPC endpoints within your Amazon VPCs. Many AWS services, including Amazon S3 and Amazon DynamoDB, support VPC endpoints, and this technology has been generalized in AWS PrivateLink. We recommend you use this approach to access AWS services, third-party services, and your own services hosted in other VPCs securely. All network traffic on AWS PrivateLink stays on the global AWS backbone and never traverses the internet. Connectivity can only be initiated by the consumer of the service, and not by the provider of the service. Using AWS PrivateLink for external service access allows you to create air-gapped VPCs with no internet access and helps protect your VPCs from external threat vectors. Third-party services can use AWS PrivateLink to allow their customers to connect to the services from their VPCs over private IP addresses. For VPC assets that need to make outbound connections to the internet, these can be made outbound only (one-way) through an AWS managed NAT gateway, outbound only internet gateway, or web proxies that you create and manage.Level of risk exposed if this best practice is not established: HighControl network traffic in a VPC: Implement VPC best practices to control traffic.Amazon VPC securityVPC endpointsAmazon VPC security groupNetwork ACLsControl traffic at the edge: Implement edge services, such as Amazon CloudFront, to provide an additional layer of protection and other features.Amazon CloudFront use casesAWS Global AcceleratorAWS Web Application Firewall (AWS WAF)Amazon Route 53Amazon VPC Ingress RoutingControl private network traffic: Implement services that protect your private traffic for your workload.Amazon VPC PeeringAmazon VPC Endpoint Services (AWS PrivateLink)Amazon VPC Transit GatewayAWS Direct ConnectAWS Site-to-Site VPNAWS Client VPNAmazon S3 Access PointsRelated documents:AWS Firewall ManagerAmazon InspectorGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n\nKnowledge Area: Securing external network connections to and from the AWS Cloud (for example, VPN, AWS Direct Connect)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"A gaming company wants to use AWS to host and scale their multiplayer game servers. What service should they use to reduce data transfer and establish a secure and private connection between their private network and the AWS global network?\", \"answer_choices\": [{\"answer\": \"Amazon CloudFront\", \"is_correct\": \"false\", \"explanation\": \"Amazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds, but it is not used to establish a secure and private connection between your private network and the AWS global network.\"},{\"answer\": \"AWS Direct Connect\", \"is_correct\": \"false\", \"explanation\": \"AWS Direct Connect allows you to establish a dedicated network connection to AWS, but it is not used to reduce data transfer.\"},{\"answer\": \"AWS Virtual Private Network (VPN)\", \"is_correct\": \"true\", \"explanation\": \"AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network, which can reduce data transfer.\"},{\"answer\": \"VPC Endpoints\", \"is_correct\": \"false\", \"explanation\": \"VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs, but it is not used to establish a secure and private connection between your private network and the AWS global network.\"}]}", "docs": ["('COST08-BP03 Implement services to reduce data transfer costs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_implement_services.html')", "('SEC05-BP02 Control traffic at all layers - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_layered.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Knowledge of", "item": "Data access and governance", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* Your defined lifecycle strategy should be based on sensitivity level as well as legal and organization requirements. Aspects including the duration for which you retain data, data destruction processes, data access management, data transformation, and data sharing should be considered. When choosing a data classification methodology, balance usability versus access. You should also accommodate the multiple levels of access and nuances for implementing a secure, but still usable, approach for each level. Always use a defense in depth approach and reduce human access to data and mechanisms for transforming, deleting, or copying data. For example, require users to strongly authenticate to an application, and give the application, rather than the users, the requisite access permission to perform action at a distance. In addition, ensure that users come from a trusted network path and require access to the decryption keys. Use tools, such as dashboards and automated reporting, to give users information from the data rather than giving them direct access to the data.Level of risk exposed if this best practice is not established: LowIdentify data types: Identify the types of data that you are storing or processing in your workload. That data could be text, images, binary databases, and so forth.Related documents:Data Classification WhitepaperGetting started with Amazon MacieRelated videos:Introducing the New Amazon Macie\n* Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.Best practices:\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n\nKnowledge Area: Data access and governance\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following best practices should be used when selecting a data classification methodology to use in the given scenario?\",\n  \"answer_choices\": [\n    {\"answer\": \"Select one database vendor to simplify operations management.\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern and should be avoided. The best practice is to balance usability and access, and use a defense in depth approach.\"},\n    {\"answer\": \"Require users to authenticate to an application and give the application the required access permission to perform action at a distance.\",\"is_correct\": \"True\",\"explanation\": \"This is a best practice that should be used when selecting a data classification methodology to use in the given scenario.\"},\n    {\"answer\": \"Allow users direct access to the data.\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern and should be avoided. The best practice is to use tools such as dashboards and automated reporting to give users information from the data rather than giving them direct access to the data.\"},\n    {\"answer\": \"Use a shared database for transactional and analytical uses.\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern and should be avoided. The best practice is to use the access patterns of the workload to decide which services and technologies to use.\"}\n  ]\n}", "docs": ["('SEC07-BP04 Define data lifecycle management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_data_classification_lifecycle_management.html')", "('SUS 4 How do you take advantage of data access and usage patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-04.html')", "('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Knowledge of", "item": "Data recovery", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO).Best practices\n* Define a disaster recovery (DR) strategy that meets your workload's recovery objectives. Choose a strategy such as: backup and restore; standby (active/passive); or active/active.\n* To minimize storage consumption, only back up data that has business value or is needed to satisfy compliance requirements. Examine backup policies and exclude ephemeral storage that doesn\u2019t provide value in a recovery scenario.Level of risk exposed if this best practice is not established: LowUse your data classification to establish what data needs to be backed up.Exclude data that you can easily recreate.Exclude ephemeral data from your backups.Exclude local copies of data, unless the time required to restore that data from a common location exceeds your service level agreements (SLAs).Related documents:Using AWS Backup to back up and restore Amazon EFS file systemsAmazon EBS snapshotsWorking with backups on Amazon Relational Database Service\n* Regularly test failover to your recovery site to ensure proper operation, and that RTO and RPO are met.A pattern to avoid is developing recovery paths that are rarely exercised. For example, you might have a secondary data store that is used for read-only queries. When you write to a data store and the primary fails, you might want to fail over to the secondary data store. If you don\u2019t frequently test this failover, you might find that your assumptions about the capabilities of the secondary data store are incorrect. The capacity of the secondary, which might have been sufficient when you last tested, might be no longer be able to tolerate the load under this scenario. Our experience has shown that the only error recovery that works is the path you test frequently. This is why having a small number of recovery paths is best. You can establish recovery patterns and regularly test them. If you have a complex or critical recovery path, you still need to regularly exercise that failure in production to convince yourself that the recovery path works. In the example we just discussed, you should fail over to the standby regularly, regardless of need.Common anti-patterns:Never exercise failovers in production.Benefits of establishing this best practice: Regularly testing you disaster recovery plan ensures that it will work when it needs to, and that your team knows how to execute the strategy.Level of risk exposed if this best practice is not established: HighEngineer your workloads for recovery. Regularly test your recovery paths Recovery Oriented Computing identifies the characteristics in systems that enhance recovery. These characteristics are: isolation and redundancy, system-wide ability to roll back changes, ability to monitor and determine health, ability to provide diagnostics, automated recovery, modular design, and ability to restart. Exercise the recovery path to ensure that you can accomplish the recovery in the specified time to the specified state. Use your runbooks during this recovery to document problems and find solutions for them before the next test.The Berkeley/Stanford recovery-oriented computing projectUse AWS Elastic Disaster Recovery to implement and launch drill instances for your DR strategy.AWS Elastic Disaster Recovery Preparing for FailoverWhat is Elastic Disaster Recovery?AWS Elastic Disaster RecoveryRelated documents:APN Partner: partners that can help with disaster recoveryAWS Architecture Blog: Disaster Recovery SeriesAWS Marketplace: products that can be used for disaster recoveryAWS Elastic Disaster RecoveryDisaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)AWS Elastic Disaster Recovery Preparing for FailoverThe Berkeley/Stanford recovery-oriented computing projectWhat is AWS Fault Injection Simulator?Related videos:AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)AWS re:Invent 2019: Backup-and-restore and disaster-recovery solutions with AWS (STG208)Related examples:AWS Well-Architected Labs - Testing for Resiliency\n\nKnowledge Area: Data recovery\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which disaster recovery strategy best suits the retail company's e-commerce platform during peak shopping seasons?\", \n\"answer_choices\": [\n{\"answer\": \"Back up and restore data\",\"is_correct\": \"False\",\"explanation\": \"Backup and restore is not suitable for peak shopping season given the latency it may cause.\"}, \n{\"answer\": \"Active/Active\",\"is_correct\": \"True\",\"explanation\": \"Active/Active is best suited for high availability and scalability, which are necessary to accommodate peak shopping season.\"}, \n{\"answer\": \"Active/Passive\",\"is_correct\": \"False\",\"explanation\": \"Active/Passive, while suitable for disaster recovery, is not the best option for peak shopping season due to its limited scalability.\"}, \n{\"answer\": \"Examine backup policies\",\"is_correct\": \"False\",\"explanation\": \"Examining backup policies is an important consideration in DR, but is not suitable for peak shopping season given the need for increased scalability.\"}\n]}", "docs": ["('REL 9\\xa0 How do you back up data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-09.html')", "('REL13-BP02 Use defined recovery strategies to meet the recovery objectives - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_disaster_recovery.html')", "('SUS04-BP08 Back up data only when difficult to recreate - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a9.html')", "('REL13-BP03 Test disaster recovery implementation to validate the implementation - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_dr_tested.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Knowledge of", "item": "Data retention and classification", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* Classify data to understand its significance to business outcomes. Use this information to determine when you can move data to more energy-efficient storage or safely delete it.Level of risk exposed if this best practice is not established: LowDetermine requirements for the distribution, retention, and deletion of your data.Use tagging on volumes and objects to record the metadata that\u2019s used to determine how it\u2019s managed, including data classification.Periodically audit your environment for untagged and unclassified data, and classify and tag the data appropriately.Related documents:Data Classification ProcessLeveraging AWS Cloud to Support Data ClassificationTag policies from AWS Organizations\n* Classification provides a way to categorize data, based on criticality and sensitivity in order to help you determine appropriate protection and retention controls.Best practices\n* You need to understand the type and classification of data your workload is processing, the associated business processes, data owner, applicable legal and compliance requirements, where it\u2019s stored, and the resulting controls that are needed to be enforced. This may include classifications to indicate if the data is intended to be publicly available, if the data is internal use only such as customer personally identifiable information (PII), or if the data is for more restricted access such as intellectual property, legally privileged or marked sensitive, and more. By carefully managing an appropriate data classification system, along with each workload\u2019s level of protection requirements, you can map the controls and level of access or protection appropriate for the data. For example, public content is available for anyone to access, but important content is encrypted and stored in a protected manner that requires authorized access to a key for decrypting the content.Level of risk exposed if this best practice is not established: HighConsider discovering data using Amazon Macie: Macie recognizes sensitive data such as personally identifiable information (PII) or intellectual property.Amazon MacieRelated documents:Amazon MacieData Classification WhitepaperGetting started with Amazon MacieRelated videos:Introducing the New Amazon Macie\n* Your defined lifecycle strategy should be based on sensitivity level as well as legal and organization requirements. Aspects including the duration for which you retain data, data destruction processes, data access management, data transformation, and data sharing should be considered. When choosing a data classification methodology, balance usability versus access. You should also accommodate the multiple levels of access and nuances for implementing a secure, but still usable, approach for each level. Always use a defense in depth approach and reduce human access to data and mechanisms for transforming, deleting, or copying data. For example, require users to strongly authenticate to an application, and give the application, rather than the users, the requisite access permission to perform action at a distance. In addition, ensure that users come from a trusted network path and require access to the decryption keys. Use tools, such as dashboards and automated reporting, to give users information from the data rather than giving them direct access to the data.Level of risk exposed if this best practice is not established: LowIdentify data types: Identify the types of data that you are storing or processing in your workload. That data could be text, images, binary databases, and so forth.Related documents:Data Classification WhitepaperGetting started with Amazon MacieRelated videos:Introducing the New Amazon Macie\n* Before architecting any system, foundational practices that influence security should be in place. For example, data classification provides a way to categorize organizational data based on levels of sensitivity, and encryption protects data by way of rendering it unintelligible to unauthorized access. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.In AWS, the following practices facilitate protection of data:As an AWS customer you maintain full control over your data.AWS makes it easier for you to encrypt your data and manage keys, including regular key rotation, which can be easily automated by AWS or maintained by you.Detailed logging that contains important content, such as file access and changes, is available.AWS has designed storage systems for exceptional resiliency. For example, Amazon S3 Standard, S3 Standard\u2013IA, S3 One Zone-IA, and Amazon Glacier are all designed to provide 99.999999999% durability of objects over a given year. This durability level corresponds to an average annual expected loss of 0.000000001% of objects.Versioning, which can be part of a larger data lifecycle management process, can protect against accidental overwrites, deletes, and similar harm.AWS never initiates the movement of data between Regions. Content placed in a Region will remain in that Region unless you explicitly enable a feature or leverage a service that provides that functionality.The following questions focus on these considerations for security.AWS provides multiple means for encrypting data at rest and in transit. We build features into our services that make it easier to encrypt your data. For example, we have implemented server-side encryption (SSE) for Amazon S3 to make it easier for you to store your data in an encrypted form. You can also arrange for the entire HTTPS encryption and decryption process (generally known as SSL termination) to be handled by Elastic Load Balancing (ELB).SEC 7: How do you classify your data?Classification provides a way to categorize data, based on criticality and sensitivity in order to help you determine appropriate protection and retention controls.SEC 8: How do you protect your data at rest?Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access or mishandling.SEC 9: How do you protect your data in transit?Protect your data in transit by implementing multiple controls to reduce the risk of unauthorized access or loss.\n\nKnowledge Area: Data retention and classification\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What best practice should a retail company use when leveraging AWS to support their e-commerce platform during peak shopping seasons to determine appropriate protection and retention controls?\", \n\"answer_choices\": [\n    {\"answer\": \"Deploy Amazon Macie to discover sensitive data\",\"is_correct\": \"False\",\"explanation\": \"Amazon Macie is used for discovering sensitive data, but first you need to classify the data to understand its significance to business outcomes before you can determine appropriate protection and retention controls.\"},\n{\"answer\": \"Use tagging to record metadata of data\",\"is_correct\": \"True\",\"explanation\": \"Using tagging on volumes and objects can help to record the metadata that's used to determine how it's managed, including data classification.\"},\n{\"answer\": \"Audit environment for unclassified data\",\"is_correct\": \"True\",\"explanation\": \"Periodically audit the environment for untagged and unclassified data, and classify and tag the data appropriately.\"},\n{\"answer\": \"Encrypt important content\",\"is_correct\": \"False\",\"explanation\": \"Encrypting important content is important, but it should be based on the sensitivity level as well as legal and organization requirements after classifying the data.\"}\n]}", "docs": ["('SUS04-BP01 Implement a data classification policy - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a2.html')", "('SEC 7\\xa0 How do you classify your data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-07.html')", "('SEC07-BP01 Identify the data within your workload - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_data_classification_identify_data.html')", "('SEC07-BP04 Define data lifecycle management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_data_classification_lifecycle_management.html')", "('Data protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-dataprot.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Knowledge of", "item": "Encryption and appropriate key management", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* By defining an encryption approach that includes the storage, rotation, and access control of keys, you can help provide protection for your content against unauthorized users and against unnecessary exposure to authorized users. AWS Key Management Service (AWS KMS) helps you manage encryption keys and integrates with many AWS services. This service provides durable, secure, and redundant storage for your AWS KMS keys. You can define your key aliases as well as key-level policies. The policies help you define key administrators as well as key users. Additionally, AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys in the AWS Cloud. It helps you meet corporate, contractual, and regulatory compliance requirements for data security by using FIPS 140-2 Level 3 validated HSMs.Level of risk exposed if this best practice is not established: HighImplement AWS KMS: AWS KMS makes it easy for you to create and manage keys and control the use of encryption across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses FIPS 140-2 validated hardware security modules to protect your keys.Getting started: AWS Key Management Service (AWS KMS)Consider AWS Encryption SDK: Use the AWS Encryption SDK with AWS KMS integration when your application needs to encrypt data client-side.AWS Encryption SDKRelated documents:AWS Key Management ServiceAWS cryptographic services and toolsGetting started: AWS Key Management Service (AWS KMS)Protecting Amazon S3 Data Using EncryptionRelated videos:How Encryption Works in AWSSecuring Your Block Storage on AWS\n* Store encryption keys and certificates securely and rotate them at appropriate time intervals with strict access control. The best way to accomplish this is to use a managed service, such as AWS Certificate Manager (ACM). It lets you easily provision, manage, and deploy public and private Transport Layer Security (TLS) certificates for use with AWS services and your internal connected resources. TLS certificates are used to secure network communications and establish the identity of websites over the internet as well as resources on private networks. ACM integrates with AWS resources, such as Elastic Load Balancers (ELBs), AWS distributions, and APIs on API Gateway, also handling automatic certificate renewals. If you use ACM to deploy a private root CA, both certificates and private keys can be provided by it for use in Amazon Elastic Compute Cloud (Amazon EC2) instances, containers, and so on.Level of risk exposed if this best practice is not established: HighImplement secure key and certificate management: Implement your defined secure key and certificate management solution.AWS Certificate ManagerHow to host and manage an entire private certificate infrastructure in AWSImplement secure protocols: Use secure protocols that offer authentication and confidentiality, such as Transport Layer Security (TLS) or IPsec, to reduce the risk of data tampering or loss. Check the AWS documentation for the protocols and security relevant to the services that you are using.Related documents:AWS Documentation\n* Control and detect access to backups using authentication and authorization, such as AWS IAM. Prevent and detect if data integrity of backups is compromised using encryption.Amazon S3 supports several methods of encryption of your data at rest. Using server-side encryption, Amazon S3 accepts your objects as unencrypted data, and then encrypts them as they are stored. Using client-side encryption, your workload application is responsible for encrypting the data before it is sent to Amazon S3. Both methods allow you to use AWS Key Management Service (AWS KMS) to create and store the data key, or you can provide your own key, which you are then responsible for. Using AWS KMS, you can set policies using IAM on who can and cannot access your data keys and decrypted data.For Amazon RDS, if you have chosen to encrypt your databases, then your backups are encrypted also. DynamoDB backups are always encrypted.Common anti-patterns:Having the same access to the backups and restoration automation as you do to the data.Not encrypting your backups.Benefits of establishing this best practice: Securing your backups prevents tampering with the data, and encryption of the data prevents access to that data if it is accidentally exposed.Level of risk exposed if this best practice is not established: HighUse encryption on each of your data stores. If your source data is encrypted, then the backup will also be encrypted.Enable encryption in RDS. You can configure encryption at rest using AWS Key Management Service when you create an RDS instance.Encrypting Amazon RDS ResourcesEnable encryption on EBS volumes. You can configure default encryption or specify a unique key upon volume creation.Amazon EBS EncryptionUse the required Amazon DynamoDB encryption. DynamoDB encrypts all data at rest. You can either use an AWS owned AWS KMS key or an AWS managed KMS key, specifying a key that is stored in your account.DynamoDB Encryption at RestManaging Encrypted TablesEncrypt your data stored in Amazon EFS. Configure the encryption when you create your file system.Encrypting Data and Metadata in EFSConfigure the encryption in the source and destination Regions. You can configure encryption at rest in Amazon S3 using keys stored in KMS, but the keys are Region-specific. You can specify the destination keys when you configure the replication.CRR Additional Configuration: Replicating Objects Created with Server-Side Encryption (SSE) Using Encryption Keys stored in AWS KMSImplement least privilege permissions to access your backups. Follow best practices to limit the access to the backups, snapshots, and replicas in accordance with security best practices.Security Pillar: AWS Well-ArchitectedRelated documents:AWS Marketplace: products that can be used for backupAmazon EBS EncryptionAmazon S3: Protecting Data Using EncryptionCRR Additional Configuration: Replicating Objects Created with Server-Side Encryption (SSE) Using Encryption Keys stored in AWS KMSDynamoDB Encryption at RestEncrypting Amazon RDS ResourcesEncrypting Data and Metadata in EFSEncryption for Backups in AWSManaging Encrypted TablesSecurity Pillar: AWS Well-ArchitectedRelated examples:Well-Architected lab: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon S3\n\nKnowledge Area: Encryption and appropriate key management\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which is the best encryption and key management solution for the healthcare company in the scenario to protect patient information in the cloud?\", \"answer_choices\": [{\"answer\": \"Using the AWS Encryption SDK with AWS KMS integration\",\"is_correct\": \"true\",\"explanation\": \"The AWS Encryption SDK integrated with AWS KMS provides best-practice encryption and key management for sensitive patient information.\"},{\"answer\": \"Using AWS Certificate Manager (ACM) to deploy a private root CA\",\"is_correct\": \"false\",\"explanation\": \"AWS Certificate Manager is used to securely provision, manage, and deploy TLS certificates, but does not provide encryption for patient information.\"},{\"answer\": \"Using Amazon EBS encryption\",\"is_correct\": \"false\",\"explanation\": \"Amazon EBS encryption protects data stored on EBS volumes, but does not provide encryption for patient information.\"},{\"answer\": \"Using Amazon S3 server-side encryption\",\"is_correct\": \"false\",\"explanation\": \"Amazon S3 server-side encryption will protect data stored in S3, but does not provide encryption for patient information.\"}]}", "docs": ["('SEC08-BP01 Implement secure key management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_key_mgmt.html')", "('SEC09-BP01 Implement secure key and certificate management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_transit_key_cert_mgmt.html')", "('REL09-BP02 Secure and encrypt backups - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_backing_up_data_secured_backups_data.html')"]}, {"scenario": "A startup wants to host their new web application on AWS.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Skills in", "item": "Aligning AWS technologies to meet compliance requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA startup wants to host their new web application on AWS.`\"\n\nContext:\n\n* Based on your compliance requirements and risks identified from your threat model, derive and validate the control objectives and controls that you need to apply to your workload. Ongoing validation of control objectives and controls help you measure the effectiveness of risk mitigation.Level of risk exposed if this best practice is not established: HighIdentify compliance requirements: Discover the organizational, legal, and compliance requirements that your workload must comply with.Identify AWS compliance resources: Identify resources that AWS has available to assist you with compliance.https://aws.amazon.com/compliance/https://aws.amazon.com/artifact/Related documents:AWS Security Audit GuidelinesSecurity BulletinsRelated videos:AWS Security Hub: Manage Security Alerts and Automate ComplianceSecurity Best Practices the Well-Architected Way\n* Refer to the following resources to learn more about our best practices for Security.AWS Cloud SecurityAWS ComplianceAWS Security BlogSecurity PillarAWS Security OverviewAWS Risk and ComplianceAWS Security State of the UnionShared Responsibility Overview\n* AWS Architecture CenterAWS Cloud ComplianceAWS Well-Architected Partner programAWS Well-Architected ToolAWS Well-Architected homepageOperational Excellence Pillar whitepaperSecurity Pillar whitepaperReliability Pillar whitepaperPerformance Efficiency Pillar whitepaperCost Optimization Pillar whitepaperSustainability Pillar whitepaperThe Amazon Builders' Library\n* There are six best practice areas for security in the cloud:SecurityIdentity and Access ManagementDetectionInfrastructure ProtectionData ProtectionIncident ResponseBefore you architect any workload, you need to put in place practices that influence security. You will want to control who can do what. In addition, you want to be able to identify security incidents, protect your systems and services, and maintain the confidentiality and integrity of data through data protection. You should have a well-defined and practiced process for responding to security incidents. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.The AWS Shared Responsibility Model enables organizations that adopt the cloud to achieve their security and compliance goals. Because AWS physically secures the infrastructure that supports our cloud services, as an AWS customer you can focus on using services to accomplish your goals. The AWS Cloud also provides greater access to security data and an automated approach to responding to security events.\n* As AWS releases new services and features, it's a best practice to review your existing architectural decisions to ensure they continue to be the most cost effective.Best practices\n* There are four best practice areas for performance efficiency in the cloud:SelectionReviewMonitoringTradeoffsTake a data-driven approach to building a high-performance architecture. Gather data on all aspects of the architecture, from the high-level design to the selection and configuration of resource types.Reviewing your choices on a regular basis ensures that you are taking advantage of the continually evolving AWS Cloud. Monitoring ensures that you are aware of any deviance from expected performance. Make trade-o\ufb00s in your architecture to improve performance, such as using compression or caching, or relaxing consistency requirements.\n* Share best practices across teams to increase awareness and maximize the benefits of development efforts.On AWS, application, compute, infrastructure, and operations can be defined and managed using code methodologies. This allows for easy release, sharing, and adoption.Many AWS services and resources are designed to be shared across accounts, enabling you to share created assets and learnings across your teams. For example, you can share CodeCommit repositories, Lambda functions, Amazon S3 buckets, and AMIs to specific accounts.When you publish new resources or updates, use Amazon SNS to provide cross account notifications. Subscribers can use Lambda to get new versions.If shared standards are enforced in your organization, it\u2019s critical that mechanisms exist to request additions, changes, and exceptions to standards in support of teams\u2019 activities. Without this option, standards become a constraint on innovation.Common anti-patterns:You have created your own user authentication mechanism, as have each of the other development teams in your organization. Your users have to maintain a separate set of credentials for each part of the system they want to access.You have created your own user authentication mechanism, as have each of the other development teams in your organization. Your organization is given a new compliance requirement that must be met. Every individual development team must now invest the resources to implement the new requirement.You have created your own screen layout, as have each of the other development teams in your organization. Your users are complaining about the difficulty of navigating the inconsistent interfaces.Benefits of establishing this best practice: Use shared standards to support the adoption of best practices and to maximizes the benefits of development efforts where standards satisfy requirements for multiple applications or organizations.Level of risk exposed if this best practice is not established: MediumShare design standards: Share existing best practices, design standards, checklists, operating procedures, and guidance and governance requirements across teams to reduce complexity and maximize the benefits from development efforts. Ensure that procedures exist to request changes, additions, and exceptions to design standards to support continual improvement and innovation. Ensure that teams are aware of published content so that they can take advantage of content, and limit rework and wasted effort.Delegating access to your AWS environmentShare an AWS CodeCommit repositoryEasy authorization of AWS Lambda functionsSharing an AMI with specific AWS accountsSpeed template sharing with an AWS CloudFormation designer URLUsing AWS Lambda with Amazon SNSRelated documents:Easy authorization of AWS Lambda functionsShare an AWS CodeCommit repositorySharing an AMI with specific AWS accountsSpeed template sharing with an AWS CloudFormation designer URLUsing AWS Lambda with Amazon SNSRelated videos:Delegating access to your AWS environment\n\nKnowledge Area: Aligning AWS technologies to meet compliance requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best practice to align AWS technologies to meet compliance requirements?\",\n\"answer_choices\": [{\"answer\": \"Share design standards and procedures to request changes and additions\",\"is_correct\": \"true\",\"explanation\": \"Sharing design standards and procedures to request changes and additions is the best practice to align AWS technologies to meet compliance requirements.\"},{\"answer\": \"Share an AWS CodeCommit repository\",\"is_correct\": \"false\",\"explanation\": \"Sharing an AWS CodeCommit repository is not the best practice to align AWS technologies to meet compliance requirements.\"},{\"answer\": \"Delegating access to your AWS environment\",\"is_correct\": \"false\",\"explanation\": \"Delegating access to your AWS environment is not the best practice to align AWS technologies to meet compliance requirements.\"},{\"answer\": \"Using AWS Lambda with Amazon SNS\",\"is_correct\": \"false\",\"explanation\": \"Using AWS Lambda with Amazon SNS is not the best practice to align AWS technologies to meet compliance requirements.\"}]}", "docs": ["('SEC01-BP03 Identify and validate control objectives - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_securely_operate_control_objectives.html')", "('Resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-resources.html')", "('Further reading - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/further-reading.html')", "('Definition - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-def.html')", "('COST 10\\xa0 How do you evaluate new services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-10.html')", "('Definition - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-def.html')", "('OPS05-BP06 Share design standards - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_dev_integ_share_design_stds.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Skills in", "item": "Encrypting data at rest (for example, AWS Key Management Service [AWS KMS])", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* You should ensure that the only way to store data is by using encryption. AWS Key Management Service (AWS KMS) integrates seamlessly with many AWS services to make it easier for you to encrypt all your data at rest. For example, in Amazon Simple Storage Service (Amazon S3), you can set default encryption on a bucket so that all new objects are automatically encrypted. Additionally, Amazon Elastic Compute Cloud (Amazon EC2) and Amazon S3 support the enforcement of encryption by setting default encryption. You can use AWS Config Rules to check automatically that you are using encryption, for example, for Amazon Elastic Block Store (Amazon EBS) volumes, Amazon Relational Database Service (Amazon RDS) instances, and Amazon S3 buckets.Level of risk exposed if this best practice is not established: HighEnforce encryption at rest for Amazon Simple Storage Service (Amazon S3): Implement Amazon S3 bucket default encryption.How do I enable default encryption for an S3 bucket?Use AWS Secrets Manager: Secrets Manager is an AWS service that makes it easy for you to manage secrets. Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text.AWS Secrets ManagerConfigure default encryption for new EBS volumes: Specify that you want all newly created EBS volumes to be created in encrypted form, with the option of using the default key provided by AWS, or a key that you create.Default encryption for EBS volumesConfigure encrypted Amazon Machine Images (AMIs): Copying an existing AMI with encryption enabled will automatically encrypt root volumes and snapshots.AMIs with encrypted SnapshotsConfigure Amazon Relational Database Service (Amazon RDS) encryption: Configure encryption for your Amazon RDS database clusters and snapshots at rest by enabling the encryption option.Encrypting Amazon RDS resourcesConfigure encryption in additional AWS services: For the AWS services you use, determine the encryption capabilities.AWS DocumentationRelated documents:AMIs with encrypted SnapshotsAWS Crypto ToolsAWS DocumentationAWS Encryption SDKAWS KMS Cryptographic Details WhitepaperAWS Key Management ServiceAWS Secrets ManagerAWS cryptographic services and toolsAmazon EBS EncryptionDefault encryption for EBS volumesEncrypting Amazon RDS ResourcesHow do I enable default encryption for an S3 bucket?Protecting Amazon S3 Data Using EncryptionRelated videos:How Encryption Works in AWSSecuring Your Block Storage on AWS\n* Use automated tools to validate and enforce data at rest controls continuously, for example, verify that there are only encrypted storage resources. You can automate validation that all EBS volumes are encrypted using AWS Config Rules. AWS Security Hub can also verify several different controls through automated checks against security standards. Additionally, your AWS Config Rules can automatically remediate noncompliant resources.Level of risk exposed if this best practice is not established: MediumData at rest represents any data that you persist in non-volatile storage for any duration in your workload. This includes block storage, object storage, databases, archives, IoT devices, and any other storage medium on which data is persisted. Protecting your data at rest reduces the risk of unauthorized access, when encryption and appropriate access controls are implemented.Enforce encryption at rest: You should ensure that the only way to store data is by using encryption. AWS KMS integrates seamlessly with many AWS services to make it easier for you to encrypt all your data at rest. For example, in Amazon Simple Storage Service (Amazon S3) you can set default encryption on a bucket so that all new objects are automatically encrypted. Additionally, Amazon EC2 and Amazon S3 support the enforcement of encryption by setting default encryption. You can use AWS Managed Config Rules to check automatically that you are using encryption, for example, for EBS volumes, Amazon Relational Database Service (Amazon RDS) instances, and Amazon S3 buckets.Related documents:AWS Crypto ToolsAWS Encryption SDKRelated videos:How Encryption Works in AWSSecuring Your Block Storage on AWS\n\nKnowledge Area: Encrypting data at rest (for example, AWS Key Management Service [AWS KMS])\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best way to ensure that all data is being stored with encryption in the scenario described?\", \n\"answer_choices\": [\n    {\"answer\": \"Set up a default encryption key and use it to encrypt all data stored on AWS services.\", \"is_correct\": \"true\", \"explanation\": \"Setting up a default encryption key and using it to encrypt all data stored on AWS services is the best way to ensure all data is stored with encryption. This can be accomplished by setting default encryption on an S3 bucket and enabling the encryption option in Amazon RDS.\"},\n    {\"answer\": \"Enable AWS Security Hub to verify data encryption.\", \"is_correct\": \"false\", \"explanation\": \"AWS Security Hub can verify several different controls through automated checks against security standards, but it will not encrypt data. You must set up a default encryption key and use it to encrypt all of the data stored on AWS services.\"},\n    {\"answer\": \"Use AWS Config Rules to automatically check for encryption.\", \"is_correct\": \"false\", \"explanation\": \"AWS Config Rules can automatically check for encryption, but this will not perform the encryption for you. You must set up a default encryption key and use it to encrypt all of the data stored on AWS services.\"},\n    {\"answer\": \"Use AWS Secrets Manager to encrypt all data.\", \"is_correct\": \"false\", \"explanation\": \"AWS Secrets Manager is an AWS service that can be used to manage secrets, but it will not encrypt data. You must set up a default encryption key and use it to encrypt all of the data stored on AWS services.\"}\n]\n}", "docs": ["('SEC08-BP02 Enforce encryption at rest - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_encrypt.html')", "('SEC08-BP03 Automate data at rest protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_automate_protection.html')"]}, {"scenario": "An education organization wants to use AWS to deliver online learning courses to students globally.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Skills in", "item": "Encrypting data in transit (for example, AWS Certificate Manager [ACM] using TLS)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nAn education organization wants to use AWS to deliver online learning courses to students globally.`\"\n\nContext:\n\n* Store encryption keys and certificates securely and rotate them at appropriate time intervals with strict access control. The best way to accomplish this is to use a managed service, such as AWS Certificate Manager (ACM). It lets you easily provision, manage, and deploy public and private Transport Layer Security (TLS) certificates for use with AWS services and your internal connected resources. TLS certificates are used to secure network communications and establish the identity of websites over the internet as well as resources on private networks. ACM integrates with AWS resources, such as Elastic Load Balancers (ELBs), AWS distributions, and APIs on API Gateway, also handling automatic certificate renewals. If you use ACM to deploy a private root CA, both certificates and private keys can be provided by it for use in Amazon Elastic Compute Cloud (Amazon EC2) instances, containers, and so on.Level of risk exposed if this best practice is not established: HighImplement secure key and certificate management: Implement your defined secure key and certificate management solution.AWS Certificate ManagerHow to host and manage an entire private certificate infrastructure in AWSImplement secure protocols: Use secure protocols that offer authentication and confidentiality, such as Transport Layer Security (TLS) or IPsec, to reduce the risk of data tampering or loss. Check the AWS documentation for the protocols and security relevant to the services that you are using.Related documents:AWS Documentation\n* Enforce your defined encryption requirements based on appropriate standards and recommendations to help you meet your organizational, legal, and compliance requirements. AWS services provide HTTPS endpoints using TLS for communication, thus providing encryption in transit when communicating with the AWS APIs. Insecure protocols, such as HTTP, can be audited and blocked in a VPC through the use of security groups. HTTP requests can also be automatically redirected to HTTPS in Amazon CloudFront or on an Application Load Balancer. You have full control over your computing resources to implement encryption in transit across your services. Additionally, you can use VPN connectivity into your VPC from an external network to facilitate encryption of traffic. Third-party solutions are available in the AWS Marketplace, if you have special requirements.Level of risk exposed if this best practice is not established: HighEnforce encryption in transit: Your defined encryption requirements should be based on the latest standards and best practices and only allow secure protocols. For example, only configure a security group to allow HTTPS protocol to an application load balancer or Amazon Elastic Compute Cloud (Amazon EC2) instance.Configure secure protocols in edge services: Configure HTTPS with Amazon CloudFront and required ciphers.Using HTTPS with CloudFrontUse a VPN for external connectivity: Consider using an IPsec virtual private network (VPN) for securing point-to-point or network-to-network connections to provide both data privacy and integrity.VPN connectionsConfigure secure protocols in load balancers: Enable HTTPS listener for securing connections to load balancers.HTTPS listeners for your application load balancerConfigure secure protocols for instances: Consider configuring HTTPS encryption on instances.Tutorial: Configure Apache web server on Amazon Linux 2 to use SSL/TLSConfigure secure protocols in Amazon Relational Database Service (Amazon RDS): Use secure socket layer (SSL) or transport layer security (TLS) to encrypt connection to database instances.Using SSL to encrypt a connection to a DB InstanceConfigure secure protocols in Amazon Redshift: Configure your cluster to require an secure socket layer (SSL) or transport layer security (TLS) connection.Configure security options for connectionsConfigure secure protocols in additional AWS services For the AWS services you use, determine the encryption-in-transit capabilities.Related documents:AWS documentation\n* Verify the identity of communications by using protocols that support authentication, such as Transport Layer Security (TLS) or IPsec.Using network protocols that support authentication, allows for trust to be established between the parties. This adds to the encryption used in the protocol to reduce the risk of communications being altered or intercepted. Common protocols that implement authentication include Transport Layer Security (TLS), which is used in many AWS services, and IPsec, which is used in AWS Virtual Private Network (AWS VPN).Level of risk exposed if this best practice is not established: LowImplement secure protocols: Use secure protocols that offer authentication and confidentiality, such as TLS or IPsec, to reduce the risk of data tampering or loss. Check the AWS documentation for the protocols and security relevant to the services you are using.Related documents:AWS Documentation\n* Before architecting any system, foundational practices that influence security should be in place. For example, data classification provides a way to categorize organizational data based on levels of sensitivity, and encryption protects data by way of rendering it unintelligible to unauthorized access. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.In AWS, the following practices facilitate protection of data:As an AWS customer you maintain full control over your data.AWS makes it easier for you to encrypt your data and manage keys, including regular key rotation, which can be easily automated by AWS or maintained by you.Detailed logging that contains important content, such as file access and changes, is available.AWS has designed storage systems for exceptional resiliency. For example, Amazon S3 Standard, S3 Standard\u2013IA, S3 One Zone-IA, and Amazon Glacier are all designed to provide 99.999999999% durability of objects over a given year. This durability level corresponds to an average annual expected loss of 0.000000001% of objects.Versioning, which can be part of a larger data lifecycle management process, can protect against accidental overwrites, deletes, and similar harm.AWS never initiates the movement of data between Regions. Content placed in a Region will remain in that Region unless you explicitly enable a feature or leverage a service that provides that functionality.The following questions focus on these considerations for security.AWS provides multiple means for encrypting data at rest and in transit. We build features into our services that make it easier to encrypt your data. For example, we have implemented server-side encryption (SSE) for Amazon S3 to make it easier for you to store your data in an encrypted form. You can also arrange for the entire HTTPS encryption and decryption process (generally known as SSL termination) to be handled by Elastic Load Balancing (ELB).SEC 7: How do you classify your data?Classification provides a way to categorize data, based on criticality and sensitivity in order to help you determine appropriate protection and retention controls.SEC 8: How do you protect your data at rest?Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access or mishandling.SEC 9: How do you protect your data in transit?Protect your data in transit by implementing multiple controls to reduce the risk of unauthorized access or loss.\n\nKnowledge Area: Encrypting data in transit (for example, AWS Certificate Manager [ACM] using TLS)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice for the organization to use to encrypt data in transit, such as certificates and keys, in order to deliver online learning courses to students globally?\", \n\"answer_choices\": [\n{\"answer\": \"Configure secure protocols on the Amazon CloudFront edge service\",\"is_correct\": \"false\",\"explanation\": \"This is not the best option as this is only used to secure connections to the load balancers\"},\n{\"answer\": \"Use an IPsec Virtual Private Network (VPN)\",\"is_correct\": \"false\",\"explanation\": \"This is not the best option as this is only used for point-to-point or network-to-network connections\"},\n{\"answer\": \"Use AWS Certificate Manager (ACM)\",\"is_correct\": \"true\",\"explanation\": \"ACM is the best option for this scenario as it lets you easily provision, manage, and deploy public and private TLS certificates for use with AWS services and connected resources\"},\n{\"answer\": \"Configure secure protocols on instances\",\"is_correct\": \"false\",\"explanation\": \"This is not the best option as this is used for configuring HTTPS encryption, not for encryption in transit\"}\n]}", "docs": ["('SEC09-BP01 Implement secure key and certificate management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_transit_key_cert_mgmt.html')", "('SEC09-BP02 Enforce encryption in transit - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_transit_encrypt.html')", "('SEC09-BP04 Authenticate network communications - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_transit_authentication.html')", "('Data protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-dataprot.html')"]}, {"scenario": "A company wants to build a mobile application that needs to access and store data in the cloud.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Skills in", "item": "Implementing access policies for encryption keys", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to build a mobile application that needs to access and store data in the cloud.`\"\n\nContext:\n\n* Enforce access control with least privileges and mechanisms, including backups, isolation, and versioning, to help protect your data at rest. Prevent operators from granting public access to your data.Different controls including access (using least privilege), backups (see Reliability whitepaper), isolation, and versioning can all help protect your data at rest. Access to your data should be audited using detective mechanisms covered earlier in this paper including CloudTrail, and service level log, such as Amazon Simple Storage Service (Amazon S3) access logs. You should inventory what data is publicly accessible, and plan for how you can reduce the amount of data available over time. Amazon S3 Glacier Vault Lock and Amazon S3 Object Lock are capabilities providing mandatory access control\u2014once a vault policy is locked with the compliance option, not even the root user can change it until the lock expires. The mechanism meets the Books and Records Management requirements of the SEC, CFTC, and FINRA. For more details, see this whitepaper.Level of risk exposed if this best practice is not established: LowEnforce access control: Enforce access control with least privileges, including access to encryption keys.Introduction to Managing Access Permissions to Your Amazon S3 ResourcesSeparate data based on different classification levels: Use different AWS accounts for data classification levels managed by AWS Organizations.AWS OrganizationsReview AWS KMS policies: Review the level of access granted in AWS KMS policies.Overview of managing access to your AWS KMS resourcesReview Amazon S3 bucket and object permissions: Regularly review the level of access granted in Amazon S3 bucket policies. Best practice is to not have publicly readable or writeable buckets. Consider using AWS Config to detect buckets that are publicly available, and Amazon CloudFront to serve content from Amazon S3.AWS Config RulesAmazon S3 + Amazon CloudFront: A Match Made in the CloudEnable Amazon S3 versioning and object lock.Using versioningLocking Objects Using Amazon S3 Object LockUse Amazon S3 Inventory: Amazon S3 inventory is one of the tools you can use to audit and report on the replication and encryption status of your objects.Amazon S3 InventoryReview Amazon EBS and AMI sharing permissions: Sharing permissions can allow images and volumes to be shared to AWS accounts external to your workload.Sharing an Amazon EBS SnapshotShared AMIsRelated documents:AWS KMS Cryptographic Details WhitepaperRelated videos:Securing Your Block Storage on AWS\n* Control and detect access to backups using authentication and authorization, such as AWS IAM. Prevent and detect if data integrity of backups is compromised using encryption.Amazon S3 supports several methods of encryption of your data at rest. Using server-side encryption, Amazon S3 accepts your objects as unencrypted data, and then encrypts them as they are stored. Using client-side encryption, your workload application is responsible for encrypting the data before it is sent to Amazon S3. Both methods allow you to use AWS Key Management Service (AWS KMS) to create and store the data key, or you can provide your own key, which you are then responsible for. Using AWS KMS, you can set policies using IAM on who can and cannot access your data keys and decrypted data.For Amazon RDS, if you have chosen to encrypt your databases, then your backups are encrypted also. DynamoDB backups are always encrypted.Common anti-patterns:Having the same access to the backups and restoration automation as you do to the data.Not encrypting your backups.Benefits of establishing this best practice: Securing your backups prevents tampering with the data, and encryption of the data prevents access to that data if it is accidentally exposed.Level of risk exposed if this best practice is not established: HighUse encryption on each of your data stores. If your source data is encrypted, then the backup will also be encrypted.Enable encryption in RDS. You can configure encryption at rest using AWS Key Management Service when you create an RDS instance.Encrypting Amazon RDS ResourcesEnable encryption on EBS volumes. You can configure default encryption or specify a unique key upon volume creation.Amazon EBS EncryptionUse the required Amazon DynamoDB encryption. DynamoDB encrypts all data at rest. You can either use an AWS owned AWS KMS key or an AWS managed KMS key, specifying a key that is stored in your account.DynamoDB Encryption at RestManaging Encrypted TablesEncrypt your data stored in Amazon EFS. Configure the encryption when you create your file system.Encrypting Data and Metadata in EFSConfigure the encryption in the source and destination Regions. You can configure encryption at rest in Amazon S3 using keys stored in KMS, but the keys are Region-specific. You can specify the destination keys when you configure the replication.CRR Additional Configuration: Replicating Objects Created with Server-Side Encryption (SSE) Using Encryption Keys stored in AWS KMSImplement least privilege permissions to access your backups. Follow best practices to limit the access to the backups, snapshots, and replicas in accordance with security best practices.Security Pillar: AWS Well-ArchitectedRelated documents:AWS Marketplace: products that can be used for backupAmazon EBS EncryptionAmazon S3: Protecting Data Using EncryptionCRR Additional Configuration: Replicating Objects Created with Server-Side Encryption (SSE) Using Encryption Keys stored in AWS KMSDynamoDB Encryption at RestEncrypting Amazon RDS ResourcesEncrypting Data and Metadata in EFSEncryption for Backups in AWSManaging Encrypted TablesSecurity Pillar: AWS Well-ArchitectedRelated examples:Well-Architected lab: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon S3\n\nKnowledge Area: Implementing access policies for encryption keys\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best practice for implementing access policies for encryption keys in the provided scenario?\",\n\"answer_choices\": [\n    {\"answer\": \"Use the same access permissions to the backups and restoration automation as the data\",\"is_correct\": \"false\",\"explanation\": \"This is an anti-pattern as it would not provide least privilege permissions for accessing the backups.\"},\n    {\"answer\": \"Use AWS Key Management Service (AWS KMS) to create and store the data key\",\"is_correct\": \"true\",\"explanation\": \"Using AWS KMS, you can set policies using IAM on who can and cannot access your data keys and decrypted data.\"},\n    {\"answer\": \"Ensure backups are not publicly readable or writeable\",\"is_correct\": \"false\",\"explanation\": \"This is one of the best practices, but not the best practice for implementing access policies for encryption keys.\"},\n    {\"answer\": \"Use AWS Organizations to separate data based on different classification levels\",\"is_correct\": \"false\",\"explanation\": \"Using AWS Organizations is one of the best practices, but not the best practice for implementing access policies for encryption keys.\"}\n]}", "docs": ["('SEC08-BP04 Enforce access control - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_access_control.html')", "('REL09-BP02 Secure and encrypt backups - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_backing_up_data_secured_backups_data.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Skills in", "item": "Implementing data backups and replications", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO).Best practices\n* All AWS data stores offer backup capabilities. Services such as Amazon RDS and Amazon DynamoDB additionally support automated backup that enables point-in-time recovery (PITR), which allows you to restore a backup to any time up to five minutes or less before the current time. Many AWS services offer the ability to copy backups to another AWS Region. AWS Backup is a tool that gives you the ability to centralize and automate data protection across AWS services.Amazon S3 can be used as a backup destination for self-managed and AWS-managed data sources. AWS services such as Amazon EBS, Amazon RDS, and Amazon DynamoDB have built in capabilities to create backups. Third-party backup software can also be used.On-premises data can be backed up to the AWS Cloud using AWS Storage Gateway or AWS DataSync. Amazon S3 buckets can be used to store this data on AWS. Amazon S3 offers multiple storage tiers such as Amazon S3 Glacier or S3 Glacier Deep Archive to reduce cost of data storage.You might be able to meet data recovery needs by reproducing the data from other sources. For example, Amazon Elasticache replica nodes or RDS read replicas could be used to reproduce data if the primary is lost. In cases where sources like this can be used to meet your Recovery Point Objective (RPO) and Recovery Time Objective (RTO), you might not require a backup. Another example, if working with Amazon EMR, it might not be necessary to backup your HDFS data store, as long as you can reproduce the data into EMR from S3.When selecting a backup strategy, consider the time it takes to recover data. The time needed to recover data depends on the type of backup (in the case of a backup strategy), or the complexity of the data reproduction mechanism. This time should fall within the RTO for the workload.Desired Outcome:Data sources have been identified and classified based on criticality. Then, establish a strategy for data recovery based on the RPO. This strategy involves either backing up these data sources, or having the ability to reproduce data from other sources. In the case of data loss, the strategy implemented enables recovery or reproduction of data within the defined RPO and RTO.Cloud Maturity Phase: FoundationalCommon anti-patterns:Not aware of all data sources for the workload and their criticality.Not taking backups of critical data sources.Taking backups of only some data sources without using criticality as a criterion.No defined RPO, or backup frequency cannot meet RPO.Not evaluating if a backup is necessary or if data can be reproduced from other sources.Benefits of establishing this best practice: Identifying the places where backups are necessary and implementing a mechanism to create backups, or being able to reproduce the data from an external source improves the ability to restore and recover data during an outage.Level of risk exposed if this best practice is not established: HighUnderstand and use the backup capabilities of the AWS services and resources used by the workload. Most AWS services provides capabilities to back up workload data.Implementation Steps:Identify all data sources for the workload. Data can be stored on a number of resources such as databases, volumes, filesystems, logging systems, and object storage. Refer to the Resources section to find Related documents on different AWS services where data is stored, and the backup capability these services provide.Classify data sources based on criticality. Different data sets will have different levels of criticality for a workload, and therefore different requirements for resiliency. For example, some data might be critical and require a RPO near zero, while other data might be less critical and can tolerate a higher RPO and some data loss. Similarly, different data sets might have different RTO requirements as well.Use AWS or third-party services to create backups of the data. AWS Backup is a managed service that enables creating backups of various data sources on AWS. Most of these services also have native capabilities to create backups. The AWS Marketplace has many solutions that provide these capabilites as well. Refer to the Resources listed below for information on how to create backups of data from various AWS services.For data that is not backed up, establish a data reproduction mechanism. You might choose not to backup data that can be reproduced from other sources for various reasons. There might be a situation where it is cheaper to reproduce data from sources when needed rather than creating a backup as there may be a cost associated with storing backups. Another example is where restoring from a backup takes longer than reproducing the data from sources, resulting in a breach in RTO. In such situations, consider tradeoffs and establish a well-defined process for how data can be reproduced from these sources when data recovery is necessary. For example, if you have loaded data from Amazon S3 to a data warehouse (like Amazon Redshift), or MapReduce cluster (like Amazon EMR) to do analysis on that data, this may be an example of data that can be reproduced from other sources. As long as the results of these analyses are either stored somewhere or reproducible, you would not suffer a data loss from a failure in the data warehouse or MapReduce cluster. Other examples that can be reproduced from sources include caches (like Amazon ElastiCache) or RDS read replicas.Establish a cadence for backing up data. Creating backups of data sources is a periodic process and the frequency should depend on the RPO.Level of effort for the Implementation Plan: ModerateRelated Best Practices:REL13-BP01 Define recovery objectives for downtime and data lossREL13-BP02 Use defined recovery strategies to meet the recovery objectivesRelated documents:What Is AWS Backup?What is AWS DataSync?What is Volume Gateway?APN Partner: partners that can help with backupAWS Marketplace: products that can be used for backupAmazon EBS SnapshotsBacking Up Amazon EFSBacking up Amazon FSx for Windows File ServerBackup and Restore for ElastiCache for RedisCreating a DB Cluster Snapshot in NeptuneCreating a DB SnapshotCreating an EventBridge Rule That Triggers on a ScheduleCross-Region Replication with Amazon S3EFS-to-EFS AWS BackupExporting Log Data to Amazon S3Object lifecycle managementOn-Demand Backup and Restore for DynamoDBPoint-in-time recovery for DynamoDBWorking with Amazon OpenSearch Service Index SnapshotsRelated videos:AWS re:Invent 2021 - Backup, disaster recovery, and ransomware protection with AWSAWS Backup Demo: Cross-Account and Cross-Region BackupAWS re:Invent 2019: Deep dive on AWS Backup, ft. Rackspace (STG341)Related examples:Well-Architected lab: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon S3Well-Architected lab: Testing Backup and Restore of DataWell-Architected lab: Backup and Restore with Failback for Analytics WorkloadWell-Architected lab: Disaster Recovery - Backup and Restore\n\nKnowledge Area: Implementing data backups and replications\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice for backing up critical data sources in the financial services company scenario described above?\",\n\"answer_choices\": [\n    {\"answer\": \"Use AWS Backup to back up data to Amazon S3 buckets and use Amazon S3 Glacier or S3 Glacier Deep Archive to reduce cost of data storage.\",\"is_correct\": \"True\",\"explanation\": \"AWS Backup is a tool that gives you the ability to centralize and automate data protection across AWS services. Amazon S3 can be used as a backup destination for self-managed and AWS-managed data sources. It offers multiple storage tiers such as Amazon S3 Glacier or S3 Glacier Deep Archive to reduce cost of data storage.\"},\n    {\"answer\": \"Use Amazon EMR to store data on HDFS and back up the data store to AWS.\",\"is_correct\": \"False\",\"explanation\": \"In cases where sources like Amazon EMR can be used to meet your Recovery Point Objective (RPO) and Recovery Time Objective (RTO), you might not require a backup if you can reproduce the data into EMR from S3.\"},\n    {\"answer\": \"Back up all data sources to AWS using AWS Storage Gateway or AWS DataSync.\",\"is_correct\": \"False\",\"explanation\": \"On-premises data can be backed up to the AWS Cloud using AWS Storage Gateway or AWS DataSync. Amazon S3 buckets can be used to store this data on AWS. However, only critical data sources should be backed up as some data might be able to be reproduced from other sources.\"},\n    {\"answer\": \"Take manual backups of data sources at a frequency that meets the Recovery Point Objective (RPO) for the workload.\",\"is_correct\": \"False\",\"explanation\": \"Most AWS services provides capabilities to back up workload data. AWS Backup is a managed service that enables creating backups of various data sources on AWS. These services also have native capabilities to create backups. Establish a cadence for backing up data that meets the Recovery Point Objective (RPO) for the workload.\"}\n]}", "docs": ["('REL 9\\xa0 How do you back up data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-09.html')", "('REL09-BP01 Identify and back up all data that needs to be backed up, or reproduce the data from sources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_backing_up_data_identified_backups_data.html')"]}, {"scenario": "A marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Skills in", "item": "Implementing policies for data access, lifecycle, and protection", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.`\"\n\nContext:\n\n* Manage the lifecycle of all your data and automatically enforce deletion timelines to minimize the total storage requirements of your workload.Level of risk exposed if this best practice is not established: LowDefine lifecycle policies for all your data classification types.Set automated lifecycle policies to enforce lifecycle rules.Delete unused volumes and snapshots.Aggregate data where applicable based on lifecycle rules.Related documents:Amazon ECR Lifecycle policiesAmazon EFS lifecycle managementAmazon S3 Intelligent-TieringEvaluating Resources with AWS Config RulesManaging your storage lifecycle on Amazon S3Object lifecycle policies in AWS Elemental MediaStoreRelated videos:Amazon S3 Lifecycle\n* Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.Best practices:\n* Your defined lifecycle strategy should be based on sensitivity level as well as legal and organization requirements. Aspects including the duration for which you retain data, data destruction processes, data access management, data transformation, and data sharing should be considered. When choosing a data classification methodology, balance usability versus access. You should also accommodate the multiple levels of access and nuances for implementing a secure, but still usable, approach for each level. Always use a defense in depth approach and reduce human access to data and mechanisms for transforming, deleting, or copying data. For example, require users to strongly authenticate to an application, and give the application, rather than the users, the requisite access permission to perform action at a distance. In addition, ensure that users come from a trusted network path and require access to the decryption keys. Use tools, such as dashboards and automated reporting, to give users information from the data rather than giving them direct access to the data.Level of risk exposed if this best practice is not established: LowIdentify data types: Identify the types of data that you are storing or processing in your workload. That data could be text, images, binary databases, and so forth.Related documents:Data Classification WhitepaperGetting started with Amazon MacieRelated videos:Introducing the New Amazon Macie\n* Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access or mishandling.Best practices\n* Integrate access controls with operator and application lifecycle and your centralized federation provider. For example, remove a user\u2019s access when they leave the organization or change roles.As you manage workloads using separate accounts, there will be cases where you need to share resources between those accounts. We recommend that you share resources using AWS Resource Access Manager (AWS RAM). This service enables you to easily and securely share AWS resources within your AWS Organizations and Organizational Units. Using AWS RAM, access to shared resources is automatically granted or revoked as accounts are moved in and out of the Organization or Organization Unit with which they are shared. This helps ensure that resources are only shared with the accounts that you intend.Level of risk exposed if this best practice is not established: LowImplement a user access lifecycle policy for new users joining, job function changes, and users leaving so that only current users have access.Related documents:Attribute-based access control (ABAC)Grant least privilegeIAM Access AnalyzerRemove unnecessary credentialsWorking with PoliciesRelated videos:Become an IAM Policy Master in 60 Minutes or LessSeparation of Duties, Least Privilege, Delegation, and CI/CD\n* Before architecting any system, foundational practices that influence security should be in place. For example, data classification provides a way to categorize organizational data based on levels of sensitivity, and encryption protects data by way of rendering it unintelligible to unauthorized access. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.In AWS, the following practices facilitate protection of data:As an AWS customer you maintain full control over your data.AWS makes it easier for you to encrypt your data and manage keys, including regular key rotation, which can be easily automated by AWS or maintained by you.Detailed logging that contains important content, such as file access and changes, is available.AWS has designed storage systems for exceptional resiliency. For example, Amazon S3 Standard, S3 Standard\u2013IA, S3 One Zone-IA, and Amazon Glacier are all designed to provide 99.999999999% durability of objects over a given year. This durability level corresponds to an average annual expected loss of 0.000000001% of objects.Versioning, which can be part of a larger data lifecycle management process, can protect against accidental overwrites, deletes, and similar harm.AWS never initiates the movement of data between Regions. Content placed in a Region will remain in that Region unless you explicitly enable a feature or leverage a service that provides that functionality.The following questions focus on these considerations for security.AWS provides multiple means for encrypting data at rest and in transit. We build features into our services that make it easier to encrypt your data. For example, we have implemented server-side encryption (SSE) for Amazon S3 to make it easier for you to store your data in an encrypted form. You can also arrange for the entire HTTPS encryption and decryption process (generally known as SSL termination) to be handled by Elastic Load Balancing (ELB).SEC 7: How do you classify your data?Classification provides a way to categorize data, based on criticality and sensitivity in order to help you determine appropriate protection and retention controls.SEC 8: How do you protect your data at rest?Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access or mishandling.SEC 9: How do you protect your data in transit?Protect your data in transit by implementing multiple controls to reduce the risk of unauthorized access or loss.\n\nKnowledge Area: Implementing policies for data access, lifecycle, and protection\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following practices is a best practice for protecting data in transit?\", \"answer_choices\": [{\"answer\": \"Integrate access controls with operator and application lifecycle and your centralized federation provider.\",\"is_correct\": \"false\",\"explanation\": \"This best practice is for protecting data at rest.\"},{\"answer\": \"Require users to authenticate to an application.\",\"is_correct\": \"true\",\"explanation\": \"Requiring users to authenticate to an application and giving the application the requisite access permission to perform action at a distance is an effective way to protect data in transit.\"},{\"answer\": \"Use automated lifecycle policies to enforce lifecycle rules.\",\"is_correct\": \"false\",\"explanation\": \"This best practice is for managing the lifecycle of data.\"},{\"answer\": \"Use tools such as dashboards and automated reporting to give users information from the data.\",\"is_correct\": \"false\",\"explanation\": \"This best practice is for protecting data at rest.\"}]}", "docs": ["('SUS04-BP03 Use lifecycle policies to delete unnecessary data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a4.html')", "('SUS 4 How do you take advantage of data access and usage patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-04.html')", "('SEC07-BP04 Define data lifecycle management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_data_classification_lifecycle_management.html')", "('SEC 8\\xa0 How do you protect your data at rest? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-08.html')", "('SEC03-BP06 Manage access based on lifecycle - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_lifecycle.html')", "('Data protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-dataprot.html')"]}, {"scenario": "A startup wants to host their new web application on AWS.", "domain": "Domain 1: Design Secure Architectures", "task": "Task Statement 3: Determine appropriate data security controls", "focus": "Skills in", "item": "Rotating encryption keys and renewing certificates", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA startup wants to host their new web application on AWS.`\"\n\nContext:\n\n* Store encryption keys and certificates securely and rotate them at appropriate time intervals with strict access control. The best way to accomplish this is to use a managed service, such as AWS Certificate Manager (ACM). It lets you easily provision, manage, and deploy public and private Transport Layer Security (TLS) certificates for use with AWS services and your internal connected resources. TLS certificates are used to secure network communications and establish the identity of websites over the internet as well as resources on private networks. ACM integrates with AWS resources, such as Elastic Load Balancers (ELBs), AWS distributions, and APIs on API Gateway, also handling automatic certificate renewals. If you use ACM to deploy a private root CA, both certificates and private keys can be provided by it for use in Amazon Elastic Compute Cloud (Amazon EC2) instances, containers, and so on.Level of risk exposed if this best practice is not established: HighImplement secure key and certificate management: Implement your defined secure key and certificate management solution.AWS Certificate ManagerHow to host and manage an entire private certificate infrastructure in AWSImplement secure protocols: Use secure protocols that offer authentication and confidentiality, such as Transport Layer Security (TLS) or IPsec, to reduce the risk of data tampering or loss. Check the AWS documentation for the protocols and security relevant to the services that you are using.Related documents:AWS Documentation\n* For workforce and machine identities that require secrets such as passwords to third-party applications, store them with automatic rotation using the latest industry standards in a specialized service, such as for credentials that are not IAM-related and cannot take advantage of temporary credentials, such as database logins, use a service that is designed to handle management of secrets, such as AWS Secrets Manager. Secrets Manager makes it easy to manage, rotate, and securely store encrypted secrets using supported services. Calls to access the secrets are logged in AWS CloudTrail for auditing purposes, and IAM permissions can grant least-privilege access to them.Level of risk exposed if this best practice is not established: HighUse AWS Secrets Manager: AWS Secrets Manager is an AWS service that makes it easier for you to manage secrets. Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text.Related documents:Getting Started with AWS Secrets ManagerIdentity Providers and FederationRelated videos:Best Practices for Managing, Retrieving, and Rotating Secrets at Scale\n* By defining an encryption approach that includes the storage, rotation, and access control of keys, you can help provide protection for your content against unauthorized users and against unnecessary exposure to authorized users. AWS Key Management Service (AWS KMS) helps you manage encryption keys and integrates with many AWS services. This service provides durable, secure, and redundant storage for your AWS KMS keys. You can define your key aliases as well as key-level policies. The policies help you define key administrators as well as key users. Additionally, AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys in the AWS Cloud. It helps you meet corporate, contractual, and regulatory compliance requirements for data security by using FIPS 140-2 Level 3 validated HSMs.Level of risk exposed if this best practice is not established: HighImplement AWS KMS: AWS KMS makes it easy for you to create and manage keys and control the use of encryption across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses FIPS 140-2 validated hardware security modules to protect your keys.Getting started: AWS Key Management Service (AWS KMS)Consider AWS Encryption SDK: Use the AWS Encryption SDK with AWS KMS integration when your application needs to encrypt data client-side.AWS Encryption SDKRelated documents:AWS Key Management ServiceAWS cryptographic services and toolsGetting started: AWS Key Management Service (AWS KMS)Protecting Amazon S3 Data Using EncryptionRelated videos:How Encryption Works in AWSSecuring Your Block Storage on AWS\n* When you cannot rely on temporary credentials and require long-term credentials, audit credentials to ensure that the defined controls for example, multi-factor authentication (MFA), are enforced, rotated regularly, and have the appropriate access level. Periodic validation, preferably through an automated tool, is necessary to verify that the correct controls are enforced. For human identities, you should require users to change their passwords periodically and retire access keys in favor of temporary credentials. As you are moving from AWS Identity and Access Management (IAM) users to centralized identities, you can generate a credential report to audit your IAM users. We also recommend that you enforce MFA settings in your identity provider. You can set up AWS Config Rules to monitor these settings. For machine identities, you should rely on temporary credentials using IAM roles. For situations where this is not possible, frequent auditing and rotating access keys is necessary.Level of risk exposed if this best practice is not established: MediumRegularly audit credentials: Use credential reports, and Identify and Access Management (IAM) Access Analyzer to audit IAM credentials and permissions.IAM Access AnalyzerGetting credential reportLab: Automated IAM user cleanupUse Access Levels to Review IAM Permissions: To improve the security of your AWS account, regularly review and monitor each of your IAM policies. Make sure that your policies grant the least privilege that is needed to perform only the necessary actions.Use access levels to review IAM permissionsConsider automating IAM resource creation and updates: AWS CloudFormation can be used to automate the deployment of IAM resources, including roles and policies, to reduce human error because the templates can be verified and version controlled.Lab: Automated deployment of IAM groups and rolesRelated documents:Getting Started with AWS Secrets ManagerIAM Best PracticesIdentity Providers and FederationSecurity Partner Solutions: Access and Access ControlTemporary Security CredentialsRelated videos:Best Practices for Managing, Retrieving, and Rotating Secrets at ScaleManaging user permissions at scale with AWS IAM Identity Center (successor to AWS Single Sign-On)Mastering identity at every layer of the cake\n\nKnowledge Area: Rotating encryption keys and renewing certificates\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS Service should a startup use to securely store encryption keys and certificates, and rotate them at appropriate time intervals with strict access control?\", \"answer_choices\": [{\"answer\": \"AWS CloudFormation\",\"is_correct\": \"false\",\"explanation\": \"AWS CloudFormation is used to automate the deployment of IAM resources, including roles and policies, but does not provide encryption key and certificate management.\"},{\"answer\": \"AWS Secrets Manager\",\"is_correct\": \"false\",\"explanation\": \"AWS Secrets Manager is used to store secrets such as database credentials, passwords, third-party API keys, and even arbitrary text, but does not provide encryption key and certificate management.\"},{\"answer\": \"AWS Key Management Service (AWS KMS)\",\"is_correct\": \"false\",\"explanation\": \"AWS KMS is used to manage encryption keys and integrate with many AWS services, but does not provide certificate management.\"},{\"answer\": \"AWS Certificate Manager (ACM)\",\"is_correct\": \"true\",\"explanation\": \"AWS Certificate Manager (ACM) lets you easily provision, manage, and deploy public and private Transport Layer Security (TLS) certificates for use with AWS services and your internal connected resources, and can handle automatic certificate renewals.\"}]}", "docs": ["('SEC09-BP01 Implement secure key and certificate management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_transit_key_cert_mgmt.html')", "('SEC02-BP03 Store and use secrets securely - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_identities_secrets.html')", "('SEC08-BP01 Implement secure key management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_key_mgmt.html')", "('SEC02-BP05 Audit and rotate credentials periodically - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_identities_audit.html')"]}, {"scenario": "A company has a critical application that needs to be up at all times", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "API creation and management (for example, Amazon API Gateway, REST API)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company has a critical application that needs to be up at all times`\"\n\nContext:\n\n* Service contracts are documented agreements between teams on service integration and include a machine-readable API definition, rate limits, and performance expectations. A versioning strategy allows your clients to continue using the existing API and migrate their applications to the newer API when they are ready. Deployment can happen anytime, as long as the contract is not violated. The service provider team can use the technology stack of their choice to satisfy the API contract. Similarly, the service consumer can use their own technology.Microservices take the concept of service-oriented architecture (SOA) to the point of creating services that have a minimal set of functionality. Each service publishes an API and design goals, limits, and other considerations for using the service. This establishes a contract with calling applications. This accomplishes three main benefits:The service has a concise business problem to be served and a small team that owns the business problem. This allows for better organizational scaling.The team can deploy at any time as long as they meet their API and other contract requirements.The team can use any technology stack they want to as long as they meet their API and other contract requirements.Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. Using OpenAPI Specification (OAS), formerly known as the Swagger Specification, you can define your API contract and import it into API Gateway. With API Gateway, you can then version and deploy the APIs.Level of risk exposed if this best practice is not established: LowProvide service contracts per API Service contracts are documented agreements between teams on service integration and include a machine-readable API definition, rate limits, and performance expectations.Amazon API Gateway: Configuring a REST API Using OpenAPIA versioning strategy allows clients to continue using the existing API and migrate their applications to the newer API when they are ready.Amazon API Gateway is a fully managed service that makes it easy for developers to create APIs at any scale. Using the OpenAPI Specification (OAS), formerly known as the Swagger Specification, you can define your API contract and import it into API Gateway. With API Gateway, you can then version and deploy the APIs.Related documents:Amazon API Gateway: Configuring a REST API Using OpenAPIBounded Context (a central pattern in Domain-Driven Design)Implementing Microservices on AWSMicroservice Trade-OffsMicroservices - a definition of this new architectural termMicroservices on AWS\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n\nKnowledge Area: API creation and management (for example, Amazon API Gateway, REST API)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice when creating an API for a company's critical application that needs to stay up at all times?\", \n\"answer_choices\": [{\"answer\": \"Use Amazon RDS to automate administrative tasks such as hardware provisioning, database setup, patching, and backups\",\"is_correct\": \"True\",\"explanation\": \"Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups, which can reduce the amount of time needed for security maintenance.\"}, \n{\"answer\": \"Use AWS Lambda to run code without provisioning or managing servers\",\"is_correct\": \"True\",\"explanation\": \"AWS Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.\"}, \n{\"answer\": \"Define an API contract and import it into Amazon API Gateway to version and deploy the APIs\",\"is_correct\": \"True\",\"explanation\": \"Using OpenAPI Specification (OAS), formerly known as the Swagger Specification, you can define your API contract and import it into Amazon API Gateway. With Amazon API Gateway, you can then version and deploy the APIs.\"}, \n{\"answer\": \"Implement services that manage resources using Amazon EC2 Systems Manager\",\"is_correct\": \"False\",\"explanation\": \"Amazon EC2 Systems Manager can be used to replace a Bastion Host, but it is not the best practice for implementing services that manage resources. Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS) are more suitable for managing resources.\"}]}", "docs": ["('REL03-BP03 Provide service contracts per API - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_service_architecture_api_contracts.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "AWS managed services with appropriate use cases (for example, AWS Transfer Family, Amazon Simple Queue Service [Amazon SQS], Secrets Manager)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n* Managed services shift responsibility for maintaining high-average utilization, and sustainability optimization of the deployed hardware to AWS. Use managed services to distribute the sustainability impact of the service across all tenants of the service, reducing your individual contribution.Level of risk exposed if this best practice is not established: LowMigrate from self-hosted services to managed services. For example, use managed Amazon Relational Database Service (Amazon RDS) instances instead of maintaining your own Amazon RDS instances on Amazon Elastic Compute Cloud (Amazon EC2), or use managed container services, such as AWS Fargate, instead of implementing your own container infrastructure.Related documents:AWS FargateAmazon DocumentDBAmazon Elastic Kubernetes Service (EKS)Amazon Managed Streaming for Apache Kafka (Amazon MSK)Amazon RedshiftAmazon Relational Database Service (RDS)\n* For workforce and machine identities that require secrets such as passwords to third-party applications, store them with automatic rotation using the latest industry standards in a specialized service, such as for credentials that are not IAM-related and cannot take advantage of temporary credentials, such as database logins, use a service that is designed to handle management of secrets, such as AWS Secrets Manager. Secrets Manager makes it easy to manage, rotate, and securely store encrypted secrets using supported services. Calls to access the secrets are logged in AWS CloudTrail for auditing purposes, and IAM permissions can grant least-privilege access to them.Level of risk exposed if this best practice is not established: HighUse AWS Secrets Manager: AWS Secrets Manager is an AWS service that makes it easier for you to manage secrets. Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text.Related documents:Getting Started with AWS Secrets ManagerIdentity Providers and FederationRelated videos:Best Practices for Managing, Retrieving, and Rotating Secrets at Scale\n\nKnowledge Area: AWS managed services with appropriate use cases (for example, AWS Transfer Family, Amazon Simple Queue Service [Amazon SQS], Secrets Manager)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which managed service should the transportation company use to process real-time data from connected vehicles to optimize their operations?\", \"answer_choices\": [{\"answer\": \"Amazon RDS\",\"is_correct\": \"False\",\"explanation\": \"Amazon RDS is a managed relational database service, which is not suitable for real-time processing of data from connected vehicles. \"},{\"answer\": \"Amazon DynamoDB\",\"is_correct\": \"False\",\"explanation\": \"Amazon DynamoDB is a managed NoSQL database service, which is not suitable for real-time processing of data from connected vehicles. \"},{\"answer\": \"AWS Lambda\",\"is_correct\": \"True\",\"explanation\": \"AWS Lambda is a managed compute service that can be used for real-time processing of data from connected vehicles. \"},{\"answer\": \"Amazon ECS\",\"is_correct\": \"False\",\"explanation\": \"Amazon ECS is a managed container service, which is not suitable for real-time processing of data from connected vehicles. \"}]}", "docs": ["('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')", "('SUS05-BP03 Use managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a4.html')", "('SEC02-BP03 Store and use secrets securely - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_identities_secrets.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Caching strategies", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* Where applicable, use multiple strategies to improve performance. For example, using strategies like caching data to prevent excessive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Common anti-patterns:You assume that workload performance is adequate if customers are not complaining.You only collect data on performance after you have made performance-related changes.Benefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and historical reference. You can create dashboards and perform metric math on your data to derive operational and utilization insights for your workloads as they change over time.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Related documents:Amazon Builders\u2019 LibraryBest Practices for Implementing Amazon ElastiCacheAWS Database CachingAmazon CloudWatch RUMDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)AWS purpose-built databases (DAT209-L)Optimize applications through Amazon CloudWatch RUMRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n* Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload. Fetch data from shared storage only as needed. Detach unused volumes to make more resources available.Level of risk exposed if this best practice is not established: LowMigrate data to shared storage when the data has multiple consumers.Fetch data from shared storage only as needed.Delete data as appropriate for your usage patterns, and implement time-to-live (TTL) functionality to manage cached data.Detach volumes from clients that are not actively using them.Related documents:Amazon FSxCaching strategiesWhat is Amazon Elastic File System?What is Amazon S3?\n* Mechanisms exist for your team members to discover the information that they are looking for in a timely manner, access it, and identify that it\u2019s current and complete. Mechanisms are present to identify needed content, content in need of refresh, and content that should be archived so that it\u2019s no longer referenced.Common anti-patterns:A single frustrated customer opens a support case for a new product feature request to address a perceived issue. It is added to the list of priority improvements.Level of risk exposed if this best practice is not established: HighKnowledge management: Ensure mechanisms exist for your team members to discover the information that they are looking for in a timely manner, access it, and identify that it\u2019s current and complete. Maintain mechanisms to identify needed content, content in need of refresh, and content that should be archived so that it\u2019s no longer referenced.\n* When evaluating performance-related improvements, determine which choices will impact your customers and workload efficiency. For example, if using a key-value data store increases system performance, it is important to evaluate how the eventually consistent nature of it will impact customers.Identify areas of poor performance in your system through metrics and monitoring. Determine how you can make improvements, what trade-offs those improvements bring, and how they impact the system and the user experience. For example, implementing caching data can help dramatically improve performance but requires a clear strategy for how and when to update or invalidate cached data to prevent incorrect system behavior.Common anti-patterns:You assume that all performance gains should be implemented, even if there are tradeoffs for implementation such as eventual consistency.You only evaluate changes to workloads when a performance issue has reached a critical point.Benefits of establishing this best practice: When you are evaluating potential performance-related improvements, you must decide if the tradeoffs for the changes are consistent with the workload requirements. In some cases, you may have to implement additional controls to compensate for the tradeoffs.Level of risk exposed if this best practice is not established: HighIdentify tradeoffs: Use metrics and monitoring to identify areas of poor performance in your system. Determine how to make improvements, and how tradeoffs will impact the system and the user experience. For example, implementing caching data can help dramatically improve performance, but it requires a clear strategy for how and when to update or invalidate cached data to prevent incorrect system behavior.Related documents:Amazon Builders\u2019 LibraryAmazon QuickSight KPIsAmazon CloudWatch RUMX-Ray DocumentationRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)Build a Monitoring PlanOptimize applications through Amazon CloudWatch RUMDemo of Amazon CloudWatch SyntheticsRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web Client\n* When architecting solutions, determining tradeoffs enables you to select an optimal approach. Often you can improve performance by trading consistency, durability, and space for time and latency.Best practices\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n\nKnowledge Area: Caching strategies\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best approach to implement caching strategies for an e-commerce platform during peak shopping seasons?\", \n\"answer_choices\": [\n{\"answer\": \"Use a single source of truth to avoid data duplication and reduce total storage requirements.\",\"is_correct\": \"False\",\"explanation\": \"This is a good approach to reduce storage requirements, but it does not involve caching which is necessary to improve performance during peak shopping seasons.\"}, \n{\"answer\": \"Fetch data from shared storage and delete it as appropriate for your usage patterns.\",\"is_correct\": \"False\",\"explanation\": \"This is a good approach to reduce storage requirements, but it does not involve caching which is necessary to improve performance during peak shopping seasons.\"},\n{\"answer\": \"Implement a time-to-live (TTL) functionality to manage cached data.\",\"is_correct\": \"True\",\"explanation\": \"Implementing a TTL functionality to manage cached data is a good approach to improve performance during peak shopping seasons as it will allow data to be stored in a cache memory rather than hitting the database every time.\"}, \n{\"answer\": \"Detach volumes from clients that are not actively using them.\",\"is_correct\": \"False\",\"explanation\": \"This is a good approach to reduce storage requirements, but it does not involve caching which is necessary to improve performance during peak shopping seasons.\"}\n]}", "docs": ["('PERF08-BP05 Use various performance-related strategies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_implement_strategy.html')", "('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')", "('SUS04-BP06 Use shared file systems or object storage to access common data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a7.html')", "('OPS11-BP04 Perform knowledge management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_evolve_ops_knowledge_management.html')", "('PERF08-BP03 Identify how tradeoffs impact customers and efficiency - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_understand_impact.html')", "('PERF 8\\xa0 How do you use tradeoffs to improve performance? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-08.html')", "('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Design principles for microservices (for example, stateless workloads compared with stateful workloads)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Workload segmentation is important when determining the resilience requirements of your application. Monolithic architecture should be avoided whenever possible. Instead, carefully consider which application components can be broken out into microservices. Depending on your application requirements, this may end up being a combination of a service-oriented architecture (SOA) with microservices where possible. Workloads that are capable of statelessness are more capable of being deployed as microservices.Desired outcome: Workloads should be supportable, scalable, and as loosely coupled as possible.When making choices about how to segment your workload, balance the benefits against the complexities. What is right for a new product racing to first launch is different than what a workload built to scale from the start needs. When refactoring an existing monolith, you will need to consider how well the application will support a decomposition towards statelessness. Breaking services into smaller pieces allows small, well-defined teams to develop and manage them. However, smaller services can introduce complexities which include possible increased latency, more complex debugging, and increased operational burden.Common anti-patterns:The microservice Death Star is a situation in which the atomic components become so highly interdependent that a failure of one results in a much larger failure, making the components as rigid and fragile as a monolith.Benefits of establishing this practice:More specific segments lead to greater agility, organizational flexibility, and scalability.Reduced impact of service interruptions.Application components may have different availability requirements, which can be supported by a more atomic segmentation.Well-defined responsibilities for teams supporting the workload.Level of risk exposed if this best practice is not established: HighChoose your architecture type based on how you will segment your workload. Choose an SOA or microservices architecture (or in some rare cases, a monolithic architecture). Even if you choose to start with a monolith architecture, you must ensure that it\u2019s modular and can ultimately evolve to SOA or microservices as your product scales with user adoption. SOA and microservices offer respectively smaller segmentation, which is preferred as a modern scalable and reliable architecture, but there are trade-offs to consider, especially when deploying a microservice architecture.One primary trade-off is that you now have a distributed compute architecture that can make it harder to achieve user latency requirements and there is additional complexity in the debugging and tracing of user interactions. You can use AWS X-Ray to assist you in solving this problem. Another effect to consider is increased operational complexity as you increase the number of applications that you are managing, which requires the deployment of multiple independency components.Monolithic, service-oriented, and microservices architecturesDetermine the appropriate architecture to refactor or build your application. SOA and microservices offer respectively smaller segmentation, which is preferred as a modern scalable and reliable architecture. SOA can be a good compromise for achieving smaller segmentation while avoiding some of the complexities of microservices. For more details, see Microservice Trade-Offs.If your workload is amenable to it, and your organization can support it, you should use a microservices architecture to achieve the best agility and reliability. For more details, see Implementing Microservices on AWS.Consider following the Strangler Fig pattern to refactor a monolith into smaller components. This involves gradually replacing specific application components with new applications and services. AWS Migration Hub Refactor Spaces acts as the starting point for incremental refactoring. For more details, see Seamlessly migrate on-premises legacy workloads using a strangler pattern.Implementing microservices may require a service discovery mechanism to allow these distributed services to communicate with each other. AWS App Mesh can be used with service-oriented architectures to provide reliable discovery and access of services. AWS Cloud Map can also be used for dynamic, DNS-based service discovery.If you\u2019re migrating from a monolith to SOA, Amazon MQ can help bridge the gap as a service bus when redesigning legacy applications in the cloud.For existing monoliths with a single, shared database, choose how to reorganize the data into smaller segments. This could be by business unit, access pattern, or data structure. At this point in the refactoring process, you should choose to move forward with a relational or non-relational (NoSQL) type of database. For more details, see From SQL to NoSQL.Level of effort for the implementation plan: HighRelated best practices:REL03-BP02 Build services focused on specific business domains and functionalityRelated documents:Amazon API Gateway: Configuring a REST API Using OpenAPIWhat is Service-Oriented Architecture?Bounded Context (a central pattern in Domain-Driven Design)Implementing Microservices on AWSMicroservice Trade-OffsMicroservices - a definition of this new architectural termMicroservices on AWSWhat is AWS App Mesh?Related examples:Iterative App Modernization WorkshopRelated videos:Delivering Excellence with Microservices on AWS\n* A reliable workload starts with upfront design decisions for both software and infrastructure. Your architecture choices will impact your workload behavior across all of the Well-Architected pillars. For reliability, there are specific patterns you must follow.With AWS, workload developers have their choice of languages and technologies to use. AWS SDKs take the complexity out of coding by providing language-specific APIs for AWS services. These SDKs, plus the choice of languages, allow developers to implement the reliability best practices listed here. Developers can also read about and learn from how Amazon builds and operates software in The Amazon Builders' Library.The following questions focus on these considerations for reliability.REL 3: How do you design your workload service architecture?Build highly scalable and reliable workloads using a service-oriented architecture (SOA) or a microservices architecture. Service-oriented architecture (SOA) is the practice of making software components reusable via service interfaces. Microservices architecture goes further to make components smaller and simpler.REL 4: How do you design interactions in a distributed system to prevent failures?Distributed systems rely on communications networks to interconnect components, such as servers or services. Your workload must operate reliably despite data loss or latency in these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices prevent failures and improve mean time between failures (MTBF).REL 5: How do you design interactions in a distributed system to mitigate or withstand failures?Distributed systems rely on communications networks to interconnect components (such as servers or services). Your workload must operate reliably despite data loss or latency over these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices enable workloads to withstand stresses or failures, more quickly recover from them, and mitigate the impact of such impairments. The result is improved mean time to recovery (MTTR).\n\nKnowledge Area: Design principles for microservices (for example, stateless workloads compared with stateful workloads)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When migrating from a monolith to SOA, which of the following services can be used to assist in the transition?\", \"answer_choices\": [{\"answer\": \"Amazon S3\", \"is_correct\": false, \"explanation\": \"Amazon S3 is an object storage service, and is not suitable for transitioning from monolith to SOA.\"},{\"answer\": \"AWS App Mesh\", \"is_correct\": false, \"explanation\": \"AWS App Mesh is used for service discovery and access of services, and it is not suitable for transitioning from monolith to SOA.\"},{\"answer\": \"AWS Cloud Map\", \"is_correct\": false, \"explanation\": \"AWS Cloud Map is used for dynamic, DNS-based service discovery, and it is not suitable for transitioning from monolith to SOA.\"},{\"answer\": \"Amazon MQ\", \"is_correct\": true, \"explanation\": \"Amazon MQ is a message broker service that can act as a service bus when redesigning legacy applications in the cloud and is suitable for transitioning from monolith to SOA.\"}]}", "docs": ["('REL03-BP01 Choose how to segment your workload - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_service_architecture_monolith_soa_microservice.html')", "('Workload architecture - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-workload-arch.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Event-driven architectures", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.Best practices\n* The optimal solution for a particular workload varies, and solutions often combine multiple approaches. Well-architected workloads use multiple solutions and enable different features to improve performance.AWS resources are available in many types and configurations, which makes it easier to find an approach that closely matches your workload needs. You can also find options that are not easily achievable with on-premises infrastructure. For example, a managed service such as Amazon DynamoDB provides a fully managed NoSQL database with single-digit millisecond latency at any scale.The following question focuses on these considerations for performance efficiency. (For a list of performance efficiency questions and best practices, see the Appendix.).Use a data-driven approach to select the patterns and implementation for your architecture and achieve a cost effective solution. AWS Solutions Architects, AWS Reference Architectures, and AWS Partner Network (APN) partners can help you select an architecture based on industry knowledge, but data obtained through benchmarking or load testing will be required to optimize your architecture.Your architecture will likely combine a number of different architectural approaches (for example, event-driven, ETL, or pipeline). The implementation of your architecture will use the AWS services that are specific to the optimization of your architecture's performance. In the following sections we discuss the four main resource types to consider (compute, storage, database, and network).PERF 1: How do you select the best performing architecture?Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.\n* Build highly scalable and reliable workloads using a service-oriented architecture (SOA) or a microservices architecture. Service-oriented architecture (SOA) is the practice of making software components reusable via service interfaces. Microservices architecture goes further to make components smaller and simpler.Best practices\n* The workload has a recovery time objective (RTO) and recovery point objective (RPO).\n* The Well-Architected Framework identifies a set of general design principles to facilitate good design in the cloud:Stop guessing your capacity needs: If you make a poor capacity decision when deploying a workload, you might end up sitting on expensive idle resources or dealing with the performance implications of limited capacity. With cloud computing, these problems can go away. You can use as much or as little capacity as you need, and scale up and down automatically.Test systems at production scale: In the cloud, you can create a production-scale test environment on demand, complete your testing, and then decommission the resources. Because you only pay for the test environment when it's running, you can simulate your live environment for a fraction of the cost of testing on premises.Automate to make architectural experimentation easier: Automation allows you to create and replicate your workloads at low cost and avoid the expense of manual effort. You can track changes to your automation, audit the impact, and revert to previous parameters when necessary.Allow for evolutionary architectures: In a traditional environment, architectural decisions are often implemented as static, onetime events, with a few major versions of a system during its lifetime. As a business and its context continue to evolve, these initial decisions might hinder the system's ability to deliver changing business requirements. In the cloud, the capability to automate and test on demand lowers the risk of impact from design changes. This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice.Drive architectures using data: In the cloud, you can collect data on how your architectural choices affect the behavior of your workload. This lets you make fact-based decisions on how to improve your workload. Your cloud infrastructure is code, so you can use that data to inform your architecture choices and improvements over time.Improve through game days: Test how your architecture and processes perform by regularly scheduling game days to simulate events in production. This will help you understand where improvements can be made and can help develop organizational experience in dealing with events.\n* A reliable workload starts with upfront design decisions for both software and infrastructure. Your architecture choices will impact your workload behavior across all of the Well-Architected pillars. For reliability, there are specific patterns you must follow.With AWS, workload developers have their choice of languages and technologies to use. AWS SDKs take the complexity out of coding by providing language-specific APIs for AWS services. These SDKs, plus the choice of languages, allow developers to implement the reliability best practices listed here. Developers can also read about and learn from how Amazon builds and operates software in The Amazon Builders' Library.The following questions focus on these considerations for reliability.REL 3: How do you design your workload service architecture?Build highly scalable and reliable workloads using a service-oriented architecture (SOA) or a microservices architecture. Service-oriented architecture (SOA) is the practice of making software components reusable via service interfaces. Microservices architecture goes further to make components smaller and simpler.REL 4: How do you design interactions in a distributed system to prevent failures?Distributed systems rely on communications networks to interconnect components, such as servers or services. Your workload must operate reliably despite data loss or latency in these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices prevent failures and improve mean time between failures (MTBF).REL 5: How do you design interactions in a distributed system to mitigate or withstand failures?Distributed systems rely on communications networks to interconnect components (such as servers or services). Your workload must operate reliably despite data loss or latency over these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices enable workloads to withstand stresses or failures, more quickly recover from them, and mitigate the impact of such impairments. The result is improved mean time to recovery (MTTR).\n* When architecting workloads, there are finite options that you can choose from. However, over time, new technologies and approaches become available that could improve the performance of your workload.Best practices\n\nKnowledge Area: Event-driven architectures\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which type of architecture should be used to store log files from applications for analysis and compliance purposes?\", \"answer_choices\": [{\"answer\": \"Service-oriented architecture (SOA)\",\"is_correct\": \"True\", \"explanation\": \"SOA is the practice of making software components reusable via service interfaces, which is ideal for storing log files.\"},{\"answer\": \"Microservices architecture\",\"is_correct\": \"True\", \"explanation\": \"Microservices architecture goes further to make components smaller and simpler, which is also ideal for storing log files.\"},{\"answer\": \"Data-driven architecture\",\"is_correct\": \"False\", \"explanation\": \"Data-driven architecture is used to select patterns and implementation for the architecture to achieve a cost-effective solution, not for storing log files.\"},{\"answer\": \"Event-driven architecture\",\"is_correct\": \"True\", \"explanation\": \"Event-driven architectures are ideal for storing log files since they enable applications to react to events in real-time.\"}]}", "docs": ["('PERF 1\\xa0 How do you select the best performing architecture? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-01.html')", "('Selection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-sel.html')", "('REL 3\\xa0 How do you design your workload service architecture? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-03.html')", "('REL13-BP01 Define recovery objectives for downtime and data loss - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_objective_defined_recovery.html')", "('General design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/general-design-principles.html')", "('Workload architecture - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-workload-arch.html')", "('PERF 6\\xa0 How do you evolve your workload to take advantage of new releases? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-06.html')"]}, {"scenario": "A company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Horizontal scaling and vertical scaling", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.`\"\n\nContext:\n\n* Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Update deployed components as your needs change.Level of risk exposed if this best practice is not established: MediumEnable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.Scale using small increments for variable workloads.Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years.Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.Related documents:AWS Compute Optimizer DocumentationOperating Lambda: Performance optimizationAuto Scaling Documentation\n* A scalable workload provides elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time.Best practices\n* Identify periods of low or no utilization and scale down resources to eliminate excess capacity and improve efficiency.Common anti-patterns:You do not scale your infrastructure with user load.You manually scale your infrastructure all the time.You leave increased capacity after a scaling event instead of scaling back down.Benefits of establishing this best practice: Configuring and testing workload elasticity will help reduce workload environmental impact, save money, and maintain performance benchmarks. You can take advantage of elasticity in the cloud to automatically scale capacity during and after user load spikes to make sure you are only using the exact number of resources needed to meet the needs of your customers.Level of risk exposed if this best practice is not established: MediumElasticity matches the supply of resources you have against the demand for those resources. Instances, containers, and functions provide mechanisms for elasticity, either in combination with automatic scaling or as a feature of the service. Use elasticity in your architecture to ensure that workload can scale down quickly and easily during the period of low user load:Amazon EC2 Auto ScalingUse to verify you have the correct number of Amazon EC2 instances available to handle the user load for your application.Application Auto ScalingUse to automatically scale the resources for individual AWS services beyond Amazon EC2, such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.Kubernetes Cluster AutoscalerUse to automatically scale Kubernetes clusters on AWS.Verify that the metrics for scaling up or down are validated against the type of workload being deployed. If you are deploying a video transcoding application, 100% CPU utilization is expected and should not be your primary metric. You can use a customized metric (such as memory utilization) for your scaling policy if required. To choose the right metrics, consider the following guidance for Amazon EC2:The metric should be a valid utilization metric and describe how busy an instance is.The metric value must increase or decrease proportionally to the number of instances in the Auto Scaling group.Use dynamic scaling instead of manual scaling for your Auto Scaling group. We also recommend that you use target tracking scaling policies in your dynamic scaling.Verify that workload deployments can handle both scale-up and scale-down events. Create test scenarios for scale-down events to ensure that the workload behaves as expected. You can use Activity history to test and verify a scaling activity for an Auto Scaling group.Evaluate your workload for predictable patterns and proactively scale as you anticipate predicted and planned changes in demand. Use Predictive Scaling with Amazon EC2 Auto Scaling to eliminate the need to overprove capacity.Related documents:Getting Started with Amazon EC2 Auto ScalingPredictive Scaling for EC2, Powered by Machine LearningAnalyze user behavior using Amazon OpenSearch Service, Amazon Kinesis Data Firehose and KibanaWhat is Amazon CloudWatch?What is AWS X-Ray?VPC Flow LogsMonitoring DB load with Performance Insights on Amazon RDSIntroducing Native Support for Predictive Scaling with Amazon EC2 Auto ScalingHow to create an Amazon EC2 Auto Scaling policy based on a memory utilization metric (Linux)Introducing Karpenter - An Open-Source, High-Performance Kubernetes Cluster AutoscalerRelated videos:Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Related examples:Lab: Amazon EC2 Auto Scaling Group ExamplesLab: Implement Autoscaling with KarpenterAuto-scaling mechanismWhere to useAmazon EC2 Auto ScalingUse to verify you have the correct number of Amazon EC2 instances available to handle the user load for your application.Application Auto ScalingUse to automatically scale the resources for individual AWS services beyond Amazon EC2, such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.Kubernetes Cluster AutoscalerUse to automatically scale Kubernetes clusters on AWS.\n\nKnowledge Area: Horizontal scaling and vertical scaling\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What practice should be established in order to minimize costs when running AWS batch processing jobs?\",\n\"answer_choices\": [{\"answer\": \"Use Negotiated Service Level Agreements (SLAs) that allow for a temporary reduction in capacity\",\"is_correct\": \"False\",\"explanation\": \"Negotiated SLAs primarily protect the customer and do not provide cost savings.\"},\n{\"answer\": \"Manually scale your infrastructure all the time\",\"is_correct\": \"False\",\"explanation\": \"Manually scaling your infrastructure is an anti-pattern and will not reduce costs.\"},\n{\"answer\": \"Identify periods of low or no utilization and scale down resources\",\"is_correct\": \"True\",\"explanation\": \"Identifying periods of low or no utilization and scaling down resources eliminates excess capacity and improves efficiency, thus minimizing costs.\"},\n{\"answer\": \"Configure and test workload elasticity\",\"is_correct\": \"True\",\"explanation\": \"Configuring and testing workload elasticity will help reduce workload environmental impact, save money, and maintain performance benchmarks, thus minimizing costs.\"}]}", "docs": ["('SUS05-BP01 Use the minimum amount of hardware to meet your needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a2.html')", "('REL 7\\xa0 How do you design your workload to adapt to changes in demand? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-07.html')", "('SUS02-BP01 Scale infrastructure with user load - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_user_a2.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "How to appropriately use edge accelerators (for example, content delivery network [CDN])", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies.Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public endpoints. You might also choose to evaluate AWS Marketplace software appliances for load balancing and proxying.Consumers of the service your workload provides, whether they are end-users or other services, make requests on these service endpoints. Several AWS resources are available to enable you to provide highly available endpoints.Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.Distributed Denial of Service (DDoS) attacks risk shutting out legitimate traffic and lowering availability for your users. AWS Shield provides automatic protection against these attacks at no extra cost for AWS service endpoints on your workload. You can augment these features with virtual appliances from APN Partners and the AWS Marketplace to meet your needs.Common anti-patterns:Using public internet addresses on instances or containers and managing the connectivity to them via DNS.Using Internet Protocol addresses instead of domain names for locating services.Providing content (web pages, static assets, media files) to a large geographic area and not using a content delivery network.Benefits of establishing this best practice: By implementing highly available services in your workload, you know that your workload will be available to your users.Level of risk exposed if this best practice is not established: HighEnsure that you have highly available connectivity for users of the workload Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public facing endpoints. You may also choose to evaluate AWS Marketplace software appliances for load-balancing and proxying.Ensure that you have a highly available connection to your users.Ensure that you are using a highly available DNS to manage the domain names of your application endpoints.If your users access your application via the internet, use service API operations to confirm the correct usage of Internet Gateways. Also confirm that the route tables entries for the subnets hosting your application endpoints are correct.DescribeInternetGatewaysDescribeRouteTablesEnsure that you are using a highly available reverse proxy or load balancer in front of your application.If your users access your application via your on-premises environment, ensure that your connectivity between AWS and your on-premises environment is highly available.Use Route 53 to manage your domain names.What is Amazon Route 53?Use a third-party DNS provider that meets your requirements.Use Elastic Load Balancing.What is Elastic Load Balancing?Use an AWS Marketplace appliance that meets your requirements.Related documents:APN Partner: partners that can help plan your networkingAWS Direct Connect Resiliency RecommendationsAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivityUsing the Direct Connect Resiliency Toolkit to get startedVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is AWS Global Accelerator?What Is Amazon VPC?What Is a Transit Gateway?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?Working with Direct Connect GatewaysRelated videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n* Implement services to reduce data transfer. For example, using a content delivery network (CDN) such as Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using AWS Direct Connect instead of VPN for connectivity to AWS.Level of risk exposed if this best practice is not established: LowAmazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources. By using CloudFront, you can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network. It is ideal for small offices or business partners because it provides quick and easy connectivity, and it is a fully managed and elastic service.VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs. Gateway VPC endpoints have no hourly charges, and support Amazon Simple Storage Service(Amazon S3) and Amazon DynamoDB. Interface VPC endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost.Implementation stepsImplement services: Using the data transfer modeling, look at where the largest costs and highest volume flows are. Review the AWS services and assess whether there is a service that reduces or removes the transfer, specifically networking and content delivery. Also look for caching services where there is repeated access to data, or large amounts of data.Related documents:AWS Direct ConnectAWS Explore Our ProductsAWS caching solutionsAmazon CloudFrontDeliver content faster with Amazon CloudFront\n\nKnowledge Area: How to appropriately use edge accelerators (for example, content delivery network [CDN])\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is an AWS service that helps reduce data transfer costs and deliver content to end users with low latency and high transfer speeds?\", \"answer_choices\": [{\"answer\": \"Amazon Route 53\",\"is_correct\": \"False\",\"explanation\": \"Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS. It is not an AWS service that helps reduce data transfer costs and deliver content to end users with low latency and high transfer speeds.\"},{\"answer\": \"AWS Global Accelerator\",\"is_correct\": \"False\",\"explanation\": \"AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network. It is not an AWS service that helps reduce data transfer costs and deliver content to end users with low latency and high transfer speeds.\"},{\"answer\": \"Amazon VPC\",\"is_correct\": \"False\",\"explanation\": \"Amazon VPC lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. It is not an AWS service that helps reduce data transfer costs and deliver content to end users with low latency and high transfer speeds.\"},{\"answer\": \"Amazon CloudFront\",\"is_correct\": \"True\",\"explanation\": \"Amazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources. By using CloudFront, you can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.\"}]}", "docs": ["('REL02-BP01 Use highly available network connectivity for your workload public endpoints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ha_conn_users.html')", "('COST08-BP03 Implement services to reduce data transfer costs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_implement_services.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "How to migrate applications into containers", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Implement changes onto parallel environments, and then transition over to the new environment. Maintain the prior environment until there is confirmation of successful deployment. Doing so minimizes recovery time by enabling rollback to the previous environment.Common anti-patterns:You perform a mutable deployment by modifying your existing systems. After discovering that the change was unsuccessful, you are forced to modify the systems again to restore the old version extending your time to recovery.During a maintenance window, you decommission the old environment and then start building your new environment. Many hours into the procedure, you discover unrecoverable issues with the deployment. While extremely tired, you are forced to find the previous deployment procedures and start rebuilding the old environment.Benefits of establishing this best practice: By using parallel environments, you can pre-deploy the new environment and transition over to them when desired. If the new environment is not successful, you can recover quickly by transitioning back to your original environment.Level of risk exposed if this best practice is not established: MediumDeploy using parallel environments: Implement changes onto parallel environments, and transition or cut over to the new environment. Maintain the prior environment until there is confirmation of successful deployment. This minimizes recovery time by enabling rollback to the previous environment. For example, use immutable infrastructures with blue/green deployments.Working with deployment configurations in AWS CodeDeployBlue/Green deployments with AWS Elastic BeanstalkSet up an API Gateway canary release deploymentTry a Sample Blue/Green Deployment in AWS CodeDeployRelated documents:AWS CodeDeploy User GuideBlue/Green deployments with AWS Elastic BeanstalkSet up an API Gateway canary release deploymentTry a Sample Blue/Green Deployment in AWS CodeDeployWorking with deployment configurations in AWS CodeDeployRelated videos:Deep Dive on Advanced Continuous Delivery Techniques Using AWS\n* Up-to-date operating systems, libraries, and applications can improve workload efficiency and enable easier adoption of more efficient technologies. Up-to-date software might also include features to measure the sustainability impact of your workload more accurately, as vendors deliver features to meet their own sustainability goals.Common anti-patterns:You assume your current architecture will become static with no updates over time.You do not have any systems or a regular cadence to evaluate if updated software and packages are compatible with your workload.You introduce architecture changes over time without justification.Benefits of establishing this best practice: By establishing a process to keep your workload up to date, you will be able to adopt new features and capabilities, resolve issues, and improve workload efficiency.Level of risk exposed if this best practice is not established: LowDefine a process and a schedule to evaluate new features or instances for your workload. Take advantage of agility in the cloud to quickly test how new features can improve your workload to:Reduce sustainability impacts.Gain performance efficiencies.Remove barriers for a planned improvement.Improve your ability to measure and manage sustainability impacts.Inventory your workload software and architecture and identify components that need to be updated. You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and quickly understand which instances are running the software and configurations required by your software policy and which instances need to be updated.Understand how to update the components of your workload.Machine imagesManage updates to Amazon Machine Images (AMI) for Linux or Windows server images using EC2 Image Builder.Container imagesYou should use Amazon Elastic Container Registry (Amazon ECR) with your existing pipeline to manage Amazon Elastic Container Service (Amazon ECS) images and manage Amazon Elastic Kubernetes Service images.AWS LambdaAWS Lambda includes version management features.Use automation for the update process to reduce the level of effort to deploy new features and limit errors caused by manual processes. Use tools such as AWS Systems Manager Patch Manager to automate the process of system updates, and schedule the activity using AWS Systems Manager Maintenance Windows.Related documents:AWS Architecture CenterWhat's New with AWSAWS Developer ToolsAWS Systems Manager Patch ManagerRelated examples:Well-Architected Labs: Inventory and Patch ManagementLab: AWS Systems ManagerWorkload componentHow to updateMachine imagesManage updates to Amazon Machine Images (AMI) for Linux or Windows server images using EC2 Image Builder.Container imagesYou should use Amazon Elastic Container Registry (Amazon ECR) with your existing pipeline to manage Amazon Elastic Container Service (Amazon ECS) images and manage Amazon Elastic Kubernetes Service images.AWS LambdaAWS Lambda includes version management features.\n* Use automation and infrastructure-as-code to bring pre-production environments up when needed and take them down when not used. A common pattern is to schedule periods of availability that coincide with the working hours of your development team members. Hibernation is a useful tool to preserve the state and rapidly bring instances online only when needed. Use instance types with burst capacity, Spot Instances, elastic database services, containers, and other technologies to align development and test capacity with use.Level of risk exposed if this best practice is not established: LowUse automation to maximize utilization of your development and test environments.Use automation to manage the lifecycle of your development and test environments.Use minimum viable representative environments to develop and test potential improvements.Use On-Demand Instances to supplement your developer devices.Use automation to maximize the efficiency of your build resources.Use instance types with burst capacity, Spot Instances, and other technologies to align build capacity with use.Adopt native cloud services for secure instance shell access rather than deploying fleets of bastion hosts.Related documents:AWS Systems Manager Session ManagerAmazon EC2 Burstable performance instancesWhat is AWS CloudFormation?\n\nKnowledge Area: How to migrate applications into containers\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\n\"question\": \"Which of the following is the best practice to migrate applications into containers?\", \n\"answer_choices\": [\n    {\"answer\": \"Deploy using parallel environments\", \"is_correct\": \"False\", \"explanation\": \"This is the best practice for deploying changes onto parallel environments, but is not the best practice for migrating applications into containers.\"}, \n    {\"answer\": \"Set up an API Gateway canary release deployment\", \"is_correct\": \"False\", \"explanation\": \"This is the best practice for setting up an API Gateway canary release deployment, but is not the best practice for migrating applications into containers.\"}, \n    {\"answer\": \"Use automation and infrastructure-as-code to bring pre-production environments up when needed\", \"is_correct\": \"True\", \"explanation\": \"This is the best practice for migrating applications into containers, as it allows for quicker and easier deployments, as well as cost savings.\"}, \n    {\"answer\": \"Inventory your workload software and architecture and identify components that need to be updated\", \"is_correct\": \"False\", \"explanation\": \"This is the best practice for inventorying your workload software and architecture, but is not the best practice for migrating applications into containers.\"}\n    ]\n}", "docs": ["('OPS06-BP05 Deploy using parallel environments - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_mit_deploy_risks_deploy_to_parallel_env.html')", "('SUS06-BP02 Keep your workload up-to-date - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_dev_a3.html')", "('SUS06-BP03 Increase utilization of build environments - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_dev_a4.html')"]}, {"scenario": "A software development team wants to build and deploy a microservices-based application on AWS.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Load balancing concepts (for example, Application Load Balancer)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA software development team wants to build and deploy a microservices-based application on AWS.`\"\n\nContext:\n\n* Distribute traffic across multiple resources or services to allow your workload to take advantage of the elasticity that the cloud provides. You can also use load balancing for offloading encryption termination to improve performance and to manage and route traffic effectively.When implementing a scale-out architecture where you want to use multiple instances for service content, you can use load balancers inside your Amazon VPC. AWS provides multiple models for your applications in the ELB service. Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers.Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. It is capable of handling millions of requests per second while maintaining ultra-low latencies, and it is optimized to handle sudden and volatile traffic patterns.Elastic Load Balancing provides integrated certificate management and SSL/TLS decryption, allowing you the flexibility to centrally manage the SSL settings of the load balancer and offload CPU intensive work from your workload.Common anti-patterns:You route all internet traffic through existing load balancers.You use generic TCP load balancing and making each compute node handle SSL encryption.Benefits of establishing this best practice: A load balancer handles the varying load of your application traffic in a single Availability Zone, or across multiple Availability Zones. Load balancers feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant.Level of risk exposed if this best practice is not established: HighUse the appropriate load balancer for your workload: Select the appropriate load balancer for your workload. If you must load balance HTTP requests, we recommend Application Load Balancer. For network and transport protocols (layer 4 \u2013 TCP, UDP) load balancing, and for extreme performance and low latency applications, we recommend Network Load Balancer. Application Load Balancers support HTTPS and Network Load Balancers support TLS encryption offloading.Enable offload of HTTPS or TLS encryption: Elastic Load Balancing includes integrated certificate management, user-authentication, and SSL/TLS decryption. It provides the flexibility to centrally manage TLS settings and offload CPU intensive workloads from your applications. Encrypt all HTTPS traffic as part of your load balancer deployment.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies.Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public endpoints. You might also choose to evaluate AWS Marketplace software appliances for load balancing and proxying.Consumers of the service your workload provides, whether they are end-users or other services, make requests on these service endpoints. Several AWS resources are available to enable you to provide highly available endpoints.Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.Distributed Denial of Service (DDoS) attacks risk shutting out legitimate traffic and lowering availability for your users. AWS Shield provides automatic protection against these attacks at no extra cost for AWS service endpoints on your workload. You can augment these features with virtual appliances from APN Partners and the AWS Marketplace to meet your needs.Common anti-patterns:Using public internet addresses on instances or containers and managing the connectivity to them via DNS.Using Internet Protocol addresses instead of domain names for locating services.Providing content (web pages, static assets, media files) to a large geographic area and not using a content delivery network.Benefits of establishing this best practice: By implementing highly available services in your workload, you know that your workload will be available to your users.Level of risk exposed if this best practice is not established: HighEnsure that you have highly available connectivity for users of the workload Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public facing endpoints. You may also choose to evaluate AWS Marketplace software appliances for load-balancing and proxying.Ensure that you have a highly available connection to your users.Ensure that you are using a highly available DNS to manage the domain names of your application endpoints.If your users access your application via the internet, use service API operations to confirm the correct usage of Internet Gateways. Also confirm that the route tables entries for the subnets hosting your application endpoints are correct.DescribeInternetGatewaysDescribeRouteTablesEnsure that you are using a highly available reverse proxy or load balancer in front of your application.If your users access your application via your on-premises environment, ensure that your connectivity between AWS and your on-premises environment is highly available.Use Route 53 to manage your domain names.What is Amazon Route 53?Use a third-party DNS provider that meets your requirements.Use Elastic Load Balancing.What is Elastic Load Balancing?Use an AWS Marketplace appliance that meets your requirements.Related documents:APN Partner: partners that can help plan your networkingAWS Direct Connect Resiliency RecommendationsAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivityUsing the Direct Connect Resiliency Toolkit to get startedVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is AWS Global Accelerator?What Is Amazon VPC?What Is a Transit Gateway?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?Working with Direct Connect GatewaysRelated videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n\nKnowledge Area: Load balancing concepts (for example, Application Load Balancer)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service is best suited for load balancing HTTP and HTTPS traffic, providing advanced request routing and delivering modern application architectures such as microservices and containers?\", \n\"answer_choices\": [{\"answer\": \"AWS Transit Gateway\",\"is_correct\": \"false\",\"explanation\": \"AWS Transit Gateway is a service that enables you to connect your Amazon Virtual Private Clouds (VPCs) and your on-premises networks to a single gateway.\"},{\"answer\": \"Amazon CloudFront\",\"is_correct\": \"false\",\"explanation\": \"Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, and image files, to your users.\"},{\"answer\": \"Amazon Route 53\",\"is_correct\": \"false\",\"explanation\": \"Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets.\"},{\"answer\": \"Application Load Balancer\",\"is_correct\": \"true\",\"explanation\": \"Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers.\"}]}", "docs": ["('PERF05-BP04 Leverage load-balancing and encryption offloading - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_encryption_offload.html')", "('REL02-BP01 Use highly available network connectivity for your workload public endpoints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ha_conn_users.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Multi-tier architectures", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.Best practices\n* The workload has a recovery time objective (RTO) and recovery point objective (RPO).\n* The optimal solution for a particular workload varies, and solutions often combine multiple approaches. Well-architected workloads use multiple solutions and enable different features to improve performance.AWS resources are available in many types and configurations, which makes it easier to find an approach that closely matches your workload needs. You can also find options that are not easily achievable with on-premises infrastructure. For example, a managed service such as Amazon DynamoDB provides a fully managed NoSQL database with single-digit millisecond latency at any scale.The following question focuses on these considerations for performance efficiency. (For a list of performance efficiency questions and best practices, see the Appendix.).Use a data-driven approach to select the patterns and implementation for your architecture and achieve a cost effective solution. AWS Solutions Architects, AWS Reference Architectures, and AWS Partner Network (APN) partners can help you select an architecture based on industry knowledge, but data obtained through benchmarking or load testing will be required to optimize your architecture.Your architecture will likely combine a number of different architectural approaches (for example, event-driven, ETL, or pipeline). The implementation of your architecture will use the AWS services that are specific to the optimization of your architecture's performance. In the following sections we discuss the four main resource types to consider (compute, storage, database, and network).PERF 1: How do you select the best performing architecture?Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.\n* In on-premises environments, customers often have a central team for technology architecture that acts as an overlay to other product or feature teams to ensure they are following best practice. Technology architecture teams typically include a set of roles such as: Technical Architect (infrastructure), Solutions Architect (software), Data Architect, Networking Architect, and Security Architect. Often these teams use TOGAF or the Zachman Framework as part of an enterprise architecture capability.At AWS, we prefer to distribute capabilities into teams rather than having a centralized team with that capability. There are risks when you choose to distribute decision making authority, for example, ensuring that teams are meeting internal standards. We mitigate these risks in two ways. First, we have practices (ways of doing things, process, standards, and accepted norms) that focus on enabling each team to have that capability, and we put in place experts who ensure that teams raise the bar on the standards they need to meet. Second, we implement mechanisms that carry out automated checks to ensure standards are being met.\u201cGood intentions never work, you need good mechanisms to make anything happen\u201d \u2014 Jeff Bezos.This means replacing a human's best efforts with mechanisms (often automated) that check for compliance with rules or process. This distributed approach is supported by the Amazon leadership principles, and establishes a culture across all roles that works back from the customer. Working backward is a fundamental part of our innovation process. We start with the customer and what they want, and let that define and guide our efforts. Customer-obsessed teams build products in response to a customer need.For architecture, this means that we expect every team to have the capability to create architectures and to follow best practices. To help new teams gain these capabilities or existing teams to raise their bar, we enable access to a virtual community of principal engineers who can review their designs and help them understand what AWS best practices are. The principal engineering community works to make best practices visible and accessible. One way they do this, for example, is through lunchtime talks that focus on applying best practices to real examples. These talks are recorded and can be used as part of onboarding materials for new team members.AWS best practices emerge from our experience running thousands of systems at internet scale. We prefer to use data to define best practice, but we also use subject matter experts, like principal engineers, to set them. As principal engineers see new best practices emerge, they work as a community to ensure that teams follow them. In time, these best practices are formalized into our internal review processes, as well as into mechanisms that enforce compliance. The Well-Architected Framework is the customer-facing implementation of our internal review process, where we have codified our principal engineering thinking across field roles, like Solutions Architecture and internal engineering teams. The Well-Architected Framework is a scalable mechanism that lets you take advantage of these learnings.By following the approach of a principal engineering community with distributed ownership of architecture, we believe that a Well-Architected enterprise architecture can emerge that is driven by customer need. Technology leaders (such as a CTOs or development managers), carrying out Well-Architected reviews across all your workloads will allow you to better understand the risks in your technology portfolio. Using this approach, you can identify themes across teams that your organization could address by mechanisms, training, or lunchtime talks where your principal engineers can share their thinking on specific areas with multiple teams.\n* The Well-Architected Framework identifies a set of general design principles to facilitate good design in the cloud:Stop guessing your capacity needs: If you make a poor capacity decision when deploying a workload, you might end up sitting on expensive idle resources or dealing with the performance implications of limited capacity. With cloud computing, these problems can go away. You can use as much or as little capacity as you need, and scale up and down automatically.Test systems at production scale: In the cloud, you can create a production-scale test environment on demand, complete your testing, and then decommission the resources. Because you only pay for the test environment when it's running, you can simulate your live environment for a fraction of the cost of testing on premises.Automate to make architectural experimentation easier: Automation allows you to create and replicate your workloads at low cost and avoid the expense of manual effort. You can track changes to your automation, audit the impact, and revert to previous parameters when necessary.Allow for evolutionary architectures: In a traditional environment, architectural decisions are often implemented as static, onetime events, with a few major versions of a system during its lifetime. As a business and its context continue to evolve, these initial decisions might hinder the system's ability to deliver changing business requirements. In the cloud, the capability to automate and test on demand lowers the risk of impact from design changes. This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice.Drive architectures using data: In the cloud, you can collect data on how your architectural choices affect the behavior of your workload. This lets you make fact-based decisions on how to improve your workload. Your cloud infrastructure is code, so you can use that data to inform your architecture choices and improvements over time.Improve through game days: Test how your architecture and processes perform by regularly scheduling game days to simulate events in production. This will help you understand where improvements can be made and can help develop organizational experience in dealing with events.\n\nKnowledge Area: Multi-tier architectures\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best approach for selecting the best performing multi-tier architecture for a workload?\", \n\"answer_choices\": [{\"answer\": \"Choose a managed service to provide a fully managed NoSQL database with single-digit millisecond latency at any scale.\",\"is_correct\": \"False\",\"explanation\": \"Although choosing a managed service can help optimize performance, it is not the best approach for selecting the best performing multi-tier architecture for a workload.\"},{\"answer\": \"Follow the Amazon leadership principles.\",\"is_correct\": \"False\",\"explanation\": \"The Amazon leadership principles are important for establishing a culture across all roles, but they are not the best approach for selecting the best performing multi-tier architecture for a workload.\"},{\"answer\": \"Distribute decision making authority among teams.\",\"is_correct\": \"False\",\"explanation\": \"Distributing decision making authority among teams is important for mitigating risks, but it is not the best approach for selecting the best performing multi-tier architecture for a workload.\"},{\"answer\": \"Use a data-driven approach to select the patterns and implementation for your architecture.\",\"is_correct\": \"True\",\"explanation\": \"Using a data-driven approach to select the patterns and implementation for your architecture is the best approach for selecting the best performing multi-tier architecture for a workload.\"}]}", "docs": ["('PERF 1\\xa0 How do you select the best performing architecture? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-01.html')", "('REL13-BP01 Define recovery objectives for downtime and data loss - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_objective_defined_recovery.html')", "('Selection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-sel.html')", "('On architecture - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/on-architecture.html')", "('General design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/general-design-principles.html')"]}, {"scenario": "A large corporation wants to move their on-premises compute to the cloud for increased scalability and security.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Queuing and messaging concepts (for example, publish/subscribe)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA large corporation wants to move their on-premises compute to the cloud for increased scalability and security.`\"\n\nContext:\n\n* Communicate directly with your users (for example, with email or SMS) when the services they use are impacted, and again when the services return to normal operating conditions, to enable users to take appropriate action.Common anti-patterns:Your application is experiencing a distributed denial of service incident and has been unresponsive for days. There is no error message. You have not sent a notification email. You have not sent text notifications. You have not shared information on social media. You customers are frustrated and looking for other vendors who can support them.On Monday, your application had issues following a patch and was down for a couple of hours. On Tuesday, your application had issues following a code deployment and was unreliable for a couple of hours. On Wednesday, your application had issues following a code deployment to mitigate a security vulnerability associated to the failed patch and was unavailable for a couple of hours. On Thursday, your frustrated customers started looking for another vendor who could support them.Your application is going to be down for maintenance this weekend. You don't inform your customers. Some of your customers had scheduled activities involving the use of your application. They are very frustrated upon discovery that your application is not available.Benefits of establishing this best practice: By defining notifications, triggers for notifications, and procedures for notifications you enable your customer to be informed and respond when issues with your workload impact them.Level of risk exposed if this best practice is not established: MediumEnable push notifications: Communicate directly with your users (for example, with email or SMS) when the services they use are impacted, and when the services return to normal operating conditions, to enable users to take appropriate action.Amazon SES featuresWhat is Amazon SES?Set up Amazon SNS notificationsRelated documents:Amazon SES featuresSet up Amazon SNS notificationsWhat is Amazon SES?\n* Use efficient software designs and architectures to minimize the average resources required per unit of work. Implement mechanisms that result in even utilization of components to reduce resources that are idle between tasks and minimize the impact of load spikes.Level of risk exposed if this best practice is not established: LowQueue requests that don\u2019t require immediate processing.Increase serialization to flatten utilization across your pipeline.Modify the capacity of individual components to prevent idling resources waiting for input.Create buffers and establish rate limiting to smooth the consumption of external services.Use the most efficient available hardware for your software optimizations.Use queue-driven architectures, pipeline management, and On-Demand Instance workers to maximize utilization for batch processing.Schedule tasks to avoid load spikes and resource contention from simultaneous execution.Schedule jobs during times of day where carbon intensity for power is lowest.Related documents:What is Amazon Simple Queue Service?What is Amazon MQ?Scaling based on Amazon SQSWhat is AWS Step Functions?What is AWS Lambda?Using AWS Lambda with Amazon SQSWhat is Amazon EventBridge?Related videos:Building Sustainably on AWSMoving to event-driven architectures\n\nKnowledge Area: Queuing and messaging concepts (for example, publish/subscribe)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following techniques should be used to reduce the impact of load spikes on a large corporation's on-premises compute to the cloud for increased scalability and security?\", \n\"answer_choices\": [{\"answer\": \"Increase serialization to flatten utilization across the pipeline.\",\"is_correct\": \"True\", \"explanation\": \"Increasing serialization will help to better distribute the load and reduce the impact of spikes on the system.\"},{\"answer\": \"Modify the capacity of individual components to prevent idling resources waiting for input.\",\"is_correct\": \"True\", \"explanation\": \"Modifying the capacity of individual components can help prevent resources from idling while waiting for input.\"},{\"answer\": \"Use the most efficient available hardware for your software optimizations.\",\"is_correct\": \"False\",\"explanation\": \"This is not a technique which can reduce the impact of load spikes, but rather a technique to optimize the available hardware for software optimizations.\"},{\"answer\": \"Use queue-driven architectures, pipeline management and On-Demand Instance workers.\",\"is_correct\": \"True\",\"explanation\": \"Using queue-driven architectures, pipeline management and On-Demand Instance workers can maximize utilization for batch processing and reduce the impact of load spikes.\"}]}", "docs": ["('OPS10-BP05 Enable push notifications - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_event_response_push_notify.html')", "('SUS03-BP01 Optimize software and architecture for asynchronous and scheduled jobs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a2.html')"]}, {"scenario": "A startup wants to host their new web application on AWS.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Serverless technologies and patterns (for example, AWS Fargate, AWS Lambda)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA startup wants to host their new web application on AWS.`\"\n\nContext:\n\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n* There are five design principles for performance efficiency in the cloud:Democratize advanced technologies: Make advanced technology implementation easier for your team by delegating complex tasks to your cloud vendor. Rather than asking your IT team to learn about hosting and running a new technology, consider consuming the technology as a service. For example, NoSQL databases, media transcoding, and machine learning are all technologies that require specialized expertise. In the cloud, these technologies become services that your team can consume, allowing your team to focus on product development rather than resource provisioning and management.Go global in minutes: Deploying your workload in multiple AWS Regions around the world allows you to provide lower latency and a better experience for your customers at minimal cost.Use serverless architectures: Serverless architectures remove the need for you to run and maintain physical servers for traditional compute activities. For example, serverless storage services can act as static websites (removing the need for web servers) and event services can host code. This removes the operational burden of managing physical servers, and can lower transactional costs because managed services operate at cloud scale.Experiment more often: With virtual and automatable resources, you can quickly carry out comparative testing using different types of instances, storage, or configurations.Consider mechanical sympathy: Understand how cloud services are consumed and always use the technology approach that aligns best with your workload goals. For example, consider data access patterns when you select database or storage approaches.\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n* Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Update deployed components as your needs change.Level of risk exposed if this best practice is not established: MediumEnable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.Scale using small increments for variable workloads.Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years.Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.Related documents:AWS Compute Optimizer DocumentationOperating Lambda: Performance optimizationAuto Scaling Documentation\n* The workload has a recovery time objective (RTO) and recovery point objective (RPO).\n\nKnowledge Area: Serverless technologies and patterns (for example, AWS Fargate, AWS Lambda)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best solution to reduce cost and minimize license costs for the startup's web application on AWS?\", \n\"answer_choices\": [\n{\"answer\": \"Use AWS Lambda for compute workloads\",\"is_correct\": \"True\",\"explanation\":\"AWS Lambda is a serverless computing solution that allows the startup to use code execution without the need to manage a resource and provides scalability so that cost and performance can be efficiently allocated and attributed.\"},\n{\"answer\": \"Use Amazon RDS for databases\",\"is_correct\": \"False\",\"explanation\":\"Amazon RDS is used to set up, operate, and scale a relational database and automate administration tasks such as hardware provisioning, database setup, patching, and backups. It is not used to minimize license costs.\"},\n{\"answer\": \"Use Amazon S3 for static websites\",\"is_correct\": \"False\",\"explanation\":\"Amazon S3 is used for storage and not to minimize license costs.\"},\n{\"answer\": \"Use Amazon Linux for compute workloads\",\"is_correct\": \"True\",\"explanation\":\"Using Amazon Linux for compute workloads can help minimize license costs by leveraging open source software or software that does not have license fees.\"}\n]}", "docs": ["('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')", "('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-dp.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')", "('SUS05-BP01 Use the minimum amount of hardware to meet your needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a2.html')", "('REL13-BP01 Define recovery objectives for downtime and data loss - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_objective_defined_recovery.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Storage types with associated characteristics (for example, object, file, block)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n* Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file storage services as well as cloud data migration options for your workload.In AWS, storage is available in three forms: object, block, and file:Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems.The following question focuses on these considerations for performance efficiency.When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want.PERF 3: How do you select your storage solution?The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n\nKnowledge Area: Storage types with associated characteristics (for example, object, file, block)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When selecting a storage solution to achieve optimal performance for a gaming company's multiplayer game servers, which type of storage should be used and why?\",\n\"answer_choices\": [\n{\"answer\": \"Amazon EBS for its low latency and IOPS performance,\", \"is_correct\": \"True\", \"explanation\": \"Amazon EBS provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon EBS is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.\"},\n{\"answer\": \"Amazon S3 for its transfer acceleration,\", \"is_correct\": \"False\", \"explanation\": \"Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 Transfer Acceleration enables fast transfer of files over long distances between your client and your S3 bucket, but it is not the optimal choice for a gaming company's multiplayer game servers.\"},\n{\"answer\": \"Amazon EFS for its shared file system,\", \"is_correct\": \"False\", \"explanation\": \"Amazon Elastic File System (Amazon EFS) is ideal for use cases such as large content repositories, development environments, media stores, or user home directories. It provides access to a shared file system across multiple systems, but is not the optimal choice for a gaming company's multiplayer game servers.\"},\n{\"answer\": \"Amazon FSx for its SSD-backed storage,\", \"is_correct\": \"True\", \"explanation\": \"Amazon FSx makes it easy and cost effective to launch and run popular file systems, and is SSD-backed with fast performance. It is designed to deliver fast, predictable, scalable, and consistent performance, making it an ideal choice for a gaming company's multiplayer game servers.\"}\n]}", "docs": ["('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')", "('Storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-storage.html')", "('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')"]}, {"scenario": "A marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "The orchestration of containers (for example, Amazon Elastic Container Service [Amazon ECS], Amazon Elastic Kubernetes Service [Amazon EKS])", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.`\"\n\nContext:\n\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n\nKnowledge Area: The orchestration of containers (for example, Amazon Elastic Container Service [Amazon ECS], Amazon Elastic Kubernetes Service [Amazon EKS])\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What type of service should the marketing company use to reduce their security maintenance tasks and focus on securing their application according to the shared responsibility model?\",\n\"answer_choices\": [\n    {\"answer\": \"Amazon EC2 Systems Manager\",\"is_correct\": \"False\",\"explanation\": \"Amazon EC2 Systems Manager is used to manage EC2 instances and automate common administrative tasks, however it does not reduce security maintenance tasks.\"},\n    {\"answer\": \"Amazon RDS\",\"is_correct\": \"True\",\"explanation\": \"Amazon RDS helps set up, operate, and scale a relational database and automates administration tasks such as hardware provisioning, database setup, patching, and backups, which reduces security maintenance tasks.\"},\n    {\"answer\": \"AWS Lambda\",\"is_correct\": \"True\",\"explanation\": \"AWS Lambda allows you to run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level, reducing security maintenance tasks.\"},\n    {\"answer\": \"Amazon ECS\",\"is_correct\": \"True\",\"explanation\": \"Amazon ECS helps manage resources, such as containers, and can reduce your security maintenance tasks as part of the shared responsibility model.\"}\n]}", "docs": ["('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')"]}, {"scenario": "A social media platform wants to use AWS to store and process user-generated content.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "When to use read replicas", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA social media platform wants to use AWS to store and process user-generated content.`\"\n\nContext:\n\n* If components of the workload can only run in a single Availability Zone or in an on-premises data center, you must implement the capability to do a complete rebuild of the workload within your defined recovery objectives.If the best practice to deploy the workload to multiple locations is not possible due to technological constraints, you must implement an alternate path to resiliency. You must automate the ability to recreate necessary infrastructure, redeploy applications, and recreate necessary data for these cases.For example, Amazon EMR launches all nodes for a given cluster in the same Availability Zone because running a cluster in the same zone improves performance of the jobs flows as it provides a higher data access rate. If this component is required for workload resilience, then you must have a way to redeploy the cluster and its data. Also for Amazon EMR, you should provision redundancy in ways other than using Multi-AZ. You can provision multiple nodes. Using EMR File System (EMRFS), data in EMR can be stored in Amazon S3, which in turn can be replicated across multiple Availability Zones or AWS Regions.Similarly, for Amazon Redshift, by default it provisions your cluster in a randomly selected Availability Zone within the AWS Region that you select. All the cluster nodes are provisioned in the same zone.Level of risk exposed if this best practice is not established: MediumImplement self-healing. Deploy your instances or containers using automatic scaling when possible. If you cannot use automatic scaling, use automatic recovery for EC2 instances or implement self-healing automation based on Amazon EC2 or ECS container lifecycle events.Use Auto Scaling groups for instances and container workloads that have no requirements for a single instance IP address, private IP address, Elastic IP address, and instance metadata.What Is EC2 Auto Scaling?Service automatic scalingThe launch template user data can be used to implement automation that can self-heal most workloads.Use automatic recovery of EC2 instances for workloads that require a single instance ID address, private IP address, Elastic IP address, and instance metadata.Recover your instance.Automatic Recovery will send recovery status alerts to a SNS topic as the instance failure is detected.Use EC2 instance lifecycle events or ECS events to automate self-healing where automatic scaling or EC2 recovery cannot be used.EC2 Auto Scaling lifecycle hooksAmazon ECS eventsUse the events to invoke automation that will heal your component according to the process logic you require.Related documents:Amazon ECS eventsEC2 Auto Scaling lifecycle hooksRecover your instance.Service automatic scalingWhat Is EC2 Auto Scaling?\n* Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload. Fetch data from shared storage only as needed. Detach unused volumes to make more resources available.Level of risk exposed if this best practice is not established: LowMigrate data to shared storage when the data has multiple consumers.Fetch data from shared storage only as needed.Delete data as appropriate for your usage patterns, and implement time-to-live (TTL) functionality to manage cached data.Detach volumes from clients that are not actively using them.Related documents:Amazon FSxCaching strategiesWhat is Amazon Elastic File System?What is Amazon S3?\n* Regularly test failover to your recovery site to ensure proper operation, and that RTO and RPO are met.A pattern to avoid is developing recovery paths that are rarely exercised. For example, you might have a secondary data store that is used for read-only queries. When you write to a data store and the primary fails, you might want to fail over to the secondary data store. If you don\u2019t frequently test this failover, you might find that your assumptions about the capabilities of the secondary data store are incorrect. The capacity of the secondary, which might have been sufficient when you last tested, might be no longer be able to tolerate the load under this scenario. Our experience has shown that the only error recovery that works is the path you test frequently. This is why having a small number of recovery paths is best. You can establish recovery patterns and regularly test them. If you have a complex or critical recovery path, you still need to regularly exercise that failure in production to convince yourself that the recovery path works. In the example we just discussed, you should fail over to the standby regularly, regardless of need.Common anti-patterns:Never exercise failovers in production.Benefits of establishing this best practice: Regularly testing you disaster recovery plan ensures that it will work when it needs to, and that your team knows how to execute the strategy.Level of risk exposed if this best practice is not established: HighEngineer your workloads for recovery. Regularly test your recovery paths Recovery Oriented Computing identifies the characteristics in systems that enhance recovery. These characteristics are: isolation and redundancy, system-wide ability to roll back changes, ability to monitor and determine health, ability to provide diagnostics, automated recovery, modular design, and ability to restart. Exercise the recovery path to ensure that you can accomplish the recovery in the specified time to the specified state. Use your runbooks during this recovery to document problems and find solutions for them before the next test.The Berkeley/Stanford recovery-oriented computing projectUse AWS Elastic Disaster Recovery to implement and launch drill instances for your DR strategy.AWS Elastic Disaster Recovery Preparing for FailoverWhat is Elastic Disaster Recovery?AWS Elastic Disaster RecoveryRelated documents:APN Partner: partners that can help with disaster recoveryAWS Architecture Blog: Disaster Recovery SeriesAWS Marketplace: products that can be used for disaster recoveryAWS Elastic Disaster RecoveryDisaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)AWS Elastic Disaster Recovery Preparing for FailoverThe Berkeley/Stanford recovery-oriented computing projectWhat is AWS Fault Injection Simulator?Related videos:AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)AWS re:Invent 2019: Backup-and-restore and disaster-recovery solutions with AWS (STG208)Related examples:AWS Well-Architected Labs - Testing for Resiliency\n\nKnowledge Area: When to use read replicas\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which method should be used to improve the resiliency of user-generated content stored on AWS for a social media platform?\", \n\"answer_choices\": [{\"answer\": \"Implement an Auto Scaling Group for the instances and containers\",\"is_correct\": \"false\",\"explanation\": \"Auto Scaling Groups are used to add or remove resources based on the workload requirements. They are not used for improving resiliency.\"}, \n{\"answer\": \"Use read replicas to store the data in multiple Availability Zones\",\"is_correct\": \"true\",\"explanation\": \"Read replicas can be used to store the data in multiple Availability Zones, which improves the resiliency of the data storage.\"}, \n{\"answer\": \"Migrate the data to Amazon S3 to provide redundancy across Availability Zones\",\"is_correct\": \"false\",\"explanation\": \"While Amazon S3 can be used to provide redundancy across Availability Zones, this is not the best approach for a social media platform, as read replicas provides a better solution for managing the user-generated content.\"}, \n{\"answer\": \"Use Amazon EFS to store the data in multiple Availability Zones\",\"is_correct\": \"false\",\"explanation\": \"Amazon EFS is a file storage service, and not the best approach for a social media platform, as read replicas provides a better solution for managing the user-generated content.\"}]}", "docs": ["('REL10-BP03 Automate recovery for components constrained to a single location - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_fault_isolation_single_az_system.html')", "('SUS04-BP06 Use shared file systems or object storage to access common data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a7.html')", "('REL13-BP03 Test disaster recovery implementation to validate the implementation - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_dr_tested.html')"]}, {"scenario": "A large corporation wants to move their on-premises compute to the cloud for increased scalability and security.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Knowledge of", "item": "Workflow orchestration (for example, AWS Step Functions)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA large corporation wants to move their on-premises compute to the cloud for increased scalability and security.`\"\n\nContext:\n\n* Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Update deployed components as your needs change.Level of risk exposed if this best practice is not established: MediumEnable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.Scale using small increments for variable workloads.Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years.Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.Related documents:AWS Compute Optimizer DocumentationOperating Lambda: Performance optimizationAuto Scaling Documentation\n* Runbooks are the predefined procedures to achieve specific outcomes. Use runbooks to perform standard activities, whether done manually or automatically. Examples include deploying a workload, patching a workload, or making DNS modifications.For example, put processes in place to ensure rollback safety during deployments. Ensuring that you can roll back a deployment without any disruption for your customers is critical in making a service reliable.For runbook procedures, start with a valid effective manual process, implement it in code, and trigger it to automatically run where appropriate.Even for sophisticated workloads that are highly automated, runbooks are still useful for running game days or meeting rigorous reporting and auditing requirements.Note that playbooks are used in response to specific incidents, and runbooks are used to achieve specific outcomes. Often, runbooks are for routine activities, while playbooks are used for responding to non-routine events.Common anti-patterns:Performing unplanned changes to configuration in production.Skipping steps in your plan to deploy faster, resulting in a failed deployment.Making changes without testing the reversal of the change.Benefits of establishing this best practice: Effective change planning increases your ability to successfully execute the change because you are aware of all the systems impacted. Validating your change in test environments increases your confidence.Level of risk exposed if this best practice is not established: HighEnable consistent and prompt responses to well understood events by documenting procedures in runbooks.AWS Well-Architected Framework: Concepts: RunbookUse the principle of infrastructure as code to define your infrastructure. By using AWS CloudFormation (or a trusted third party) to define your infrastructure, you can use version control software to version and track changes.Use AWS CloudFormation (or a trusted third-party provider) to define your infrastructure.What is AWS CloudFormation?Create templates that are singular and decoupled, using good software design principles.Determine the permissions, templates, and responsible parties for implementation.Controlling access with AWS Identity and Access ManagementUse source control, like AWS CodeCommit or a trusted third-party tool, for version control.What is AWS CodeCommit?Related documents:APN Partner: partners that can help you create automated deployment solutionsAWS Marketplace: products that can be used to automate your deploymentsAWS Well-Architected Framework: Concepts: RunbookWhat is AWS CloudFormation?What is AWS CodeCommit?Related examples:Automating operations with Playbooks and Runbooks\n* Removing the ability for interactive access reduces the risk of human error, and the potential for manual configuration or management. For example, use a change management workflow to deploy Amazon Elastic Compute Cloud (Amazon EC2) instances using infrastructure-as-code, then manage Amazon EC2 instances using tools such as AWS Systems Manager instead of allowing direct access or through a bastion host. AWS Systems Manager can automate a variety of maintenance and deployment tasks, using features including automationworkflows, documents (playbooks), and the run command. AWS CloudFormation stacks build from pipelines and can automate your infrastructure deployment and management tasks without using the AWS Management Console or APIs directly.Level of risk exposed if this best practice is not established: LowReplace console access: Replace console access (SSH or RDP) to instances with AWS Systems Manager Run Command to automate management tasks.AWS Systems Manager Run CommandRelated documents:AWS Systems ManagerAWS Systems Manager Run CommandReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: Automated Deployment of Web Application Firewall\n* Existing workloads are regularly reviewed based on for each defined processes.Level of risk exposed if this best practice is not established: LowTo realize the benefits of new AWS services and features, you must execute the review process on your workloads and implement new services and features as required. For example, you might review your workloads and replace the messaging component with Amazon Simple Email Service (Amazon SES). This removes the cost of operating and maintaining a fleet of instances, while providing all the functionality at a reduced cost.Implementation stepsRegularly review the workload: Using your defined process, perform reviews with the frequency specified. Verify that you spend the correct amount of effort on each component. This process would be similar to the initial design process where you selected services for cost optimization. Analyze the services and the benefits they would bring, this time factor in the cost of making the change, not just the long-term benefits.Implement new services: If the outcome of the analysis is to implement changes, first perform a baseline of the workload to know the current cost for each output. Implement the changes, then perform an analysis to confirm the new cost for each output.Related documents:AWS News BlogTypes of Cloud ComputingWhat's New with AWS\n\nKnowledge Area: Workflow orchestration (for example, AWS Step Functions)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best way to automate a workload deployment and management tasks in the cloud?\", \n\"answer_choices\": [\n{\"answer\": \"Using AWS Systems Manager Run Command\", \"is_correct\": \"true\", \"explanation\": \"AWS Systems Manager Run Command is a tool that can automate a variety of maintenance and deployment tasks, using features including automation workflows, documents (playbooks), and the run command.\"}, \n{\"answer\": \"Using AWS CloudFormation Stacks\", \"is_correct\": \"true\", \"explanation\": \"AWS CloudFormation stacks can be used to automate your infrastructure deployment and management tasks without using the AWS Management Console or APIs directly.\"}, \n{\"answer\": \"Using interactive console access\", \"is_correct\": \"false\", \"explanation\": \"Interactive console access should be replaced with AWS Systems Manager Run Command as it provides a more secure way to update and maintain the workload configuration.\"}, \n{\"answer\": \"Using runbooks\", \"is_correct\": \"false\", \"explanation\": \"Runbooks are used to achieve specific outcomes and are for routine activities, while AWS Systems Manager Run Command is used to automate workload deployment and management tasks.\"}\n]}", "docs": ["('SUS05-BP01 Use the minimum amount of hardware to meet your needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a2.html')", "('REL08-BP01 Use runbooks for standard activities such as deployment - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_tracking_change_management_planned_changemgmt.html')", "('SEC06-BP05 Enable people to perform actions at a distance - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_actions_distance.html')", "('COST10-BP02 Review and analyze this workload regularly - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_evaluate_new_services_review_workload.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Skills in", "item": "Designing event-driven, microservice, and/or multi-tier architectures based on requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* Build highly scalable and reliable workloads using a service-oriented architecture (SOA) or a microservices architecture. Service-oriented architecture (SOA) is the practice of making software components reusable via service interfaces. Microservices architecture goes further to make components smaller and simpler.Best practices\n* Service-oriented architecture (SOA) builds services with well-delineated functions defined by business needs. Microservices use domain models and bounded context to limit this further so that each service does just one thing. Focusing on specific functionality enables you to differentiate the reliability requirements of different services, and target investments more specifically. A concise business problem and having a small team associated with each service also enables easier organizational scaling.In designing a microservice architecture, it\u2019s helpful to use Domain-Driven Design (DDD) to model the business problem using entities. For example, for the Amazon.com website, entities might include package, delivery, schedule, price, discount, and currency. Then the model is further divided into smaller models using Bounded Context, where entities that share similar features and attributes are grouped together. So, using the Amazon.com example package, delivery, and schedule would be part of the shipping context, while price, discount, and currency are part of the pricing context. With the model divided into contexts, a template for how to boundary microservices emerges.Level of risk exposed if this best practice is not established: HighDesign your workload based on your business domains and their respective functionality. Focusing on specific functionality enables you to differentiate the reliability requirements of different services, and target investments more specifically. A concise business problem and having a small team associated with each service also enables easier organizational scaling.Perform Domain Analysis to map out a domain-driven design (DDD) for your workload. Then you can choose an architecture type to meet your workload\u2019s needs.How to break a Monolith into MicroservicesGetting Started with DDD when Surrounded by Legacy SystemsEric Evans \u201cDomain-Driven Design: Tackling Complexity in the Heart of Software\u201dImplementing Microservices on AWSDecompose your services into smallest possible components. With microservices architecture you can separate your workload into components with the minimal functionality to enable organizational scaling and agility.Define the API for the workload and its design goals, limits, and any other considerations for use.Define the API.The API definition should allow for growth and additional parameters.Define the designed availabilities.Your API may have multiple design goals for different features.Establish limitsUse testing to define the limits of your workload capabilities.Related documents:Amazon API Gateway: Configuring a REST API Using OpenAPIBounded Context (a central pattern in Domain-Driven Design)Eric Evans \u201cDomain-Driven Design: Tackling Complexity in the Heart of Software\u201dGetting Started with DDD when Surrounded by Legacy SystemsHow to break a Monolith into MicroservicesImplementing Microservices on AWSMicroservice Trade-OffsMicroservices - a definition of this new architectural termMicroservices on AWS\n\nKnowledge Area: Designing event-driven, microservice, and/or multi-tier architectures based on requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice for designing a microservice architecture for a retail company's e-commerce platform to leverage AWS during peak shopping seasons?\",\n\"answer_choices\": [{\"answer\": \"Create a monolithic architecture for the e-commerce platform.\",\"is_correct\": \"false\",\"explanation\": \"Monoliths are not recommended for microservices because they are difficult to scale, maintain, and debug. It is better to break it down into small components.\"},{\"answer\": \"Decompose services into the smallest possible components using Domain-Driven Design.\",\"is_correct\": \"true\",\"explanation\": \"Using Domain-Driven Design (DDD) to model the business problem and then breaking it down into smaller components using Bounded Context is the best practice.\"},{\"answer\": \"Define the API before defining the domain analysis.\",\"is_correct\": \"false\",\"explanation\": \"It is best to start by performing Domain Analysis and then defining the API for the workload and its design goals, limits, and considerations for use.\"},{\"answer\": \"Define the availability of the workload.\",\"is_correct\": \"false\",\"explanation\": \"It is best to use testing to define the limits of your workload capabilities instead of defining the availability.\"}]}", "docs": ["('REL 3\\xa0 How do you design your workload service architecture? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-03.html')", "('REL03-BP02 Build services focused on specific business domains and functionality - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_service_architecture_business_domains.html')"]}, {"scenario": "A large corporation wants to move their on-premises compute to the cloud for increased scalability and security.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Skills in", "item": "Determining scaling strategies for components used in an architecture design", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA large corporation wants to move their on-premises compute to the cloud for increased scalability and security.`\"\n\nContext:\n\n* Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Components might become idle from lack of use because of changes in user behavior over time. Revise patterns and architecture to consolidate under-utilized components to increase overall utilization. Retire components that are no longer required. Understand the performance of your workload components, and optimize the components that consume the most resources. Be aware of the devices your customers use to access your services, and implement patterns to minimize the need for device upgrades.The following questions focus on these considerations for sustainability:Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Components might become idle from lack of use because of changes in user behavior over time. Revise patterns and architecture to consolidate under-utilized components to increase overall utilization. Retire components that are no longer required. Understand the performance of your workload components, and optimize the components that consume the most resources. Be aware of the devices your customers use to access your services, and implement patterns to minimize the need for device upgrades.Optimize software and architecture for asynchronous and scheduled jobs: Use efficient software designs and architectures to minimize the average resources required per unit of work. Implement mechanisms that result in even utilization of components to reduce resources that are idle between tasks and minimize the impact of load spikes.Remove or refactor workload components with low or no use: Monitor workload activity to identify changes in utilization of individual components over time. Remove components that are unused and no longer required, and refactor components with little utilization, to limit wasted resources.Optimize areas of code that consume the most time or resources: Monitor workload activity to identify application components that consume the most resources. Optimize the code that runs within these components to minimize resource usage while maximizing performance.Optimize impact on customer devices and equipment: Understand the devices and equipment your customers use to consume your services, their expected lifecycle, and the financial and sustainability impact of replacing those components. Implement software patterns and architectures to minimize the need for customers to replace devices and upgrade equipment. For example, implement new features using code that is backwards compatible with older hardware and operating system versions, or manage the size of payloads so they don\u2019t exceed the storage capacity of the target device.Use software patterns and architectures that best support data access and storage patterns: Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.SUS 3: How do you take advantage of software and architecture patterns to support your sustainability goals?Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Components might become idle from lack of use because of changes in user behavior over time. Revise patterns and architecture to consolidate under-utilized components to increase overall utilization. Retire components that are no longer required. Understand the performance of your workload components, and optimize the components that consume the most resources. Be aware of the devices your customers use to access your services, and implement patterns to minimize the need for device upgrades.\n* The way users consume your workloads and other resources can help you identify improvements to meet sustainability goals. Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed. Align service levels to customer needs. Position resources to limit the network required for users to consume them. Remove existing, unused assets. Identify created assets that are unused and stop generating them. Provide your team members with devices that support their needs with minimized sustainability impact.Best practices:\n* The optimal compute solution for a workload varies based on application design, usage patterns, and configuration settings. Architectures can use different compute solutions for various components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency.Best practices\n* As changes are made to improve performance, evaluate the collected metrics and data. Use this information to determine impact that the performance improvement had on the workload, the workload\u2019s components, and your customers. This measurement helps you understand the improvements that result from the tradeoff, and helps you determine if any negative side-effects were introduced.A well-architected system uses a combination of performance related strategies. Determine which strategy will have the largest positive impact on a given hotspot or bottleneck. For example, sharding data across multiple relational database systems could improve overall throughput while retaining support for transactions and, within each shard, caching can help to reduce the load.Common anti-patterns:You deploy and manage technologies manually that are available as managed services.You focus on just one component, such as networking, when multiple components could be used to increase performance of the workload.You rely on customer feedback and perceptions as your only benchmark.Benefits of establishing this best practice: For implementing performance strategies, you must select multiple services and features that, taken together, will allow you to meet your workload requirements for performance.Level of risk exposed if this best practice is not established: MediumA well-architected system uses a combination of performance-related strategies. Determine which strategy will have the largest positive impact on a given hotspot or bottleneck. For example, sharding data across multiple relational database systems could improve overall throughput while retaining support for transactions and, within each shard, caching can help to reduce the load.Related documents:Amazon Builders\u2019 LibraryAmazon CloudWatch RUMAmazon CloudWatch SyntheticsDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)Optimize applications through Amazon CloudWatch RUMDemo of Amazon CloudWatch SyntheticsRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n* Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Components might become idle from lack of use because of changes in user behavior over time. Revise patterns and architecture to consolidate under-utilized components to increase overall utilization. Retire components that are no longer required. Understand the performance of your workload components, and optimize the components that consume the most resources. Be aware of the devices your customers use to access your services, and implement patterns to minimize the need for device upgrades.Best practices:\n* Maximize performance and efficiency by evaluating internal policies and existing reference architectures and using your analysis to select services and configurations for your workload.Common anti-patterns:You allow wide use of technology selection that may impact the management overhead of your company.Benefits of establishing this best practice: Establishing a policy for architecture, technology, and vendor choices will allow decisions to be made quickly.Level of risk exposed if this best practice is not established: MediumDeploy your workload using existing policies or reference architectures: Integrate the services into your cloud deployment, then use your performance tests to ensure that you can continue to meet your performance requirements.Related documents:AWS Architecture CenterAWS Partner NetworkAWS Solutions LibraryAWS Knowledge CenterRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)This is my ArchitectureRelated examples:AWS SamplesAWS SDK Examples\n\nKnowledge Area: Determining scaling strategies for components used in an architecture design\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best way to maximize performance and efficiency when deciding which services and configurations to use for a workload in a large corporation that wants to move their on-premises compute to the cloud?\",\n\"answer_choices\": [{\"answer\": \"Allow wide use of technology selection regardless of management overhead\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern and can increase management overhead.\"},{\"answer\": \"Deploy the workload without policies or reference architectures\",\"is_correct\": \"False\",\"explanation\": \"This will significantly decrease performance and efficiency.\"},{\"answer\": \"Evaluate internal policies and existing reference architectures and use the analysis to select services and configurations\",\"is_correct\": \"True\",\"explanation\": \"This is the best approach to maximizing performance and efficiency when deciding which services and configurations to use for a workload.\"},{\"answer\": \"Use customer feedback and perceptions as the only benchmark\",\"is_correct\": \"False\",\"explanation\": \"This should not be the only benchmark as it does not provide an accurate measure of performance and efficiency.\"}]}", "docs": ["('Software and architecture patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-software-architecture-patterns.html')", "('SUS 2 How do you take advantage of user behavior patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-02.html')", "('PERF 2\\xa0 How do you select your compute solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-02.html')", "('PERF08-BP04 Measure the impact of performance improvements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_measure.html')", "('SUS 3 How do you take advantage of software and architecture patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-03.html')", "('PERF01-BP04 Use policies or reference architectures - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_performing_architecture_use_policies.html')"]}, {"scenario": "A company wants to build a mobile application that needs to access and store data in the cloud.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Skills in", "item": "Determining the AWS services required to achieve loose coupling based on requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to build a mobile application that needs to access and store data in the cloud.`\"\n\nContext:\n\n* Dependencies such as queuing systems, streaming systems, workflows, and load balancers are loosely coupled. Loose coupling helps isolate behavior of a component from other components that depend on it, increasing resiliency and agility.If changes to one component force other components that rely on it to also change, then they are tightly coupled. Loose coupling breaks this dependency so that dependent components only need to know the versioned and published interface. Implementing loose coupling between dependencies isolates a failure in one from impacting another.Loose coupling enables you to add additional code or features to a component while minimizing risk to components that depend on it. Also, scalability is improved as you can scale out or even change underlying implementation of the dependency.To further improve resiliency through loose coupling, make component interactions asynchronous where possible. This model is suitable for any interaction that does not need an immediate response and where an acknowledgment that a request has been registered will suffice. It involves one component that generates events and another that consumes them. The two components do not integrate through direct point-to-point interaction but usually through an intermediate durable storage layer, such as an SQS queue or a streaming data platform such as Amazon Kinesis, or AWS Step Functions.Figure 4: Dependencies such as queuing systems and load balancers are loosely coupledAmazon SQS queues and Elastic Load Balancers are just two ways to add an intermediate layer for loose coupling. Event-driven architectures can also be built in the AWS Cloud using Amazon EventBridge, which can abstract clients (event producers) from the services they rely on (event consumers). Amazon Simple Notification Service (Amazon SNS) is an effective solution when you need high-throughput, push-based, many-to-many messaging. Using Amazon SNS topics, your publisher systems can fan out messages to a large number of subscriber endpoints for parallel processing.While queues offer several advantages, in most hard real-time systems, requests older than a threshold time (often seconds) should be considered stale (the client has given up and is no longer waiting for a response), and not processed. This way more recent (and likely still valid requests) can be processed instead.Common anti-patterns:Deploying a singleton as part of a workload.Directly invoking APIs between workload tiers with no capability of failover or asynchronous processing of the request.Benefits of establishing this best practice: Loose coupling helps isolate behavior of a component from other components that depend on it, increasing resiliency and agility. Failure in one component is isolated from others.Level of risk exposed if this best practice is not established: HighImplement loosely coupled dependencies. Dependencies such as queuing systems, streaming systems, workflows, and load balancers are loosely coupled. Loose coupling helps isolate behavior of a component from other components that depend on it, increasing resiliency and agility.AWS re:Invent 2019: Moving to event-driven architectures (SVS308)What Is Amazon EventBridge?What Is Amazon Simple Queue Service?Amazon EventBridge allows you to build event driven architectures, which are loosely coupled and distributed.AWS New York Summit 2019: Intro to Event-driven Architectures and Amazon EventBridge (MAD205)If changes to one component force other components that rely on it to also change, then they are tightly coupled. Loose coupling breaks this dependency so that dependency components only need to know the versioned and published interface.Make component interactions asynchronous where possible. This model is suitable for any interaction that does not need an immediate response and where an acknowledgement that a request has been registered will suffice.AWS re:Invent 2019: Scalable serverless event-driven applications using Amazon SQS and Lambda (API304)Related documents:AWS re:Invent 2019: Moving to event-driven architectures (SVS308)Amazon EC2: Ensuring IdempotencyThe Amazon Builders' Library: Challenges with distributed systemsThe Amazon Builders' Library: Reliability, constant work, and a good cup of coffeeWhat Is Amazon EventBridge?What Is Amazon Simple Queue Service?Related videos:AWS New York Summit 2019: Intro to Event-driven Architectures and Amazon EventBridge (MAD205)AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small ARC337 (includes loose coupling, constant work, static stability)AWS re:Invent 2019: Moving to event-driven architectures (SVS308)AWS re:Invent 2019: Scalable serverless event-driven applications using Amazon SQS and Lambda (API304)\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n\nKnowledge Area: Determining the AWS services required to achieve loose coupling based on requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service should be used to achieve loose coupling based on the provided requirements?\", \"answer_choices\": [{\"answer\": \"Amazon RDS\",\"is_correct\": \"False\",\"explanation\": \"Amazon RDS is a relational database service that does not provide loose coupling.\"},{\"answer\": \"Amazon DynamoDB\",\"is_correct\": \"False\",\"explanation\": \"Amazon DynamoDB is a NoSQL database service that does not provide loose coupling.\"},{\"answer\": \"AWS Lambda\",\"is_correct\": \"False\",\"explanation\": \"AWS Lambda is a compute service that does not provide loose coupling.\"},{\"answer\": \"Amazon Simple Queue Service (Amazon SQS)\",\"is_correct\": \"True\",\"explanation\": \"Amazon SQS is a message queuing service that provides loose coupling. It is an effective solution when you need high-throughput, push-based, many-to-many messaging using Amazon SNS topics, your publisher systems can fan out messages to a large number of subscriber endpoints for parallel processing.\"}]}", "docs": ["('REL04-BP02 Implement loosely coupled dependencies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_prevent_interaction_failure_loosely_coupled_system.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')"]}, {"scenario": "A company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Skills in", "item": "Determining when to use containers", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.`\"\n\nContext:\n\n* Understand how your workload can benefit from the use of different compute options, such as instances, containers and functions.Desired outcome: By understanding all of the compute options available, you will be aware of the opportunities to increase performance, reduce unnecessary infrastructure costs, and lower the operational effort required to maintain your workload. You can also accelerate your time to market when you deploy new services and features.Common anti-patterns:In a post-migration workload, using the same compute solution that was being used on premises.Lacking awareness of the cloud compute solutions and how those solutions might improve your compute performance.Oversizing an existing compute solution to meet scaling or performance requirements, when an alternative compute solution would align to your workload characteristics more precisely.Benefits of establishing this best practice: By identifying the compute requirements and evaluating the available compute solutions, business stakeholders and engineering teams will understand the benefits and limitations of using the selected compute solution. The selected compute solution should fit the workload performance criteria. Key criteria include processing needs, traffic patterns, data access patterns, scaling needs, and latency requirements.Level of risk exposed if this best practice is not established: HighUnderstand the virtualization, containerization, and management solutions that can benefit your workload and meet your performance requirements. A workload can contain multiple types of compute solutions. Each compute solution has differing characteristics. Based on your workload scale and compute requirements, a compute solution can be selected and configured to meet your needs. The cloud architect should learn the advantages and disadvantages of instances, containers, and functions. The following steps will help you through how to select your compute solution to match your workload characteristics and performance requirements.Implementation steps:Select the location of where the compute solution must reside by evaluating PERF05-BP06 Choose your workload\u2019s location based on network requirements. This location will limit the types of compute solution available to you.Identify the type of compute solution that works with the location requirement and application requirementsAmazon Elastic Compute Cloud (Amazon EC2) virtual server instances come in a wide variety of different families and sizes. They offer a wide variety of capabilities, including solid state drives (SSDs) and graphics processing units (GPUs). EC2 instances offer the greatest flexibility on instance choice. When you launch an EC2 instance, the instance type that you specify determines the hardware of your instance. Each instance type offers different compute, memory, and storage capabilities. Instance types are grouped in instance families based on these capabilities. Typical use cases include: running enterprise applications, high performance computing (HPC), training and deploying machine learning applications and running cloud native applications.Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that allows you to automatically run and manage containers on a cluster of EC2 instances or serverless instances using AWS Fargate. You can use Amazon ECS with other services such as Amazon Route 53, Secrets Manager, AWS Identity and Access Management (IAM), and Amazon CloudWatch. Amazon ECS is recommended if your application is containerized and your engineering team prefers Docker containers.Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. You can choose to run your EKS clusters using AWS Fargate, removing the need to provision and manage servers. Managing Amazon EKS is simplified due to integrations with AWS Services such as Amazon CloudWatch, Auto Scaling Groups, AWS Identity and Access Management (IAM), and Amazon Virtual Private Cloud (VPC). When using containers, you must use compute metrics to select the optimal type for your workload, similar to how you use compute metrics to select your EC2 or AWS Fargate instance types. Amazon EKS is recommended if your application is containerized and your engineering team prefers Kubernetes over Docker containers.You can use AWS Lambda to run code that supports the allowed runtime, memory, and CPU options. Simply upload your code, and AWS Lambda will manage everything required to run and scale that code. You can set up your code to automatically trigger from other AWS services or call it directly. Lambda is recommended for short running, microservice architectures developed for the cloud.After you have experimented with your new compute solution, plan your migration and validate your performance metrics. This is a continual process, see PERF02-BP04 Determine the required configuration by right-sizing.Level of effort for the implementation plan: If a workload is moving from one compute solution to another, there could be a moderate level of effort involved in refactoring the application.Related documents:Cloud Compute with AWSEC2 Instance TypesProcessor State Control for Your EC2 InstanceEKS Containers: EKS Worker NodesAmazon ECS Containers: Amazon ECS Container InstancesFunctions: Lambda Function ConfigurationPrescriptive Guidance for ContainersPrescriptive Guidance for ServerlessRelated videos:How to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Amazon EC2 foundations (CMP211-R2)Powering next-gen Amazon EC2: Deep dive into the Nitro systemDeliver high-performance ML inference with AWS Inferentia (CMP324-R1)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Related examples:Migrating the web application to containersRun a Serverless Hello WorldTypeServerContainersFunctionAWS serviceAmazon Elastic Compute Cloud (Amazon EC2)Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS)AWS LambdaKey CharacteristicsHas dedicated option for hardware license requirements, Placement Options, and a large selection of different instance families based on compute metricsEasy deployment, consistent environments, runs on top of EC2 instances, ScalableShort runtime (15 minutes or less), maximum memory and CPU are not as high as other services, Managed hardware layer, Scales to millions of concurrent requestsCommon use-casesLift and shift migrations, monolithic application, hybrid environments, enterprise applicationsMicroservices, hybrid environments,Microservices, event-driven applications\n\nKnowledge Area: Determining when to use containers\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Given the scenario, which is the best compute option to minimize costs when batch processing jobs are not running?\", \n\"answer_choices\": [{\"answer\": \"Amazon Elastic Compute Cloud (Amazon EC2)\",\"is_correct\": \"False\",\"explanation\": \"EC2 virtual server instances come with a wide variety of options, but are more expensive compared to other solutions.\"},{\"answer\": \"AWS Lambda\",\"is_correct\": \"True\",\"explanation\": \"Lambda is recommended for short running, microservice architectures developed for the cloud and is cost-efficient.\"},{\"answer\": \"Amazon Elastic Container Service (Amazon ECS)\",\"is_correct\": \"False\",\"explanation\": \"ECS is recommended if your application is containerized and your engineering team prefers Docker containers, but is more expensive compared to other solutions.\"},{\"answer\": \"Amazon Elastic Kubernetes Service (Amazon EKS)\",\"is_correct\": \"False\",\"explanation\": \"EKS is recommended if your application is containerized and your engineering team prefers Kubernetes over Docker containers, but is more expensive compared to other solutions.\"}]}", "docs": ["('PERF02-BP01 Evaluate the available compute options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_evaluate_options.html')"]}, {"scenario": "A software development team wants to build and deploy a microservices-based application on AWS.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Skills in", "item": "Determining when to use serverless technologies and patterns", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA software development team wants to build and deploy a microservices-based application on AWS.`\"\n\nContext:\n\n* There are five design principles for performance efficiency in the cloud:Democratize advanced technologies: Make advanced technology implementation easier for your team by delegating complex tasks to your cloud vendor. Rather than asking your IT team to learn about hosting and running a new technology, consider consuming the technology as a service. For example, NoSQL databases, media transcoding, and machine learning are all technologies that require specialized expertise. In the cloud, these technologies become services that your team can consume, allowing your team to focus on product development rather than resource provisioning and management.Go global in minutes: Deploying your workload in multiple AWS Regions around the world allows you to provide lower latency and a better experience for your customers at minimal cost.Use serverless architectures: Serverless architectures remove the need for you to run and maintain physical servers for traditional compute activities. For example, serverless storage services can act as static websites (removing the need for web servers) and event services can host code. This removes the operational burden of managing physical servers, and can lower transactional costs because managed services operate at cloud scale.Experiment more often: With virtual and automatable resources, you can quickly carry out comparative testing using different types of instances, storage, or configurations.Consider mechanical sympathy: Understand how cloud services are consumed and always use the technology approach that aligns best with your workload goals. For example, consider data access patterns when you select database or storage approaches.\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n\nKnowledge Area: Determining when to use serverless technologies and patterns\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best approach when determining when to use serverless technologies and patterns?\", \"answer_choices\": [{\"answer\": \"Evaluate the cost of the technology and provisioning of physical servers.\",\"is_correct\": \"False\",\"explanation\": \"While cost is an important factor for determining when to use serverless technologies, it is not the best approach. The best approach is to understand how cloud services are consumed and use the technology approach that aligns best with the workload goals.\"},{\"answer\": \"Choose open source software that does not have license fees.\",\"is_correct\": \"False\",\"explanation\": \"Minimizing license costs is important, but it is not the best approach when determining when to use serverless technologies and patterns.\"},{\"answer\": \"Understand how cloud services are consumed and use the technology approach that aligns best with your workloads goals.\",\"is_correct\": \"True\",\"explanation\": \"Understanding how cloud services are consumed and using the technology approach that aligns best with the workload goals is the best approach when determining when to use serverless technologies and patterns.\"},{\"answer\": \"Delegate complex tasks to the cloud vendor and focus on product development.\",\"is_correct\": \"False\",\"explanation\": \"This is one of the five design principles for performance efficiency in the cloud, but it is not the best approach when determining when to use serverless technologies and patterns.\"}]}", "docs": ["('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-dp.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Skills in", "item": "Recommending appropriate compute, storage, networking, and database technologies based on requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* The optimal compute solution for a workload varies based on application design, usage patterns, and configuration settings. Architectures can use different compute solutions for various components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency.Best practices\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n* There are four best practice areas for performance efficiency in the cloud:SelectionReviewMonitoringTradeoffsTake a data-driven approach to building a high-performance architecture. Gather data on all aspects of the architecture, from the high-level design to the selection and configuration of resource types.Reviewing your choices on a regular basis ensures that you are taking advantage of the continually evolving AWS Cloud. Monitoring ensures that you are aware of any deviance from expected performance. Make trade-o\ufb00s in your architecture to improve performance, such as using compression or caching, or relaxing consistency requirements.\n* Compute resources in your workload require multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more.Best practices\n* The optimal solution for a particular workload varies, and solutions often combine multiple approaches. Well-architected workloads use multiple solutions and enable different features to improve performance.AWS resources are available in many types and configurations, which makes it easier to find an approach that closely matches your workload needs. You can also find options that are not easily achievable with on-premises infrastructure. For example, a managed service such as Amazon DynamoDB provides a fully managed NoSQL database with single-digit millisecond latency at any scale.The following question focuses on these considerations for performance efficiency. (For a list of performance efficiency questions and best practices, see the Appendix.).Use a data-driven approach to select the patterns and implementation for your architecture and achieve a cost effective solution. AWS Solutions Architects, AWS Reference Architectures, and AWS Partner Network (APN) partners can help you select an architecture based on industry knowledge, but data obtained through benchmarking or load testing will be required to optimize your architecture.Your architecture will likely combine a number of different architectural approaches (for example, event-driven, ETL, or pipeline). The implementation of your architecture will use the AWS services that are specific to the optimization of your architecture's performance. In the following sections we discuss the four main resource types to consider (compute, storage, database, and network).PERF 1: How do you select the best performing architecture?Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.\n* Cloud technologies are rapidly evolving and you must ensure that workload components are using the latest technologies and approaches to continually improve performance. You must continually evaluate and consider changes to your workload components to ensure you are meeting its performance and cost objectives. New technologies, such as machine learning and artificial intelligence (AI), can allow you to reimagine customer experiences and innovate across all of your business workloads.Take advantage of the continual innovation at AWS driven by customer need. We release new Regions, edge locations, services, and features regularly. Any of these releases could positively improve the performance efficiency of your architecture.The following question focuses on these considerations for performance efficiency.Architectures performing poorly are usually the result of a non-existent or broken performance review process. If your architecture is performing poorly, implementing a performance review process will allow you to apply Deming\u2019s plan-do-check-act (PDCA) cycle to drive iterative improvement.PERF 6: How do you evolve your workload to take advantage of new releases?When architecting workloads, there are finite options that you can choose from. However, over time, new technologies and approaches become available that could improve the performance of your workload.\n* Optimize resources provided to team members to minimize the sustainability impact while supporting their needs. For example, perform complex operations, such as rendering and compilation, on highly utilized shared cloud desktops instead of on underutilized high-powered single-user systems.Level of risk exposed if this best practice is not established: LowProvision workstations and other devices to align with how they\u2019re used.Use virtual desktops and application streaming to limit upgrade and device requirements.Move processor or memory-intensive tasks to the cloud.Evaluate the impact of processes and systems on your device lifecycle, and select solutions that minimize the requirement for device replacement while satisfying business requirements.Implement remote management for devices to reduce required business travel.Related documents:What is Amazon WorkSpaces?Amazon AppStream 2.0 DocumentationNICE DCVAWS Systems Manager Fleet ManagerRelated videos:Building Sustainably on AWS\n\nKnowledge Area: Recommending appropriate compute, storage, networking, and database technologies based on requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When recommending appropriate compute, storage, networking, and database technologies for a financial services company's highly available and secure infrastructure to process financial transactions, which would be the best practice?\",\n\"answer_choices\": [{\"answer\": \"Standardize all instance types for ease of management\",\"is_correct\": \"false\",\"explanation\": \"Standardizing all instances types for ease of management does not necessarily provide the best performance efficiency for the workload.\"},{\"answer\": \"Choose memory-optimized instances for systems that require more memory than CPU\",\"is_correct\": \"true\",\"explanation\": \"Memory-optimized instances are best for systems that require more memory than CPU, while compute-optimized instances are best for components that do data processing that is not memory-intensive.\"},{\"answer\": \"Choose the largest instance available for all workloads\",\"is_correct\": \"false\",\"explanation\": \"Choosing the largest instance available for all workloads does not necessarily provide the best performance efficiency for the workload.\"},{\"answer\": \"Perform a data-driven approach to select the patterns and implementation for the architecture\",\"is_correct\": \"true\",\"explanation\": \"A data-driven approach to select the patterns and implementation for the architecture is the best practice as it allows you to determine the correct solution for your various workloads and optimize performance and overall efficiency.\"}]}", "docs": ["('PERF 2\\xa0 How do you select your compute solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-02.html')", "('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')", "('Definition - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-def.html')", "('SEC 6\\xa0 How do you protect your compute resources? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-06.html')", "('Selection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-sel.html')", "('Review - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-review.html')", "('SUS02-BP05 Optimize team member resources for activities performed - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_user_a6.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 1: Design scalable and loosely coupled architectures", "focus": "Skills in", "item": "Using purpose-built AWS services for workloads", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n* Managed services shift responsibility for maintaining high-average utilization, and sustainability optimization of the deployed hardware to AWS. Use managed services to distribute the sustainability impact of the service across all tenants of the service, reducing your individual contribution.Level of risk exposed if this best practice is not established: LowMigrate from self-hosted services to managed services. For example, use managed Amazon Relational Database Service (Amazon RDS) instances instead of maintaining your own Amazon RDS instances on Amazon Elastic Compute Cloud (Amazon EC2), or use managed container services, such as AWS Fargate, instead of implementing your own container infrastructure.Related documents:AWS FargateAmazon DocumentDBAmazon Elastic Kubernetes Service (EKS)Amazon Managed Streaming for Apache Kafka (Amazon MSK)Amazon RedshiftAmazon Relational Database Service (RDS)\n* Use metrics from the currently running workload to select the right size and type to optimize for cost. Appropriately provision throughput, sizing, and storage for services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Elastic Block Store (Amazon EBS) (PIOPS), Amazon Relational Database Service (Amazon RDS), Amazon EMR, and networking. This can be done with a feedback loop such as automatic scaling or by custom code in the workload.Level of risk exposed if this best practice is not established: LowCreate a feedback loop within the workload that uses active metrics from the running workload to make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you configure to perform the right sizing operations for you. AWS also provides APIs, SDKs, and features that allow resources to be modified with minimal effort. You can program a workload to stop-and-start an Amazon Elastic Compute Cloud(Amazon EC2) instance to allow a change of instance size or instance type. This provides the benefits of right-sizing while removing almost all the operational cost required to make the change.Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage Service(Amazon S3) Intelligent-Tiering. Amazon S3 Intelligent-Tiering automatically moves your data between two access tiers: frequent access and infrequent access, based on your usage patterns.Implementation stepsConfigure workload metrics: Ensure you capture the key metrics for the workload. These metrics provide an indication of the customer experience, such as the workload output, and align to the differences between resource types and sizes, such as CPU and memory usage.View rightsizing recommendations: Use the rightsizing recommendations in AWS Compute Optimizer to make adjustments to your workload.Select resource type and size automatically based on metrics: Using the workload metrics, manually or automatically select your workload resources. Configuring AWS Auto Scaling or implementing code within your application can reduce the effort required if frequent changes are needed, and it can potentially implement changes sooner than a manual process.Related documents:AWS Auto ScalingAWS Compute OptimizerAmazon CloudWatch featuresCloudWatch Getting Set UpCloudWatch Publishing Custom MetricsCost Optimization: Amazon EC2 Right SizingGetting Started with Amazon EC2 Auto ScalingAmazon S3 Intelligent-TieringLaunch an EC2 Instance Using the SDK\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n\nKnowledge Area: Using purpose-built AWS services for workloads\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service should the manufacturing company use to reduce their security maintenance tasks and increase their free time to focus on securing their application in other ways?\",\n \"answer_choices\": [{\"answer\": \"Amazon EC2\",\"is_correct\": \"false\",\"explanation\": \"Amazon EC2 is a building-block service and does not manage resources.\"}, \n{\"answer\": \"Amazon EBS\",\"is_correct\": \"false\",\"explanation\": \"Amazon EBS is a building-block service and does not manage resources.\"},\n{\"answer\": \"Amazon RDS\",\"is_correct\": \"true\",\"explanation\": \"Amazon RDS is a managed service and helps set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups.\"},\n{\"answer\": \"AWS Lambda\",\"is_correct\": \"false\",\"explanation\": \"AWS Lambda is a managed service that allows you to run code without provisioning or managing servers, however it does not manage resources.\"}]}", "docs": ["('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')", "('SUS05-BP03 Use managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a4.html')", "('COST06-BP03 Select resource type, size, and number automatically based on metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_metrics.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')"]}, {"scenario": "A research institution wants to perform data analysis on a large dataset using AWS compute resources.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "AWS global infrastructure (for example, Availability Zones, AWS Regions, Amazon Route 53)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA research institution wants to perform data analysis on a large dataset using AWS compute resources.`\"\n\nContext:\n\n* Ensure that if a resource failure occurs, that healthy resources can continue to serve requests. For location failures (such as Availability Zone or AWS Region) ensure that you have systems in place to fail over to healthy resources in unimpaired locations.AWS services, such as Elastic Load Balancing and Amazon EC2 Auto Scaling, help distribute load across resources and Availability Zones. Therefore, failure of an individual resource (such as an EC2 instance) or impairment of an Availability Zone can be mitigated by shifting traffic to remaining healthy resources. For multi-region workloads, this is more complicated. For example, cross-region read replicas enable you to deploy your data to multiple AWS Regions, but you still must promote the read replica to primary and point your traffic at it in the event of a failover. Amazon Route 53 and AWS Global Accelerator can help route traffic across AWS Regions.If your workload is using AWS services, such as Amazon S3 or Amazon DynamoDB, then they are automatically deployed to multiple Availability Zones. In case of failure, the AWS control plane automatically routes traffic to healthy locations for you. Data is redundantly stored in multiple Availability Zones, and remains available. For Amazon RDS, you must choose Multi-AZ as a configuration option, and then on failure AWS automatically directs traffic to the healthy instance. For Amazon EC2 instances, Amazon ECS tasks, or Amazon EKS pods, you choose which Availability Zones to deploy to. Elastic Load Balancing then provides the solution to detect instances in unhealthy zones and route traffic to the healthy ones. Elastic Load Balancing can even route traffic to components in your on-premises data center.For Multi-Region approaches (which might also include on-premises data centers), Amazon Route 53 provides a way to define internet domains, and assign routing policies that can include health checks to ensure that traffic is routed to healthy regions. Alternately, AWS Global Accelerator provides static IP addresses that act as a fixed entry point to your application, then routes to endpoints in AWS Regions of your choosing, using the AWS global network instead of the internet for better performance and reliability.AWS approaches the design of our services with fault recovery in mind. We design services to minimize the time to recover from failures and impact on data. Our services primarily use data stores that acknowledge requests only after they are durably stored across multiple replicas within a Region. These services and resources include Amazon Aurora, Amazon Relational Database Service (Amazon RDS) Multi-AZ DB instances, Amazon S3, Amazon DynamoDB, Amazon Simple Queue Service (Amazon SQS), and Amazon Elastic File System (Amazon EFS). They are constructed to use cell-based isolation and use the fault isolation provided by Availability Zones. We use automation extensively in our operational procedures. We also optimize our replace-and-restart functionality to recover quickly from interruptions.Level of risk exposed if this best practice is not established: HighFail over to healthy resources. Ensure that if a resource failure occurs, that healthy resources can continue to serve requests. For location failures (such as Availability Zone or AWS Region) ensure you have systems in place to fail over to healthy resources in unimpaired locations.If your workload is using AWS services, such as Amazon S3 or Amazon DynamoDB, then they are automatically deployed to multiple Availability Zones. In case of failure, the AWS control plane automatically routes traffic to healthy locations for you.For Amazon RDS you must choose Multi-AZ as a configuration option, and then on failure AWS automatically directs traffic to the healthy instance.High Availability (Multi-AZ) for Amazon RDSFor Amazon EC2 instances or Amazon ECS tasks, you choose which Availability Zones to deploy to. Elastic Load Balancing then provides the solution to detect instances in unhealthy zones and route traffic to the healthy ones. Elastic Load Balancing can even route traffic to components in your on-premises data center.For multi-region approaches (which might also include on-premises data centers), ensure that data and resources from healthy locations can continue to serve requestsFor example, cross-region read replicas enable you to deploy your data to multiple AWS Regions, but you still must promote the read replica to master and point your traffic at it in the event of a primary location failure.Overview of Amazon RDS Read ReplicasAmazon Route 53 provides a way to define internet domains, and assign routing policies, which might include health checks, to ensure that traffic is routed to healthy Regions. Alternately, AWS Global Accelerator provides static IP addresses that act as a fixed entry point to your application, then routes to endpoints in AWS Regions of your choosing, using the AWS global network instead of the public internet for better performance and reliability.Amazon Route 53: Choosing a Routing PolicyWhat Is AWS Global Accelerator?Related documents:APN Partner: partners that can help with automation of your fault toleranceAWS Marketplace: products that can be used for fault toleranceAWS OpsWorks: Using Auto Healing to Replace Failed InstancesAmazon Route 53: Choosing a Routing PolicyHigh Availability (Multi-AZ) for Amazon RDSOverview of Amazon RDS Read ReplicasAmazon ECS task placement strategiesCreating Kubernetes Auto Scaling Groups for Multiple Availability ZonesWhat is AWS Global Accelerator?Related examples:Well-Architected lab: Level 300: Implementing Health Checks and Managing Dependencies to Improve Reliability\n\nKnowledge Area: AWS global infrastructure (for example, Availability Zones, AWS Regions, Amazon Route 53)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What service should a research institution use to ensure that if a resource failure occurs, that healthy resources can continue to serve requests across multiple AWS Regions?\",\n\"answer_choices\": [{\"answer\": \"Amazon RDS Multi-AZ\",\"is_correct\": \"false\",\"explanation\": \"Amazon RDS Multi-AZ is used to ensure availability within a single Availability Zone or AWS Region, but not across multiple Regions.\"},{\"answer\": \"Amazon ECS task placement strategies\",\"is_correct\": \"false\",\"explanation\": \"This service is used to deploy resources to multiple Availability Zones, but not to multiple Regions.\"},{\"answer\": \"Amazon Route 53\",\"is_correct\": \"false\",\"explanation\": \"Amazon Route 53 can route traffic to specific Regions, but is not designed to fail over to healthy regions.\"},{\"answer\": \"AWS Global Accelerator\",\"is_correct\": \"true\",\"explanation\": \"AWS Global Accelerator provides static IP addresses that act as a fixed entry point to your application, then routes to endpoints in AWS Regions of your choosing, using the AWS global network instead of the public internet for better performance and reliability.\"}]\n}", "docs": ["('REL11-BP02 Fail over to healthy resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_withstand_component_failures_failover2good.html')"]}, {"scenario": "A startup wants to host their new web application on AWS.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "AWS managed services with appropriate use cases (for example, Amazon Comprehend, Amazon Polly)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA startup wants to host their new web application on AWS.`\"\n\nContext:\n\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n\nKnowledge Area: AWS managed services with appropriate use cases (for example, Amazon Comprehend, Amazon Polly)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS managed service should a startup use to host their new web application?\",\n\"answer_choices\": [\n{\"answer\": \"Amazon EC2\",\"is_correct\": \"false\",\"explanation\": \"Amazon EC2 is a building-block service and cannot be used to host a web application directly. It is used to provide compute resources.\"},\n{\"answer\": \"Amazon EBS\",\"is_correct\": \"false\",\"explanation\": \"Amazon EBS is a building-block service and cannot be used to host a web application directly. It is used to provide persistent storage for Amazon EC2 instances.\"},\n{\"answer\": \"Amazon S3\",\"is_correct\": \"false\",\"explanation\": \"Amazon S3 is a building-block service and cannot be used to host a web application directly. It is used to provide object storage.\"},\n{\"answer\": \"Amazon Relational Database Service (Amazon RDS)\",\"is_correct\": \"true\",\"explanation\": \"Amazon Relational Database Service (Amazon RDS) is an AWS managed service that can be used to host a web application. It automates administration tasks such as hardware provisioning, database setup, patching, and backups by reducing the administrative and operational overhead for the startup.\"}\n]}", "docs": ["('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')"]}, {"scenario": "A company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "Disaster recovery (DR) strategies (for example, backup and restore, pilot light, warm standby, active-active failover, recovery point objective [RPO], recovery time objective [RTO])", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.`\"\n\nContext:\n\n* Having backups and redundant workload components in place is the start of your DR strategy. RTO and RPO are your objectives for restoration of your workload. Set these based on business needs. Implement a strategy to meet these objectives, considering locations and function of workload resources and data. The probability of disruption and cost of recovery are also key factors that help to inform the business value of providing disaster recovery for a workload.Best practices\n* Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO).Best practices\n* Define a disaster recovery (DR) strategy that meets your workload's recovery objectives. Choose a strategy such as: backup and restore; standby (active/passive); or active/active.\n* Regularly test failover to your recovery site to ensure proper operation, and that RTO and RPO are met.A pattern to avoid is developing recovery paths that are rarely exercised. For example, you might have a secondary data store that is used for read-only queries. When you write to a data store and the primary fails, you might want to fail over to the secondary data store. If you don\u2019t frequently test this failover, you might find that your assumptions about the capabilities of the secondary data store are incorrect. The capacity of the secondary, which might have been sufficient when you last tested, might be no longer be able to tolerate the load under this scenario. Our experience has shown that the only error recovery that works is the path you test frequently. This is why having a small number of recovery paths is best. You can establish recovery patterns and regularly test them. If you have a complex or critical recovery path, you still need to regularly exercise that failure in production to convince yourself that the recovery path works. In the example we just discussed, you should fail over to the standby regularly, regardless of need.Common anti-patterns:Never exercise failovers in production.Benefits of establishing this best practice: Regularly testing you disaster recovery plan ensures that it will work when it needs to, and that your team knows how to execute the strategy.Level of risk exposed if this best practice is not established: HighEngineer your workloads for recovery. Regularly test your recovery paths Recovery Oriented Computing identifies the characteristics in systems that enhance recovery. These characteristics are: isolation and redundancy, system-wide ability to roll back changes, ability to monitor and determine health, ability to provide diagnostics, automated recovery, modular design, and ability to restart. Exercise the recovery path to ensure that you can accomplish the recovery in the specified time to the specified state. Use your runbooks during this recovery to document problems and find solutions for them before the next test.The Berkeley/Stanford recovery-oriented computing projectUse AWS Elastic Disaster Recovery to implement and launch drill instances for your DR strategy.AWS Elastic Disaster Recovery Preparing for FailoverWhat is Elastic Disaster Recovery?AWS Elastic Disaster RecoveryRelated documents:APN Partner: partners that can help with disaster recoveryAWS Architecture Blog: Disaster Recovery SeriesAWS Marketplace: products that can be used for disaster recoveryAWS Elastic Disaster RecoveryDisaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)AWS Elastic Disaster Recovery Preparing for FailoverThe Berkeley/Stanford recovery-oriented computing projectWhat is AWS Fault Injection Simulator?Related videos:AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)AWS re:Invent 2019: Backup-and-restore and disaster-recovery solutions with AWS (STG208)Related examples:AWS Well-Architected Labs - Testing for Resiliency\n\nKnowledge Area: Disaster recovery (DR) strategies (for example, backup and restore, pilot light, warm standby, active-active failover, recovery point objective [RPO], recovery time objective [RTO])\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is an appropriate disaster recovery strategy for the company's batch processing jobs on AWS, in order to minimize costs when the jobs are not running?\",\n\"answer_choices\": [{\"answer\": \"Backup and restore\",\"is_correct\": \"False\",\"explanation\": \"Backup and restore is a good strategy for data recovery, but it is not the best strategy for minimizing costs when the jobs are not running.\"},{\"answer\": \"Active-active failover\",\"is_correct\": \"False\",\"explanation\": \"Active-active failover is a good strategy for ensuring high availability, but it is not the best strategy for minimizing costs when the jobs are not running.\"},{\"answer\": \"Pilot light\",\"is_correct\": \"True\",\"explanation\": \"Pilot light is a good strategy for minimizing costs when the jobs are not running, as it is a low-cost strategy that keeps a minimal number of resources running in the cloud while the rest of the resources are shut down.\"},{\"answer\": \"Warm standby\",\"is_correct\": \"False\",\"explanation\": \"Warm standby is a good strategy for failover, but it is not the best strategy for minimizing costs when the jobs are not running.\"}]}", "docs": ["('REL 13\\xa0 How do you plan for disaster recovery (DR)? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-13.html')", "('REL 9\\xa0 How do you back up data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-09.html')", "('REL13-BP02 Use defined recovery strategies to meet the recovery objectives - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_disaster_recovery.html')", "('REL13-BP03 Test disaster recovery implementation to validate the implementation - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_dr_tested.html')"]}, {"scenario": "A software development team wants to build and deploy a microservices-based application on AWS.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "Distributed design patterns", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA software development team wants to build and deploy a microservices-based application on AWS.`\"\n\nContext:\n\n* The workload has a recovery time objective (RTO) and recovery point objective (RPO).\n* Research and understand the various design patterns and services that help improve workload performance. As part of the analysis, identify what you could trade to achieve higher performance. For example, using a cache service can help to reduce the load placed on database systems. However, caching can introduce eventual consistency and requires engineering effort to implement within business requirements and customer expectations.Desired outcome: Researching design patterns will lead you to choosing an architecture design that will support the best performing system. Learn which performance configuration options are available to you and how they could impact the workload. Optimizing the performance of your workload depends on understanding how these options interact with your architecture and the impact they will have on both measured performance and the performance perceived by end users.Common anti-patterns:You assume that all traditional IT workload performance strategies are best suited for cloud workloads.You build and manage caching solutions instead of using managed services.You use the same design pattern for all your workloads without evaluating which pattern would improve the workload performance.Benefits of establishing this best practice: By selecting the right design pattern and services for your workload you will be optimizing your performance, improving operational excellence and increasing reliability. The right design pattern will meet your current workload characteristics and help you scale for future growth or changes.Level of risk exposed if this best practice is not established: HighLearn which performance configuration options are available and how they could impact the workload. Optimizing the performance of your workload depends on understanding how these options interact with your architecture, and the impact they have on measured performance and user-perceived performance.Implementation steps:Evaluate and review design patterns that would improve your workload performance.The Amazon Builders\u2019 Library provides you with a detailed description of how Amazon builds and operates technology. These articles are written by senior engineers at Amazon and cover topics across architecture, software delivery, and operations.AWS Solutions Library is a collection of ready-to-deploy solutions that assemble services, code, and configurations. These solutions have been created by AWS and AWS Partners based on common use cases and design patterns grouped by industry or workload type. For example, you can set up a distributed load testing solution for your workload.AWS Architecture Center provides reference architecture diagrams grouped by design pattern, content type, and technology.AWS samples is a GitHub repository full of hands-on examples to help you explore common architecture patterns, solutions, and services. It is updated frequently with the newest services and examples.Improve your workload to model the selected design patterns and use services and the service configuration options to improve your workload performance.Train your internal team with resources available at AWS Skills Guild.Use the AWS Partner Network to provide expertise quickly and to scale your ability to make improvements.Level of effort for the implementation plan: To establish this best practice, you must be aware of the design patterns and services that could help improve your workload performance. After evaluating the design patterns, implementing the design patterns is a high level of effort.Related documents:AWS Architecture CenterAWS Partner NetworkAWS Solutions LibraryAWS Knowledge CenterAmazon Builders\u2019 LibraryUsing load shedding to avoid overloadCaching challenges and strategiesRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)This is My ArchitectureRelated examples:AWS SamplesAWS SDK Examples\n* Distributed systems rely on communications networks to interconnect components (such as servers or services). Your workload must operate reliably despite data loss or latency over these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices enable workloads to withstand stresses or failures, more quickly recover from them, and mitigate the impact of such impairments. The result is improved mean time to recovery (MTTR).Best practices\n* Distributed systems rely on communications networks to interconnect components, such as servers or services. Your workload must operate reliably despite data loss or latency in these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices prevent failures and improve mean time between failures (MTBF).Best practices\n\nKnowledge Area: Distributed design patterns\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice to improve the performance of a distributed system running on AWS?\", \n\"answer_choices\": [\n    {\"answer\": \"Using the same design pattern for all workloads\",\"is_correct\": \"False\",\"explanation\": \"Using the same design pattern for all workloads does not take into account the best practices for the particular workload and is an anti-pattern for improving performance.\"},\n    {\"answer\": \"Evaluating and reviewing design patterns that would improve the workload performance\",\"is_correct\": \"True\",\"explanation\": \"Evaluating and reviewing design patterns that would improve the workload performance is the best practice to improve the performance of a distributed system running on AWS.\"},\n    {\"answer\": \"Using server-side caching solutions instead of managed services\",\"is_correct\": \"False\",\"explanation\": \"Using server-side caching solutions instead of managed services can introduce eventual consistency and require engineering effort to implement within business requirements and customer expectations, which could negatively impact performance.\"},\n    {\"answer\": \"Implementing the design patterns selected for the workload\",\"is_correct\": \"False\",\"explanation\": \"Implementing the design patterns selected for the workload is an important step in improving performance, but it is not the best practice. The best practice is to evaluate and review design patterns that would improve the workload performance.\"}\n]\n}", "docs": ["('REL13-BP01 Define recovery objectives for downtime and data loss - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_objective_defined_recovery.html')", "('PERF08-BP02 Learn about design patterns and services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_design_patterns.html')", "('REL 5\\xa0 How do you design interactions in a distributed system to mitigate or withstand failures? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-05.html')", "('REL 4\\xa0 How do you design interactions in a distributed system to prevent failures? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-04.html')"]}, {"scenario": "A software development team wants to build and deploy a microservices-based application on AWS.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "Failover strategies", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA software development team wants to build and deploy a microservices-based application on AWS.`\"\n\nContext:\n\n* Regularly test failover to your recovery site to ensure proper operation, and that RTO and RPO are met.A pattern to avoid is developing recovery paths that are rarely exercised. For example, you might have a secondary data store that is used for read-only queries. When you write to a data store and the primary fails, you might want to fail over to the secondary data store. If you don\u2019t frequently test this failover, you might find that your assumptions about the capabilities of the secondary data store are incorrect. The capacity of the secondary, which might have been sufficient when you last tested, might be no longer be able to tolerate the load under this scenario. Our experience has shown that the only error recovery that works is the path you test frequently. This is why having a small number of recovery paths is best. You can establish recovery patterns and regularly test them. If you have a complex or critical recovery path, you still need to regularly exercise that failure in production to convince yourself that the recovery path works. In the example we just discussed, you should fail over to the standby regularly, regardless of need.Common anti-patterns:Never exercise failovers in production.Benefits of establishing this best practice: Regularly testing you disaster recovery plan ensures that it will work when it needs to, and that your team knows how to execute the strategy.Level of risk exposed if this best practice is not established: HighEngineer your workloads for recovery. Regularly test your recovery paths Recovery Oriented Computing identifies the characteristics in systems that enhance recovery. These characteristics are: isolation and redundancy, system-wide ability to roll back changes, ability to monitor and determine health, ability to provide diagnostics, automated recovery, modular design, and ability to restart. Exercise the recovery path to ensure that you can accomplish the recovery in the specified time to the specified state. Use your runbooks during this recovery to document problems and find solutions for them before the next test.The Berkeley/Stanford recovery-oriented computing projectUse AWS Elastic Disaster Recovery to implement and launch drill instances for your DR strategy.AWS Elastic Disaster Recovery Preparing for FailoverWhat is Elastic Disaster Recovery?AWS Elastic Disaster RecoveryRelated documents:APN Partner: partners that can help with disaster recoveryAWS Architecture Blog: Disaster Recovery SeriesAWS Marketplace: products that can be used for disaster recoveryAWS Elastic Disaster RecoveryDisaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)AWS Elastic Disaster Recovery Preparing for FailoverThe Berkeley/Stanford recovery-oriented computing projectWhat is AWS Fault Injection Simulator?Related videos:AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)AWS re:Invent 2019: Backup-and-restore and disaster-recovery solutions with AWS (STG208)Related examples:AWS Well-Architected Labs - Testing for Resiliency\n* Having backups and redundant workload components in place is the start of your DR strategy. RTO and RPO are your objectives for restoration of your workload. Set these based on business needs. Implement a strategy to meet these objectives, considering locations and function of workload resources and data. The probability of disruption and cost of recovery are also key factors that help to inform the business value of providing disaster recovery for a workload.Best practices\n* Use AWS or third-party tools to automate system recovery and route traffic to the DR site or Region.Based on configured health checks, AWS services, such as Elastic Load Balancing and AWS Auto Scaling, can distribute load to healthy Availability Zones while services, such as Amazon Route 53 and AWS Global Accelerator, can route load to healthy AWS Regions. Amazon Route 53 Application Recovery Controller helps you manage and coordinate failover using readiness check and routing control features. These features continually monitor your application\u2019s ability to recover from failures, so you can control application recovery across multiple AWS Regions, Availability Zones, and on premises.For workloads on existing physical or virtual data centers or private clouds, AWS Elastic Disaster Recovery allows organizations to set up an automated disaster recovery strategy in AWS. Elastic Disaster Recovery also supports cross-Region and cross-Availability Zone disaster recovery in AWS.Common anti-patterns:Implementing identical automated failover and failback can cause flapping when a failure occurs.Benefits of establishing this best practice: Automated recovery reduces your recovery time by eliminating the opportunity for manual errors.Level of risk exposed if this best practice is not established: MediumAutomate recovery paths. For short recovery times, follow your disaster recovery plan to get your IT systems back online quickly in the case of a disruption.Use Elastic Disaster Recovery for automated Failover and Failback. Elastic Disaster Recovery continuously replicates your machines (including operating system, system state configuration, databases, applications, and files) into a low-cost staging area in your target AWS account and preferred Region. In the case of a disaster, after choosing to recover using Elastic Disaster Recovery, Elastic Disaster Recovery automates the conversion of your replicated servers into fully provisioned workloads in your recovery Region on AWS.Using Elastic Disaster Recovery for Failover and FailbackAWS Elastic Disaster Recovery resourcesRelated documents:APN Partner: partners that can help with disaster recoveryAWS Architecture Blog: Disaster Recovery SeriesAWS Marketplace: products that can be used for disaster recoveryAWS Systems Manager AutomationAWS Elastic Disaster RecoveryDisaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)Related videos:AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)\n\nKnowledge Area: Failover strategies\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the most important best practice for ensuring a successful failover strategy for a microservices-based application on AWS?\", \n\"answer_choices\": [\n{\"answer\": \"Use AWS Fault Injection Simulator\",\"is_correct\": \"False\",\"explanation\": \"AWS Fault Injection Simulator can be used for testing workload resilience, but is not the most important best practice for failover strategies.\"},\n{\"answer\": \"Regularly test failover to the recovery site\",\"is_correct\": \"True\",\"explanation\": \"Regularly testing failover is the most important best practice for ensuring a successful failover strategy. This ensures that RTO and RPO are met and that assumptions about the capabilities of the secondary data store are correct.\"},\n{\"answer\": \"Set RTO and RPO based on business needs\",\"is_correct\": \"False\",\"explanation\": \"RTO and RPO are objectives for restoration of the workload, but they are not the most important best practice for failover strategies.\"},\n{\"answer\": \"Implement an identical automated failover and failback\",\"is_correct\": \"False\",\"explanation\": \"Implementing identical automated failover and failback can cause flapping when a failure occurs, so is not the most important best practice for failover strategies.\"}\n]}", "docs": ["('REL13-BP03 Test disaster recovery implementation to validate the implementation - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_dr_tested.html')", "('REL 13\\xa0 How do you plan for disaster recovery (DR)? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-13.html')", "('REL13-BP05 Automate recovery - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_auto_recovery.html')"]}, {"scenario": "A company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "Immutable infrastructure", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.`\"\n\nContext:\n\n* Immutable infrastructure is a model that mandates that no updates, security patches, or configuration changes happen in-place on production workloads. When a change is needed, the architecture is built onto new infrastructure and deployed into production.The most common implementation of the immutable infrastructure paradigm is the immutable server. This means that if a server needs an update or a fix, new servers are deployed instead of updating the ones already in use. So, instead of logging into the server via SSH and updating the software version, every change in the application starts with a software push to the code repository, for example, git push. Since changes are not allowed in immutable infrastructure, you can be sure about the state of the deployed system. Immutable infrastructures are inherently more consistent, reliable, and predictable, and they simplify many aspects of software development and operations.Use a canary or blue/green deployment when deploying applications in immutable infrastructures.Canary deployment is the practice of directing a small number of your customers to the new version, usually running on a single service instance (the canary). You then deeply scrutinize any behavior changes or errors that are generated. You can remove traffic from the canary if you encounter critical problems and send the users back to the previous version. If the deployment is successful, you can continue to deploy at your desired velocity, while monitoring the changes for errors, until you are fully deployed. AWS CodeDeploy can be configured with a deployment configuration that will enable a canary deployment.Blue/green deployment is similar to the canary deployment except that a full fleet of the application is deployed in parallel. You alternate your deployments across the two stacks (blue and green). Once again, you can send traffic to the new version, and fall back to the old version if you see problems with the deployment. Commonly all traffic is switched at once, however you can also use fractions of your traffic to each version to dial up the adoption of the new version using the weighted DNS routing capabilities of Amazon Route 53. AWS CodeDeploy and AWS Elastic Beanstalk can be configured with a deployment configuration that will enable a blue/green deployment.Figure 8: Blue/green deployment with AWS Elastic Beanstalk and Amazon Route 53Benefits of immutable infrastructure:Reduction in configuration drifts: By frequently replacing servers from a base, known and version-controlled configuration, the infrastructure is reset to a known state, avoiding configuration drifts.Simplified deployments: Deployments are simplified because they don\u2019t need to support upgrades. Upgrades are just new deployments.Reliable atomic deployments: Deployments either complete successfully, or nothing changes. It gives more trust in the deployment process.Safer deployments with fast rollback and recovery processes: Deployments are safer because the previous working version is not changed. You can roll back to it if errors are detected.Consistent testing and debugging environments: Since all servers use the same image, there are no differences between environments. One build is deployed to multiple environments. It also prevents inconsistent environments and simplifies testing and debugging.Increased scalability: Since servers use a base image, are consistent, and repeatable, automatic scaling is trivial.Simplified toolchain: The toolchain is simplified since you can get rid of configuration management tools managing production software upgrades. No extra tools or agents are installed on servers. Changes are made to the base image, tested, and rolled-out.Increased security: By denying all changes to servers, you can disable SSH on instances and remove keys. This reduces the attack vector, improving your organization\u2019s security posture.Level of risk exposed if this best practice is not established: MediumDeploy using immutable infrastructure. Immutable infrastructure is a model in which no updates, security patches, or configuration changes happen in-place on production systems. If any change is needed, a new version of the architecture is built and deployed into production.Overview of a Blue/Green DeploymentDeploying Serverless Applications GraduallyImmutable Infrastructure: Reliability, consistency and confidence through immutabilityCanaryReleaseRelated documents:CanaryReleaseDeploying Serverless Applications GraduallyImmutable Infrastructure: Reliability, consistency and confidence through immutabilityOverview of a Blue/Green DeploymentThe Amazon Builders' Library: Ensuring rollback safety during deployments\n* Infrastructure protection encompasses control methodologies, such as defense in depth, necessary to meet best practices and organizational or regulatory obligations. Use of these methodologies is critical for successful, ongoing operations in either the cloud or on-premises.In AWS, you can implement stateful and stateless packet inspection, either by using AWS-native technologies or by using partner products and services available through the AWS Marketplace. You should use Amazon Virtual Private Cloud (Amazon VPC) to create a private, secured, and scalable environment in which you can define your topology\u2014including gateways, routing tables, and public and private subnets.The following questions focus on these considerations for security.Multiple layers of defense are advisable in any type of environment. In the case of infrastructure protection, many of the concepts and methods are valid across cloud and on-premises models. Enforcing boundary protection, monitoring points of ingress and egress, and comprehensive logging, monitoring, and alerting are all essential to an effective information security plan.AWS customers are able to tailor, or harden, the configuration of an Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS) container, or AWS Elastic Beanstalk instance, and persist this configuration to an immutable Amazon Machine Image (AMI). Then, whether triggered by Auto Scaling or launched manually, all new virtual servers (instances) launched with this AMI receive the hardened configuration.SEC 5: How do you protect your network resources?Any workload that has some form of network connectivity, whether it\u2019s the internet or a private network, requires multiple layers of defense to help protect from external and internal network-based threats.SEC 6: How do you protect your compute resources?Compute resources in your workload require multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more.\n* Identify and document the workload storage needs and define the storage characteristics of each location. Examples of storage characteristics include: shareable access, file size, growth rate, throughput, IOPS, latency, access patterns, and persistence of data. Use these characteristics to evaluate if block, file, object, or instance storage services are the most efficient solution for your storage needs.\n\nKnowledge Area: Immutable infrastructure\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following deployments is best for minimizing costs when batch processing jobs are not running when using immutable infrastructure?\", \"answer_choices\": [{\"answer\": \"Canary Deployment\",\"is_correct\": \"False\",\"explanation\": \"Canary deployment is the practice of directing a small number of customers to the new version, usually running on a single service instance (the canary). This is not ideal for minimizing costs when jobs are not running.\"},{\"answer\": \"Blue/Green Deployment\",\"is_correct\": \"True\",\"explanation\": \"Blue/Green deployment is similar to the canary deployment except that a full fleet of the application is deployed in parallel. This is ideal for minimizing costs when jobs are not running.\"},{\"answer\": \"Immutable Server Deployment\",\"is_correct\": \"False\",\"explanation\": \"Immutable server deployment means that if a server needs an update or a fix, new servers are deployed instead of updating the ones already in use. This is not ideal for minimizing costs when jobs are not running.\"},{\"answer\": \"Weighted DNS Routing\",\"is_correct\": \"False\",\"explanation\": \"Weighted DNS routing capabilities of Amazon Route 53 allows for fractions of traffic to be sent to each version to dial up the adoption of the new version. This is not ideal for minimizing costs when jobs are not running.\"}]}", "docs": ["('REL08-BP04 Deploy using immutable infrastructure - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_tracking_change_management_immutable_infrastructure.html')", "('Infrastructure protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-infrastructure.html')", "('PERF03-BP01 Understand storage characteristics and requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_understand_char.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "Load balancing concepts (for example, Application Load Balancer)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* Distribute traffic across multiple resources or services to allow your workload to take advantage of the elasticity that the cloud provides. You can also use load balancing for offloading encryption termination to improve performance and to manage and route traffic effectively.When implementing a scale-out architecture where you want to use multiple instances for service content, you can use load balancers inside your Amazon VPC. AWS provides multiple models for your applications in the ELB service. Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers.Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. It is capable of handling millions of requests per second while maintaining ultra-low latencies, and it is optimized to handle sudden and volatile traffic patterns.Elastic Load Balancing provides integrated certificate management and SSL/TLS decryption, allowing you the flexibility to centrally manage the SSL settings of the load balancer and offload CPU intensive work from your workload.Common anti-patterns:You route all internet traffic through existing load balancers.You use generic TCP load balancing and making each compute node handle SSL encryption.Benefits of establishing this best practice: A load balancer handles the varying load of your application traffic in a single Availability Zone, or across multiple Availability Zones. Load balancers feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant.Level of risk exposed if this best practice is not established: HighUse the appropriate load balancer for your workload: Select the appropriate load balancer for your workload. If you must load balance HTTP requests, we recommend Application Load Balancer. For network and transport protocols (layer 4 \u2013 TCP, UDP) load balancing, and for extreme performance and low latency applications, we recommend Network Load Balancer. Application Load Balancers support HTTPS and Network Load Balancers support TLS encryption offloading.Enable offload of HTTPS or TLS encryption: Elastic Load Balancing includes integrated certificate management, user-authentication, and SSL/TLS decryption. It provides the flexibility to centrally manage TLS settings and offload CPU intensive workloads from your applications. Encrypt all HTTPS traffic as part of your load balancer deployment.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies.Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public endpoints. You might also choose to evaluate AWS Marketplace software appliances for load balancing and proxying.Consumers of the service your workload provides, whether they are end-users or other services, make requests on these service endpoints. Several AWS resources are available to enable you to provide highly available endpoints.Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.Distributed Denial of Service (DDoS) attacks risk shutting out legitimate traffic and lowering availability for your users. AWS Shield provides automatic protection against these attacks at no extra cost for AWS service endpoints on your workload. You can augment these features with virtual appliances from APN Partners and the AWS Marketplace to meet your needs.Common anti-patterns:Using public internet addresses on instances or containers and managing the connectivity to them via DNS.Using Internet Protocol addresses instead of domain names for locating services.Providing content (web pages, static assets, media files) to a large geographic area and not using a content delivery network.Benefits of establishing this best practice: By implementing highly available services in your workload, you know that your workload will be available to your users.Level of risk exposed if this best practice is not established: HighEnsure that you have highly available connectivity for users of the workload Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public facing endpoints. You may also choose to evaluate AWS Marketplace software appliances for load-balancing and proxying.Ensure that you have a highly available connection to your users.Ensure that you are using a highly available DNS to manage the domain names of your application endpoints.If your users access your application via the internet, use service API operations to confirm the correct usage of Internet Gateways. Also confirm that the route tables entries for the subnets hosting your application endpoints are correct.DescribeInternetGatewaysDescribeRouteTablesEnsure that you are using a highly available reverse proxy or load balancer in front of your application.If your users access your application via your on-premises environment, ensure that your connectivity between AWS and your on-premises environment is highly available.Use Route 53 to manage your domain names.What is Amazon Route 53?Use a third-party DNS provider that meets your requirements.Use Elastic Load Balancing.What is Elastic Load Balancing?Use an AWS Marketplace appliance that meets your requirements.Related documents:APN Partner: partners that can help plan your networkingAWS Direct Connect Resiliency RecommendationsAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivityUsing the Direct Connect Resiliency Toolkit to get startedVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is AWS Global Accelerator?What Is Amazon VPC?What Is a Transit Gateway?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?Working with Direct Connect GatewaysRelated videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n\nKnowledge Area: Load balancing concepts (for example, Application Load Balancer)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which type of load balancer is best suited for HTTP and HTTPS traffic and modern application architectures such as microservices and containers?\",\n\"answer_choices\": [{\"answer\": \"Network Load Balancer\",\"is_correct\": \"false\",\"explanation\": \"Network Load Balancer is used for layer 4 (TCP) or layer 7 (http/https) routing and is optimized to handle sudden and volatile traffic patterns, but is not well suited for modern application architectures such as microservices and containers.\"},{\"answer\": \"Elastic Load Balancer\",\"is_correct\": \"false\",\"explanation\": \"Elastic Load Balancing provides integrated certificate management, user-authentication, and SSL/TLS decryption, but is not well suited for modern application architectures such as microservices and containers.\"}, {\"answer\": \"Application Load Balancer\",\"is_correct\": \"true\",\"explanation\": \"Application Load Balancer is the recommended type of load balancer for HTTP and HTTPS traffic and is best suited for modern application architectures such as microservices and containers.\"}, {\"answer\": \"Transit Gateway\",\"is_correct\": \"false\",\"explanation\": \"Transit Gateway is used to manage traffic between multiple VPCs, not for load balancing.\"}]}", "docs": ["('PERF05-BP04 Leverage load-balancing and encryption offloading - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_encryption_offload.html')", "('REL02-BP01 Use highly available network connectivity for your workload public endpoints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ha_conn_users.html')"]}, {"scenario": "A software development team wants to build and deploy a microservices-based application on AWS.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "Proxy concepts (for example, Amazon RDS Proxy)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA software development team wants to build and deploy a microservices-based application on AWS.`\"\n\nContext:\n\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Managed services shift responsibility for maintaining high-average utilization, and sustainability optimization of the deployed hardware to AWS. Use managed services to distribute the sustainability impact of the service across all tenants of the service, reducing your individual contribution.Level of risk exposed if this best practice is not established: LowMigrate from self-hosted services to managed services. For example, use managed Amazon Relational Database Service (Amazon RDS) instances instead of maintaining your own Amazon RDS instances on Amazon Elastic Compute Cloud (Amazon EC2), or use managed container services, such as AWS Fargate, instead of implementing your own container infrastructure.Related documents:AWS FargateAmazon DocumentDBAmazon Elastic Kubernetes Service (EKS)Amazon Managed Streaming for Apache Kafka (Amazon MSK)Amazon RedshiftAmazon Relational Database Service (RDS)\n* These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies.Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public endpoints. You might also choose to evaluate AWS Marketplace software appliances for load balancing and proxying.Consumers of the service your workload provides, whether they are end-users or other services, make requests on these service endpoints. Several AWS resources are available to enable you to provide highly available endpoints.Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.Distributed Denial of Service (DDoS) attacks risk shutting out legitimate traffic and lowering availability for your users. AWS Shield provides automatic protection against these attacks at no extra cost for AWS service endpoints on your workload. You can augment these features with virtual appliances from APN Partners and the AWS Marketplace to meet your needs.Common anti-patterns:Using public internet addresses on instances or containers and managing the connectivity to them via DNS.Using Internet Protocol addresses instead of domain names for locating services.Providing content (web pages, static assets, media files) to a large geographic area and not using a content delivery network.Benefits of establishing this best practice: By implementing highly available services in your workload, you know that your workload will be available to your users.Level of risk exposed if this best practice is not established: HighEnsure that you have highly available connectivity for users of the workload Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public facing endpoints. You may also choose to evaluate AWS Marketplace software appliances for load-balancing and proxying.Ensure that you have a highly available connection to your users.Ensure that you are using a highly available DNS to manage the domain names of your application endpoints.If your users access your application via the internet, use service API operations to confirm the correct usage of Internet Gateways. Also confirm that the route tables entries for the subnets hosting your application endpoints are correct.DescribeInternetGatewaysDescribeRouteTablesEnsure that you are using a highly available reverse proxy or load balancer in front of your application.If your users access your application via your on-premises environment, ensure that your connectivity between AWS and your on-premises environment is highly available.Use Route 53 to manage your domain names.What is Amazon Route 53?Use a third-party DNS provider that meets your requirements.Use Elastic Load Balancing.What is Elastic Load Balancing?Use an AWS Marketplace appliance that meets your requirements.Related documents:APN Partner: partners that can help plan your networkingAWS Direct Connect Resiliency RecommendationsAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivityUsing the Direct Connect Resiliency Toolkit to get startedVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is AWS Global Accelerator?What Is Amazon VPC?What Is a Transit Gateway?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?Working with Direct Connect GatewaysRelated videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n\nKnowledge Area: Proxy concepts (for example, Amazon RDS Proxy)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following provides a highly available public facing endpoint to help build and deploy a microservices-based application on AWS?\",\n\"answer_choices\": [{\"answer\": \"Amazon Route 53\",\"is_correct\": \"yes\",\"explanation\": \"Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets.\"},{\"answer\": \"AWS Global Accelerator\",\"is_correct\": \"yes\",\"explanation\": \"AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.\"},{\"answer\": \"Amazon CloudFront\",\"is_correct\": \"yes\",\"explanation\": \"Amazon CloudFront provides a highly available public endpoint to help build and deploy a microservices-based application on AWS.\"},{\"answer\": \"Amazon API Gateway\",\"is_correct\": \"yes\",\"explanation\": \"Amazon API Gateway provides a highly available public endpoint to help build and deploy a microservices-based application on AWS.\"}]}", "docs": ["('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('SUS05-BP03 Use managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a4.html')", "('REL02-BP01 Use highly available network connectivity for your workload public endpoints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ha_conn_users.html')"]}, {"scenario": "A large corporation wants to move their on-premises compute to the cloud for increased scalability and security.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "Service quotas and throttling (for example, how to configure the service quotas for a workload in a standby environment)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA large corporation wants to move their on-premises compute to the cloud for increased scalability and security.`\"\n\nContext:\n\n* For cloud-based workload architectures, there are service quotas (which are also referred to as service limits). These quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations so as to protect services from abuse. There are also resource constraints, for example, the rate that you can push bits down a fiber-optic cable, or the amount of storage on a physical disk.Best practices\n* You are aware of your default quotas and quota increase requests for your workload architecture. You additionally know which resource constraints, such as disk or network, are potentially impactful.Service Quotas is an AWS service that helps you manage your quotas for over 100 AWS services from one location. Along with looking up the quota values, you can also request and track quota increases from the Service Quotas console or via the AWS SDK. AWS Trusted Advisor offers a service quotas check that displays your usage and quotas for some aspects of some services. The default service quotas per service are also in the AWS documentation per respective service, for example, see Amazon VPC Quotas. Rate limits on throttled APIs are set within the API Gateway itself by configuring a usage plan. Other limits that are set as configuration on their respective services include Provisioned IOPS, RDS storage allocated, and EBS volume allocations. Amazon Elastic Compute Cloud (Amazon EC2) has its own service limits dashboard that can help you manage your instance, Amazon Elastic Block Store (Amazon EBS), and Elastic IP address limits. If you have a use case where service quotas impact your application\u2019s performance and they are not adjustable to your needs, then contact AWS Support to see if there are mitigations.Common anti-patterns:Deploying a workload with no regard of the service quotas on the AWS services used.Designing a workload without investigating and accommodating for AWS services' design constraints.Deploying a workload with significant use that replaces a known existing workload without configuring the necessary quotas or contacting AWS Support in advance.Planning an event to drive traffic to your workload, but not configuring the necessary quotas or contacting AWS Support in advance.Benefits of establishing this best practice: Being aware of the service quotas, API throttling limits, and design constraints will allow you to account for these in your design, implementation, and operation of the workload.Level of risk exposed if this best practice is not established: HighReview AWS service quotas in the published documentation and Service QuotasAWS Service Quotas (formerly referred to as limits)Determine all the services your workload requires by looking at the deployment code.Use AWS Config to find all AWS resources used in your AWS accounts.AWS Config Supported AWS Resource Types and Resource RelationshipsYou can also use your AWS CloudFormation to determine your AWS resources used. Look at the resources that were created either in the AWS Management Console or via the list-stack-resources CLI command. You can also see resources configured to be deployed in the template itself.Viewing AWS CloudFormation Stack Data and Resources on the AWS Management ConsoleAWS CLI for CloudFormation: list-stack-resourcesDetermine the service quotas that apply. Use the programmatically accessible information via Trusted Advisor and Service Quotas.Related documents:AWS Marketplace: CMDB products that help track limitsAWS Service Quotas (formerly referred to as service limits)AWS Trusted Advisor Best Practice Checks (see the Service Limits section)AWS limit monitor on AWS answersAmazon EC2 Service LimitsWhat is Service Quotas?Related videos:AWS Live re:Inforce 2019 - Service Quotas\n* Be aware of unchangeable service quotas and physical resources, and architect to prevent these from impacting reliability.Examples include network bandwidth, AWS Lambda payload size, throttle burst rate for API Gateway, and concurrent user connections to an Amazon Redshift cluster.Common anti-patterns:Performing benchmarking for too short of time, utilizing the burst limit, but then expecting the service to perform at that capacity for sustained periods.Choosing a design that uses one resource of a service per user or customer, unaware that there are design constraints that will cause this design to fail as you scale.Benefits of establishing this best practice: Tracking fixed quotes in AWS services and constraints in other parts of your workload, such as connectivity constraints, IP address constraints, and constraints in third-party services, allows you to detect when you are trending toward a quota and gives you the ability to address the quota before it's exceeded.Level of risk exposed if this best practice is not established: MediumBe aware of fixed service quotas Be aware of fixed service quotas and constraints and architect around these.AWS Service QuotasRelated documents:AWS Marketplace: CMDB products that help track limitsAWS Service Quotas (formerly referred to as service limits)AWS Trusted Advisor Best Practice Checks (see the Service Limits section)AWS limit monitor on AWS answersAmazon EC2 Service LimitsWhat Is Service Quotas?Related videos:AWS Live re:Inforce 2019 - Service Quotas\n* If you are using multiple AWS accounts or AWS Regions, ensure that you request the appropriate quotas in all environments in which your production workloads run.Service quotas are tracked per account. Unless otherwise noted, each quota is AWS Region-specific. In addition to the production environments, also manage quotas in all applicable non-production environments, so that testing and development are not hindered.Common anti-patterns:Allowing resource utilization in one isolation zone to grow with no mechanism to maintain capacity in the other ones.Manually setting all quotas independently in isolation zones.Not ensuring Regionally isolated deployments are sized to accommodate the increase in traffic from another Region if a deployment is lost.Benefits of establishing this best practice: Ensuring that you can handle your current load if an isolation zone is unavailable can help reduce the number of errors that occur during failover, instead of causing a denial of service to your customers.Level of risk exposed if this best practice is not established: HighSelect relevant accounts and Regions based on your service requirements, latency, regulatory, and disaster recovery (DR) requirements.Identify service quotas across all relevant accounts, Regions, and Availability Zones. The limits are scoped to account and Region.What is Service Quotas?Related documents:AWS Marketplace: CMDB products that help track limitsAWS Service Quotas (formerly referred to as service limits)AWS Trusted Advisor Best Practice Checks (see the Service Limits section)AWS limit monitor on AWS answersAmazon EC2 Service LimitsWhat is Service Quotas?Related videos:AWS Live re:Inforce 2019 - Service Quotas\n\nKnowledge Area: Service quotas and throttling (for example, how to configure the service quotas for a workload in a standby environment)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When dealing with service quotas and throttling for workloads in a standby environment, which of the following best practices should be established?\", \n\"answer_choices\": [\n{\"answer\": \"Deploying a workload with no regard of the service quotas on the AWS services used.\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern that should be avoided.\"},\n{\"answer\": \"Be aware of the service quotas and request increases as needed.\",\"is_correct\": \"True\",\"explanation\": \"It is important to be aware of the service quotas and request increases as needed to ensure sufficient resource capacity while avoiding resource overprovisioning.\"},\n{\"answer\": \"Ensure that service quotas are requested in all applicable regions.\",\"is_correct\": \"True\",\"explanation\": \"Service quotas should be requested in all applicable regions to ensure that sufficient capacity is available in all environments in which production workloads run.\"},\n{\"answer\": \"Do not contact AWS Support to see if there are mitigations.\",\"is_correct\": \"False\",\"explanation\": \"If service quotas impact your application\u2019s performance, contact AWS Support to see if there are mitigations.\"}\n]}", "docs": ["('REL 1\\xa0 How do you manage service quotas and constraints? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-01.html')", "('REL01-BP01 Aware of service quotas and constraints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_manage_service_limits_aware_quotas_and_constraints.html')", "('REL01-BP03 Accommodate fixed service quotas and constraints through architecture - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_manage_service_limits_aware_fixed_limits.html')", "('REL01-BP02 Manage service quotas across accounts and regions - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_manage_service_limits_limits_considered.html')"]}, {"scenario": "A research institution wants to perform data analysis on a large dataset using AWS compute resources.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "Storage options and characteristics (for example, durability, replication)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA research institution wants to perform data analysis on a large dataset using AWS compute resources.`\"\n\nContext:\n\n* Identify and document the workload storage needs and define the storage characteristics of each location. Examples of storage characteristics include: shareable access, file size, growth rate, throughput, IOPS, latency, access patterns, and persistence of data. Use these characteristics to evaluate if block, file, object, or instance storage services are the most efficient solution for your storage needs.\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n\nKnowledge Area: Storage options and characteristics (for example, durability, replication)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS storage service is best suited for a data analysis workload that requires high throughput and low latency?\",\n\"answer_choices\": [{\"answer\": \"Amazon Glacier\",\"is_correct\": \"FALSE\",\"explanation\": \"Amazon Glacier is designed for long-term data archiving, not for high throughput and low latency.\"},{\"answer\": \"Amazon S3 with CloudFront\",\"is_correct\": \"FALSE\",\"explanation\": \"Amazon S3 with CloudFront is designed for GET requests, not for high throughput and low latency.\"},{\"answer\": \"Amazon FSx for Lustre\",\"is_correct\": \"TRUE\",\"explanation\": \"Amazon FSx for Lustre is designed for high-performance workloads, and provides fast, predictable, scalable, and consistent performance with high read and write speeds and low latency data access.\"},{\"answer\": \"Amazon EBS Volume Types\",\"is_correct\": \"FALSE\",\"explanation\": \"Amazon EBS Volume Types are primarily designed for transactional workloads and provide higher performance based on IOPS, not throughput and latency.\"}]}", "docs": ["('PERF03-BP01 Understand storage characteristics and requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_understand_char.html')", "('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Knowledge of", "item": "Workload visibility (for example, AWS X-Ray)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Use AWS X-Ray or third-party tools so that developers can more easily analyze and debug distributed systems to understand how their applications and its underlying services are performing.Level of risk exposed if this best practice is not established: MediumMonitor end-to-end tracing of requests through your system. AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases, and web APIs.What is AWS X-Ray?Debugging with Amazon CloudWatch Synthetics and AWS X-RayRelated documents:Debugging with Amazon CloudWatch Synthetics and AWS X-RayOne Observability WorkshopThe Amazon Builders' Library: Instrumenting distributed systems for operational visibilityUsing Canaries (Amazon CloudWatch Synthetics)What is AWS X-Ray?\n* After you implement your workload, you must monitor its performance so that you can remediate any issues before they impact your customers. Monitoring metrics should be used to raise alarms when thresholds are breached.Amazon CloudWatch is a monitoring and observability service that provides you with data and actionable insights to monitor your workload, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events from workloads that run on AWS and on-premises servers. AWS X-Ray helps developers analyze and debug production, distributed applications. With AWS X-Ray, you can glean insights into how your application is performing and discover root causes and identify performance bottlenecks. You can use these insights to react quickly and keep your workload running smoothly.The following question focuses on these considerations for performance efficiency.Ensuring that you do not see false positives is key to an effective monitoring solution. Automated triggers avoid human error and can reduce the time it takes to fix problems. Plan for game days, where simulations are conducted in the production environment, to test your alarm solution and ensure that it correctly recognizes issues.PERF 7: How do you monitor your resources to ensure they are performing?System performance can degrade over time. Monitor system performance to identify degradation and remediate internal or external factors, such as the operating system or application load.\n* Understand and identify areas where increasing the performance of your workload will have a positive impact on efficiency or customer experience. For example, a website that has a large amount of customer interaction can benefit from using edge services to move content delivery closer to customers.Desired outcome: Increase performance efficiency by understanding your architecture, traffic patterns, and data access patterns, and identify your latency and processing times. Identify the potential bottlenecks that might affect the customer experience as the workload grows. When you identify those areas, look at which solution you could deploy to remove those performance concerns.Common anti-patterns:You assume that standard compute metrics such as CPUUtilization or memory pressure are enough to catch performance issues.You only use the default metrics recorded by your selected monitoring software.You only review metrics when there is an issue.Benefits of establishing this best practice: Understanding critical areas of performance helps workload owners monitor KPIs and prioritize high-impact improvements.Level of risk exposed if this best practice is not established: HighSet up end-to-end tracing to identify traffic patterns, latency, and critical performance areas. Monitor your data access patterns for slow queries or poorly fragmented and partitioned data. Identify the constrained areas of the workload using load testing or monitoring.Set up end-to-end monitoring to capture all workload components and metrics.Use Amazon CloudWatch Real-User Monitoring (RUM) to capture application performance metrics from real user client-side and frontend sessions.Set up AWS X-Ray to trace traffic through the application layers and identify latency between components and dependencies. Use the X-Ray service maps to see relationships and latency between workload components.Use Amazon Relational Database Service Performance Insights to view database performance metrics and identify performance improvements.Use Amazon RDS Enhanced Monitoring to view database OS performance metrics.Collect CloudWatch metrics per workload component and service and identify which metrics impact performance efficiency.Set up Amazon DevOps Guru for additional performance insights and recommendationsPerform tests to generate metrics, identify traffic patterns, bottlenecks, and critical performance areas.Set up CloudWatch Synthetic Canaries to mimic browser-based user activities programmatically using cron jobs or rate expressions to generate consistent metrics over time.Use the AWS Distributed Load Testing solution to generate peak traffic or test the workload at the expected growth rate.Evaluate the metrics and telemetry to identify your critical performance areas. Review these areas with your team to discuss monitoring and solutions to avoid bottlenecks.Experiment with performance improvements and measure those changes with data.Use CloudWatch Evidently to test new improvements and the performance impact to the workload.Level of effort for the implementation plan: To establish this best practice, you must review your end-to-end metrics and be aware of your current workload performance. This is a moderate level of effort to set up end to end monitoring and identify your critical performance areas.Related documents:Amazon Builders\u2019 LibraryX-Ray DocumentationAmazon CloudWatch RUMAmazon DevOps GuruCloudWatch RUM and X-RayRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)Demo of Amazon CloudWatch SyntheticsRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientX-Ray SDK for Node.jsX-Ray SDK for PythonX-Ray SDK for JavaX-Ray SDK for .NetX-Ray SDK for RubyX-Ray DaemonDistributed Load Testing on AWS\n\nKnowledge Area: Workload visibility (for example, AWS X-Ray)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"How can AWS X-Ray be used to monitor and improve performance efficiency for a healthcare company storing and processing sensitive patient information in the cloud?\",\n\"answer_choices\": [{\"answer\": \"AWS X-Ray can be used to identify performance bottlenecks and react quickly to keep the workload running smoothly.\",\"is_correct\": \"True\",\"explanation\": \"AWS X-Ray helps developers analyze and debug production, distributed applications. With AWS X-Ray, you can glean insights into how your application is performing and discover root causes and identify performance bottlenecks. You can use these insights to react quickly and keep your workload running smoothly.\"},{\"answer\": \"AWS X-Ray can be used to monitor system performance and identify degradation.\",\"is_correct\": \"False\",\"explanation\": \"Amazon CloudWatch is a monitoring and observability service that provides you with data and actionable insights to monitor your workload, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events from workloads that run on AWS and on-premises servers.\"},{\"answer\": \"AWS X-Ray can be used to perform load testing to generate metrics, identify traffic patterns, and identify bottlenecks.\",\"is_correct\": \"False\",\"explanation\": \"Use the AWS Distributed Load Testing solution to generate peak traffic or test the workload at the expected growth rate.\"},{\"answer\": \"AWS X-Ray can be used to test new improvements and measure their performance impact on the workload.\",\"is_correct\": \"False\",\"explanation\": \"Use CloudWatch Evidently to test new improvements and the performance impact to the workload.\"}]}", "docs": ["('REL06-BP07 Monitor end-to-end tracing of requests through your system - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_monitor_aws_resources_end_to_end.html')", "('Monitoring - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-monitoring.html')", "('PERF08-BP01 Understand the areas where performance is most critical - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_critical_areas.html')"]}, {"scenario": "A large corporation wants to move their on-premises compute to the cloud for increased scalability and security.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Skills in", "item": "Determining automation strategies to ensure infrastructure integrity", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA large corporation wants to move their on-premises compute to the cloud for increased scalability and security.`\"\n\nContext:\n\n* Establish secure baselines and templates for security mechanisms that are tested and validated as part of your build, pipelines, and processes. Use tools and automation to test and validate all security controls continuously. For example, scan items such as machine images and infrastructure-as-code templates for security vulnerabilities, irregularities, and drift from an established baseline at each stage. AWS CloudFormation Guard can help you verify that CloudFormation templates are safe, save you time, and reduce the risk of configuration error.Reducing the number of security misconfigurations introduced into a production environment is critical\u2014the more quality control and reduction of defects you can perform in the build process, the better. Design continuous integration and continuous deployment (CI/CD) pipelines to test for security issues whenever possible. CI/CD pipelines offer the opportunity to enhance security at each stage of build and delivery. CI/CD security tooling must also be kept updated to mitigate evolving threats.Track changes to your workload configuration to help with compliance auditing, change management, and investigations that may apply to you. You can use AWS Config to record and evaluate your AWS and third-party resources. It allows you to continuously audit and assess the overall compliance with rules and conformance packs, which are collections of rules with remediation actions.Change tracking should include planned changes, which are part of your organization\u2019s change control process (sometimes referred to as MACD\u2014Move, Add, Change, Delete), unplanned changes, and unexpected changes, such as incidents. Changes might occur on the infrastructure, but they might also be related to other categories, such as changes in code repositories, machine images and application inventory changes, process and policy changes, or documentation changes.Level of risk exposed if this best practice is not established: MediumAutomate configuration management: Enforce and validate secure configurations automatically by using a configuration management service or tool.AWS Systems ManagerAWS CloudFormationSet Up a CI/CD Pipeline on AWSRelated documents:How to use service control policies to set permission guardrails across accounts in your AWS OrganizationRelated videos:Managing Multi-Account AWS Environments Using AWS OrganizationsSecurity Best Practices the Well-Architected Way\n* Automate your protective compute mechanisms including vulnerability management, reduction in attack surface, and management of resources. The automation will help you invest time in securing other aspects of your workload, and reduce the risk of human error.Level of risk exposed if this best practice is not established: MediumAutomate configuration management: Enforce and validate secure configurations automatically by using a configuration management service or tool.AWS Systems ManagerAWS CloudFormationLab: Automated deployment of VPCLab: Automated deployment of EC2 web applicationAutomate patching of Amazon Elastic Compute Cloud (Amazon EC2) instances: AWS Systems Manager Patch Manager automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications.AWS Systems Manager Patch ManagerCentralized multi-account and multi-Region patching with AWS Systems Manager AutomationImplement intrusion detection and prevention: Implement an intrusion detection and prevention tool to monitor and stop malicious activity on instances.Consider AWS Partner solutions: AWS Partners offer hundreds of industry-leading products that are equivalent, identical to, or integrate with existing controls in your on-premises environments. These products complement the existing AWS services to enable you to deploy a comprehensive security architecture and a more seamless experience across your cloud and on-premises environments.Infrastructure securityRelated documents:AWS CloudFormationAWS Systems ManagerAWS Systems Manager Patch ManagerCentralized multi-account and multi-region patching with AWS Systems Manager AutomationInfrastructure securityReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: Automated Deployment of Web Application FirewallLab: Automated deployment of Amazon EC2 web application\n* There are five design principles for reliability in the cloud:Automatically recover from failure: By monitoring a workload for key performance indicators (KPIs), you can trigger automation when a threshold is breached. These KPIs should be a measure of business value, not of the technical aspects of the operation of the service. This allows for automatic notification and tracking of failures, and for automated recovery processes that work around or repair the failure. With more sophisticated automation, it\u2019s possible to anticipate and remediate failures before they occur.Test recovery procedures: In an on-premises environment, testing is often conducted to prove that the workload works in a particular scenario. Testing is not typically used to validate recovery strategies. In the cloud, you can test how your workload fails, and you can validate your recovery procedures. You can use automation to simulate different failures or to recreate scenarios that led to failures before. This approach exposes failure pathways that you can test and fix before a real failure scenario occurs, thus reducing risk.Scale horizontally to increase aggregate workload availability: Replace one large resource with multiple small resources to reduce the impact of a single failure on the overall workload. Distribute requests across multiple, smaller resources to ensure that they don\u2019t share a common point of failure.Stop guessing capacity: A common cause of failure in on-premises workloads is resource saturation, when the demands placed on a workload exceed the capacity of that workload (this is often the objective of denial of service attacks). In the cloud, you can monitor demand and workload utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over- or under-provisioning. There are still limits, but some quotas can be controlled and others can be managed (see Manage Service Quotas and Constraints).Manage change in automation: Changes to your infrastructure should be made using automation. The changes that need to be managed include changes to the automation, which then can be tracked and reviewed.\n\nKnowledge Area: Determining automation strategies to ensure infrastructure integrity\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"Which is the best tool to automate the process of patching managed instances with both security-related and other types of updates?\",\n\"answer_choices\": [\n    {\"answer\": \"AWS Systems Manager\", \"is_correct\": \"true\", \"explanation\": \"AWS Systems Manager Patch Manager automates the process of patching managed instances with both security-related and other types of updates.\"},\n    {\"answer\": \"AWS CloudFormation\", \"is_correct\": \"false\", \"explanation\": \"AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources, enabling you to create and manage a collection of related AWS resources in an orderly and predictable fashion.\"},\n    {\"answer\": \"AWS Config\", \"is_correct\": \"false\", \"explanation\": \"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It records configuration changes to your resources and allows you to audit, monitor, and assess the configurations of your resources.\"},\n    {\"answer\": \"AWS Organizations\", \"is_correct\": \"false\", \"explanation\": \"AWS Organizations is a service that allows you to easily manage multiple AWS accounts as a single organization. Organizations provides account management and policy-based management capabilities that allow you to centrally manage and govern your environment.\"}\n]}", "docs": ["('SEC01-BP06 Automate testing and validation of security controls in pipelines - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_securely_operate_test_validate_pipeline.html')", "('SEC06-BP04 Automate compute protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_auto_protection.html')", "('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-dp.html')"]}, {"scenario": "A large corporation wants to move their on-premises compute to the cloud for increased scalability and security.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Skills in", "item": "Determining the AWS services required to provide a highly available and/or fault-tolerant architecture across AWS Regions or Availability Zones", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA large corporation wants to move their on-premises compute to the cloud for increased scalability and security.`\"\n\nContext:\n\n* Ensure that if a resource failure occurs, that healthy resources can continue to serve requests. For location failures (such as Availability Zone or AWS Region) ensure that you have systems in place to fail over to healthy resources in unimpaired locations.AWS services, such as Elastic Load Balancing and Amazon EC2 Auto Scaling, help distribute load across resources and Availability Zones. Therefore, failure of an individual resource (such as an EC2 instance) or impairment of an Availability Zone can be mitigated by shifting traffic to remaining healthy resources. For multi-region workloads, this is more complicated. For example, cross-region read replicas enable you to deploy your data to multiple AWS Regions, but you still must promote the read replica to primary and point your traffic at it in the event of a failover. Amazon Route 53 and AWS Global Accelerator can help route traffic across AWS Regions.If your workload is using AWS services, such as Amazon S3 or Amazon DynamoDB, then they are automatically deployed to multiple Availability Zones. In case of failure, the AWS control plane automatically routes traffic to healthy locations for you. Data is redundantly stored in multiple Availability Zones, and remains available. For Amazon RDS, you must choose Multi-AZ as a configuration option, and then on failure AWS automatically directs traffic to the healthy instance. For Amazon EC2 instances, Amazon ECS tasks, or Amazon EKS pods, you choose which Availability Zones to deploy to. Elastic Load Balancing then provides the solution to detect instances in unhealthy zones and route traffic to the healthy ones. Elastic Load Balancing can even route traffic to components in your on-premises data center.For Multi-Region approaches (which might also include on-premises data centers), Amazon Route 53 provides a way to define internet domains, and assign routing policies that can include health checks to ensure that traffic is routed to healthy regions. Alternately, AWS Global Accelerator provides static IP addresses that act as a fixed entry point to your application, then routes to endpoints in AWS Regions of your choosing, using the AWS global network instead of the internet for better performance and reliability.AWS approaches the design of our services with fault recovery in mind. We design services to minimize the time to recover from failures and impact on data. Our services primarily use data stores that acknowledge requests only after they are durably stored across multiple replicas within a Region. These services and resources include Amazon Aurora, Amazon Relational Database Service (Amazon RDS) Multi-AZ DB instances, Amazon S3, Amazon DynamoDB, Amazon Simple Queue Service (Amazon SQS), and Amazon Elastic File System (Amazon EFS). They are constructed to use cell-based isolation and use the fault isolation provided by Availability Zones. We use automation extensively in our operational procedures. We also optimize our replace-and-restart functionality to recover quickly from interruptions.Level of risk exposed if this best practice is not established: HighFail over to healthy resources. Ensure that if a resource failure occurs, that healthy resources can continue to serve requests. For location failures (such as Availability Zone or AWS Region) ensure you have systems in place to fail over to healthy resources in unimpaired locations.If your workload is using AWS services, such as Amazon S3 or Amazon DynamoDB, then they are automatically deployed to multiple Availability Zones. In case of failure, the AWS control plane automatically routes traffic to healthy locations for you.For Amazon RDS you must choose Multi-AZ as a configuration option, and then on failure AWS automatically directs traffic to the healthy instance.High Availability (Multi-AZ) for Amazon RDSFor Amazon EC2 instances or Amazon ECS tasks, you choose which Availability Zones to deploy to. Elastic Load Balancing then provides the solution to detect instances in unhealthy zones and route traffic to the healthy ones. Elastic Load Balancing can even route traffic to components in your on-premises data center.For multi-region approaches (which might also include on-premises data centers), ensure that data and resources from healthy locations can continue to serve requestsFor example, cross-region read replicas enable you to deploy your data to multiple AWS Regions, but you still must promote the read replica to master and point your traffic at it in the event of a primary location failure.Overview of Amazon RDS Read ReplicasAmazon Route 53 provides a way to define internet domains, and assign routing policies, which might include health checks, to ensure that traffic is routed to healthy Regions. Alternately, AWS Global Accelerator provides static IP addresses that act as a fixed entry point to your application, then routes to endpoints in AWS Regions of your choosing, using the AWS global network instead of the public internet for better performance and reliability.Amazon Route 53: Choosing a Routing PolicyWhat Is AWS Global Accelerator?Related documents:APN Partner: partners that can help with automation of your fault toleranceAWS Marketplace: products that can be used for fault toleranceAWS OpsWorks: Using Auto Healing to Replace Failed InstancesAmazon Route 53: Choosing a Routing PolicyHigh Availability (Multi-AZ) for Amazon RDSOverview of Amazon RDS Read ReplicasAmazon ECS task placement strategiesCreating Kubernetes Auto Scaling Groups for Multiple Availability ZonesWhat is AWS Global Accelerator?Related examples:Well-Architected lab: Level 300: Implementing Health Checks and Managing Dependencies to Improve Reliability\n\nKnowledge Area: Determining the AWS services required to provide a highly available and/or fault-tolerant architecture across AWS Regions or Availability Zones\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"What AWS service should be used to ensure that data and resources from healthy locations can continue to serve requests in the event of a resource failure or impairment of an Availability Zone?\",\n\"answer_choices\":[\n{\"answer\": \"Amazon S3\",\"is_correct\": \"false\",\"explanation\": \"Amazon S3 is an object storage service, and should not be used to ensure that data and resources from healthy locations can continue to serve requests in the event of a resource failure or impairment of an Availability Zone.\"},\n{\"answer\": \"Elastic Load Balancing\",\"is_correct\": \"true\",\"explanation\": \"Elastic Load Balancing should be used to detect instances in unhealthy zones and route traffic to the healthy ones, even for components in your on-premises data center.\"},\n{\"answer\": \"Amazon EC2\",\"is_correct\": \"false\",\"explanation\": \"Amazon EC2 should be used to choose which Availability Zones to deploy to, but should not be used to ensure that data and resources from healthy locations can continue to serve requests in the event of a resource failure or impairment of an Availability Zone.\"},\n{\"answer\": \"Amazon Route 53\",\"is_correct\": \"false\",\"explanation\": \"Amazon Route 53 should be used to define internet domains and assign routing policies, but should not be used to ensure that data and resources from healthy locations can continue to serve requests in the event of a resource failure or impairment of an Availability Zone.\"}\n]\n}", "docs": ["('REL11-BP02 Fail over to healthy resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_withstand_component_failures_failover2good.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Skills in", "item": "Identifying metrics based on business requirements to deliver a highly available solution", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* Create a business-level view of your operations activities to help you determine if you are satisfying needs and to identify areas that need improvement to reach business goals. Validate the effectiveness of KPIs and metrics and revise them if necessary.AWS also has support for third-party log analysis systems and business intelligence tools through the AWS service APIs and SDKs (for example, Grafana, Kibana, and Logstash).Common anti-patterns:The frequency of your deployments has increased with the growth in number of development teams. Your defined expected number of deployments is once per week. You have been regularly deploying daily. When their is an issue with your deployment system, and deployments are not possible, it goes undetected for days.When your business previously provided support only during core business hours from Monday to Friday. You established a next business day response time goal for incidents. You have recently started offering 24x7 support coverage with a two hour response time goal. Your overnight staff are overwhelmed and customers are unhappy. There is no indication that there are issues with incident response times because you are reporting against a next business day target.Benefits of establishing this best practice: By reviewing and revising KPIs and metrics, you understand how your workload supports the achievement of your business outcomes and can identify where improvement is needed to reach business goals.Level of risk exposed if this best practice is not established: LowValidate the achievement of outcomes and the effectiveness of KPIs and metrics: Create a business level view of your operations activities to help you determine if you are satisfying needs and to identify areas that need improvement to reach business goals. Validate the effectiveness of KPIs and metrics and revise them if necessary.Using Amazon CloudWatch dashboardsWhat is log analytics?Related documents:Using Amazon CloudWatch dashboardsWhat is log analytics?\n* To understand how your data management systems are performing, it is important to track relevant metrics. These metrics will help you to optimize your data management resources, to ensure that your workload requirements are met, and that you have a clear overview on how the workload performs. Use tools, libraries, and systems that record performance measurements related to database performance.There are metrics that are related to the system on which the database is being hosted (for example, CPU, storage, memory, IOPS), and there are metrics for accessing the data itself (for example, transactions per second, queries rates, response times, errors). These metrics should be readily accessible for any support or operational staff, and have sufficient historical record to be able to identify trends, anomalies, and bottlenecks.Desired outcome: To monitor the performance of your database workloads, you must record multiple performance metrics over a period of time. This allows you to detect anomalies as well as measure performance against business metrics to ensure you are meeting your workload needs.Common anti-patterns:You only use manual log file searching for metrics.You only publish metrics to internal tools used by your team and don\u2019t have a comprehensive picture of your workload.You only use the default metrics recorded by your selected monitoring software.You only review metrics when there is an issue.You only monitor system level metrics, not capturing data access or usage metrics.Benefits of establishing this best practice: Establishing a performance baseline helps in understanding normal behavior and requirements of workloads. Abnormal patterns can be identified and debugged faster improving performance and reliability of the database. Database capacity can be configured to ensure optimal cost without compromising performance.Level of risk exposed if this best practice is not established: HighInability to differentiate out of normal vs. normal performance level will create difficulties in issue identification, and decision making.Potential cost savings may not be identified.Growth patterns will not be identified which might result in reliability or performance degradation.Identify, collect, aggregate, and correlate database-related metrics. Metrics should include both the underlying system that is supporting the database and the database metrics. The underlying system metrics might include CPU utilization, memory, available disk storage, disk I/O, and network inbound and outbound metrics while the database metrics might include transactions per second, top queries, average queries rates, response times, index usage, table locks, query timeouts, and number of connections open. This data is crucial to understand how the workload is performing and how the database solution is used. Use these metrics as part of a data-driven approach to tune and optimize your workload's resources.Implementation steps:Which database metrics are important to track?Monitoring metrics for Amazon RDSMonitoring with Performance InsightsEnhanced monitoringDynamoDB metricsMonitoring DynamoDB DAXMonitoring MemoryDBMonitoring Amazon RedshiftTimeseries metrics and dimensionsCluster level metrics for AuroraMonitoring Amazon KeyspacesMonitoring Amazon NeptuneWould the database monitoring benefit from a machine learning solution that detects operational anomalies performance issues?Amazon DevOps Guru for Amazon RDS provides visibility into performance issues and makes recommendations for corrective actions.Do you need application level details about SQL usage?AWS X-Ray can be instrumented into the application to gain insights and encapsulate all the data points for single query.Do you currently have an approved logging and monitoring solution?Amazon CloudWatch can collect metrics across the resources in your architecture. You can also collect and publish custom metrics to surface business or derived metrics. Use CloudWatch or third-party solutions to set alarms that indicate when thresholds are breached.You identified and configured your data retention policies to match my security and operational goals?Default data retention for CloudWatch metricsDefault data retention for CloudWatch LogsLevel of effort for the implementation plan: There is a medium level of effort to identify, track, collect, aggregate, and correlate metrics from all database resources.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Performance InsightsRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Level 100: Monitoring with CloudWatch DashboardsAWS Dataset Ingestion Metrics Collection FrameworkAmazon RDS Monitoring Workshop\n\nKnowledge Area: Identifying metrics based on business requirements to deliver a highly available solution\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is important for the transportation company to track to ensure their operations are running smoothly and their goals are being met?\",\"answer_choices\": [{\"answer\": \"CPU utilization\",\"is_correct\": \"false\",\"explanation\": \"CPU utilization is an underlying system metric that is important to track, but does not directly relate to the operations of the transportation company.\"},{\"answer\": \"Number of deployments\",\"is_correct\": \"false\",\"explanation\": \"Number of deployments is an important metric to track, but does not directly relate to the operations of the transportation company.\"},{\"answer\": \"Response times for incidents\",\"is_correct\": \"false\",\"explanation\": \"Response times for incidents is an important metric to track, but does not directly relate to the operations of the transportation company.\"},{\"answer\": \"Metrics related to database performance\",\"is_correct\": \"true\",\"explanation\": \"Metrics related to database performance are important for the transportation company to track in order to understand how their data management systems are performing and ensure that their workload requirements are met.\"}]}", "docs": ["('OPS09-BP08 Validate the achievement of outcomes and the effectiveness of KPIs and metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_operations_health_biz_level_view_ops.html')", "('PERF04-BP03 Collect and record database performance metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_collect_metrics.html')"]}, {"scenario": "A company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Skills in", "item": "Implementing designs to mitigate single points of failure", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.`\"\n\nContext:\n\n* There are four best practice areas for reliability in the cloud:FoundationsWorkload ArchitectureChange ManagementFailure ManagementTo achieve reliability you must start with the foundations \u2014 an environment where service quotas and network topology accommodate the workload. The workload architecture of the distributed system must be designed to prevent and mitigate failures. The workload must handle changes in demand or requirements, and it must be designed to detect failure and automatically heal itself.\n* Use frequent, small, and reversible changes to reduce the scope of a change. This results in easier troubleshooting and faster remediation with the option to roll back a change.Common anti-patterns:You deploy a new version of your application quarterly.You frequently make changes to your database schema.You perform manual in-place updates, overwriting existing installations and configurations.Benefits of establishing this best practice: You recognize benefits from development efforts faster by deploying small changes frequently. When the changes are small it is much easier to identify if they have unintended consequences. When the changes are reversible there is less risk to implementing the change as recovery is simplified.Level of risk exposed if this best practice is not established: LowDeploy frequent, small, reversible changes: Use frequent, small, and reversible changes to reduce the scope of a change. This results in easier troubleshooting and faster remediation with the option to roll back a change.\n* Distributed systems rely on communications networks to interconnect components, such as servers or services. Your workload must operate reliably despite data loss or latency in these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices prevent failures and improve mean time between failures (MTBF).Best practices\n* After you have designed your workload to be resilient to the stresses of production, testing is the only way to ensure that it will operate as designed, and deliver the resiliency you expect.Best practices\n* Plan to revert to a known good state, or remediate in the production environment if a change does not have the desired outcome. This preparation reduces recovery time through faster responses.Common anti-patterns:You performed a deployment and your application has become unstable but there appear to be active users on the system. You have to decide whether to roll back the change and impact the active users or wait to roll back the change knowing the users may be impacted regardless.After making a routine change, your new environments are accessible but one of your subnets has become unreachable. You have to decide whether to roll back everything or try to fix the inaccessible subnet. While you are making that determination, the subnet remains unreachable.Benefits of establishing this best practice: Having a plan in place reduces the mean time to recover (MTTR) from unsuccessful changes, reducing the impact to your end users.Level of risk exposed if this best practice is not established: HighPlan for unsuccessful changes: Plan to revert to a known good state (that is, roll back the change), or remediate in the production environment (that is, roll forward the change) if a change does not have the desired outcome. When you identify changes that you cannot roll back if unsuccessful, apply due diligence prior to committing the change.\n* Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.Best practices\n* There are five design principles for reliability in the cloud:Automatically recover from failure: By monitoring a workload for key performance indicators (KPIs), you can trigger automation when a threshold is breached. These KPIs should be a measure of business value, not of the technical aspects of the operation of the service. This allows for automatic notification and tracking of failures, and for automated recovery processes that work around or repair the failure. With more sophisticated automation, it\u2019s possible to anticipate and remediate failures before they occur.Test recovery procedures: In an on-premises environment, testing is often conducted to prove that the workload works in a particular scenario. Testing is not typically used to validate recovery strategies. In the cloud, you can test how your workload fails, and you can validate your recovery procedures. You can use automation to simulate different failures or to recreate scenarios that led to failures before. This approach exposes failure pathways that you can test and fix before a real failure scenario occurs, thus reducing risk.Scale horizontally to increase aggregate workload availability: Replace one large resource with multiple small resources to reduce the impact of a single failure on the overall workload. Distribute requests across multiple, smaller resources to ensure that they don\u2019t share a common point of failure.Stop guessing capacity: A common cause of failure in on-premises workloads is resource saturation, when the demands placed on a workload exceed the capacity of that workload (this is often the objective of denial of service attacks). In the cloud, you can monitor demand and workload utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over- or under-provisioning. There are still limits, but some quotas can be controlled and others can be managed (see Manage Service Quotas and Constraints).Manage change in automation: Changes to your infrastructure should be made using automation. The changes that need to be managed include changes to the automation, which then can be tracked and reviewed.\n* Distributed systems rely on communications networks to interconnect components (such as servers or services). Your workload must operate reliably despite data loss or latency over these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices enable workloads to withstand stresses or failures, more quickly recover from them, and mitigate the impact of such impairments. The result is improved mean time to recovery (MTTR).Best practices\n* Frequent, small, and reversible changes reduce the scope and impact of a change. This eases troubleshooting, enables faster remediation, and provides the option to roll back a change.Common anti-patterns:You deploy a new version of your application quarterly.You frequently make changes to your database schema.You perform manual in-place updates, overwriting existing installations and configurations.Benefits of establishing this best practice: You recognize benefits from development efforts faster by deploying small changes frequently. When the changes are small, it is much easier to identify if they have unintended consequences. When the changes are reversible, there is less risk to implementing the change as recovery is simplified.Level of risk exposed if this best practice is not established: LowMake frequent, small, reversible changes: Frequent, small, and reversible changes reduce the scope and impact of a change. This eases troubleshooting, enables faster remediation, and provides the option to roll back a change. It also increases the rate at which you can deliver value to the business.\n* Adopt approaches that provide fast feedback on quality and enable rapid recovery from changes that do not have desired outcomes. Using these practices mitigates the impact of issues introduced through the deployment of changes.Best practices\n\nKnowledge Area: Implementing designs to mitigate single points of failure\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following best practices should be used to minimize costs when running batch processing jobs on AWS?\", \"answer_choices\": [{\"answer\": \"Deploy quarterly versions of the application\", \"is_correct\": \"false\", \"explanation\": \"Deploying quarterly versions of the application would not minimize costs when running batch processing jobs on AWS. Frequent, small, and reversible changes should be used to reduce the scope of a change, which results in easier troubleshooting and faster remediation with the option to roll back a change.\"},{\"answer\": \"Perform manual in-place updates, overwriting existing installations and configurations\", \"is_correct\": \"false\", \"explanation\": \"Performing manual in-place updates, overwriting existing installations and configurations would not minimize costs when running batch processing jobs on AWS. Frequent, small, and reversible changes should be used to reduce the scope of a change, which results in easier troubleshooting and faster remediation with the option to roll back a change.\"},{\"answer\": \"Make frequent, small, and reversible changes\", \"is_correct\": \"true\", \"explanation\": \"Frequent, small, and reversible changes should be used to reduce the scope of a change, which results in easier troubleshooting and faster remediation with the option to roll back a change, therefore minimizing costs when running batch processing jobs on AWS.\"},{\"answer\": \"Test recovery procedures\", \"is_correct\": \"false\", \"explanation\": \"Testing recovery procedures would not minimize costs when running batch processing jobs on AWS. Frequent, small, and reversible changes should be used to reduce the scope of a change, which results in easier troubleshooting and faster remediation with the option to roll back a change.\"}]}", "docs": ["('Definition - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-def.html')", "('OPS06-BP06 Deploy frequent, small, reversible changes - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_mit_deploy_risks_freq_sm_rev_chg.html')", "('REL 4\\xa0 How do you design interactions in a distributed system to prevent failures? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-04.html')", "('REL 12\\xa0 How do you test reliability? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-12.html')", "('OPS06-BP01 Plan for unsuccessful changes - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_mit_deploy_risks_plan_for_unsucessful_changes.html')", "('PERF 1\\xa0 How do you select the best performing architecture? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-01.html')", "('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-dp.html')", "('REL 5\\xa0 How do you design interactions in a distributed system to mitigate or withstand failures? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-05.html')", "('OPS05-BP09 Make frequent, small, reversible changes - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_dev_integ_freq_sm_rev_chg.html')", "('OPS 6\\xa0 How do you mitigate deployment risks? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops-06.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Skills in", "item": "Selecting an appropriate DR strategy to meet business requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* Having backups and redundant workload components in place is the start of your DR strategy. RTO and RPO are your objectives for restoration of your workload. Set these based on business needs. Implement a strategy to meet these objectives, considering locations and function of workload resources and data. The probability of disruption and cost of recovery are also key factors that help to inform the business value of providing disaster recovery for a workload.Best practices\n* Define a disaster recovery (DR) strategy that meets your workload's recovery objectives. Choose a strategy such as: backup and restore; standby (active/passive); or active/active.\n* Choose Regions where you will implement your workloads based on both your business requirements and sustainability goals.Best Practice:\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Where applicable, use multiple strategies to improve performance. For example, using strategies like caching data to prevent excessive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Common anti-patterns:You assume that workload performance is adequate if customers are not complaining.You only collect data on performance after you have made performance-related changes.Benefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and historical reference. You can create dashboards and perform metric math on your data to derive operational and utilization insights for your workloads as they change over time.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Related documents:Amazon Builders\u2019 LibraryBest Practices for Implementing Amazon ElastiCacheAWS Database CachingAmazon CloudWatch RUMDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)AWS purpose-built databases (DAT209-L)Optimize applications through Amazon CloudWatch RUMRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n* Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO).Best practices\n\nKnowledge Area: Selecting an appropriate DR strategy to meet business requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"Which strategy should the company employ for storing log files for analysis and compliance purposes to meet their recovery time objectives (RTO) and recovery point objectives (RPO)?\",\n\"answer_choices\": [\n  {\"answer\": \"Backup and Restore\", \"is_correct\": \"true\", \"explanation\": \"Backup and restore is a disaster recovery strategy that can provide the company with the necessary RTO and RPO objectives for data recovery.\"},\n  {\"answer\": \"Active/Active\", \"is_correct\": \"false\", \"explanation\": \"Active/Active is a disaster recovery strategy that is designed for high availability and not for data recovery.\"},\n  {\"answer\": \"Standby (Active/Passive)\", \"is_correct\": \"false\", \"explanation\": \"Standby (Active/Passive) is a disaster recovery strategy that is designed for recovery from a catastrophic event, not for data recovery.\"},\n  {\"answer\": \"Caching Data\", \"is_correct\": \"false\", \"explanation\": \"Caching data is a strategy used to improve performance, and not for data recovery.\"}\n  ]\n}", "docs": ["('REL 13\\xa0 How do you plan for disaster recovery (DR)? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-13.html')", "('REL13-BP02 Use defined recovery strategies to meet the recovery objectives - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_disaster_recovery.html')", "('SUS 1 How do you select Regions to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-01.html')", "('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('PERF08-BP05 Use various performance-related strategies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_implement_strategy.html')", "('REL 9\\xa0 How do you back up data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-09.html')"]}, {"scenario": "A company has a critical application that needs to be up at all times", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Skills in", "item": "Using AWS services that improve the reliability of legacy applications and applications not built for the cloud (for example, when application changes are not possible)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company has a critical application that needs to be up at all times`\"\n\nContext:\n\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* As AWS releases new services and features, it's a best practice to review your existing architectural decisions to ensure they continue to be the most cost effective.Best practices\n* Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Update deployed components as your needs change.Level of risk exposed if this best practice is not established: MediumEnable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.Scale using small increments for variable workloads.Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years.Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.Related documents:AWS Compute Optimizer DocumentationOperating Lambda: Performance optimizationAuto Scaling Documentation\n* As AWS releases new services and features, it's a best practice to review your existing architectural decisions to ensure they continue to be the most cost effective. As your requirements change, be aggressive in decommissioning resources, entire services, and systems that you no longer require.Implementing new features or resource types can optimize your workload incrementally, while minimizing the effort required to implement the change. This provides continual improvements in efficiency over time and ensures you remain on the most updated technology to reduce operating costs. You can also replace or add new components to the workload with new services. This can provide significant increases in efficiency, so it's essential to regularly review your workload, and implement new services and features.The following question focuses on these considerations for cost optimization.When regularly reviewing your deployments, assess how newer services can help save you money. For example, Amazon Aurora on RDS can reduce costs for relational databases. Using serverless such as Lambda can remove the need to operate and manage instances to run code.COST 10: How do you evaluate new services?As AWS releases new services and features, it's a best practice to review your existing architectural decisions to ensure they continue to be the most cost effective.\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n* Managed services shift responsibility for maintaining high-average utilization, and sustainability optimization of the deployed hardware to AWS. Use managed services to distribute the sustainability impact of the service across all tenants of the service, reducing your individual contribution.Level of risk exposed if this best practice is not established: LowMigrate from self-hosted services to managed services. For example, use managed Amazon Relational Database Service (Amazon RDS) instances instead of maintaining your own Amazon RDS instances on Amazon Elastic Compute Cloud (Amazon EC2), or use managed container services, such as AWS Fargate, instead of implementing your own container infrastructure.Related documents:AWS FargateAmazon DocumentDBAmazon Elastic Kubernetes Service (EKS)Amazon Managed Streaming for Apache Kafka (Amazon MSK)Amazon RedshiftAmazon Relational Database Service (RDS)\n\nKnowledge Area: Using AWS services that improve the reliability of legacy applications and applications not built for the cloud (for example, when application changes are not possible)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following services is best suited for a company who has a critical application that needs to be up at all times?\", \n\"answer_choices\": [{\"answer\": \"Amazon EC2\",\"is_correct\": \"false\", \"explanation\": \"Amazon EC2 is computing service that allows users to launch and manage virtual machines. It is not suitable for critical applications that require high availability.\"},{\"answer\": \"Amazon RDS\",\"is_correct\": \"true\", \"explanation\": \"Amazon RDS is a managed relational database service that helps set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This makes it ideal for critical applications that need to be up at all times.\"},{\"answer\": \"AWS Lambda\",\"is_correct\": \"false\", \"explanation\": \"AWS Lambda is a serverless compute service that lets users run code without provisioning or managing servers. It is not suitable for critical applications that require high availability.\"},{\"answer\": \"Amazon ECS\",\"is_correct\": \"false\", \"explanation\": \"Amazon ECS is a container management service that helps users run and scale containerized applications. It is not suitable for critical applications that require high availability.\"}]}", "docs": ["('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('COST 10\\xa0 How do you evaluate new services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-10.html')", "('SUS05-BP01 Use the minimum amount of hardware to meet your needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a2.html')", "('Optimize over time - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-opti.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')", "('SUS05-BP03 Use managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a4.html')"]}, {"scenario": "A marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.", "domain": "Domain 2: Design Resilient Architectures", "task": "Task Statement 2 :Design highly available and/or fault-tolerant architectures", "focus": "Skills in", "item": "Using purpose-built AWS services for workloads", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.`\"\n\nContext:\n\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n* Managed services shift responsibility for maintaining high-average utilization, and sustainability optimization of the deployed hardware to AWS. Use managed services to distribute the sustainability impact of the service across all tenants of the service, reducing your individual contribution.Level of risk exposed if this best practice is not established: LowMigrate from self-hosted services to managed services. For example, use managed Amazon Relational Database Service (Amazon RDS) instances instead of maintaining your own Amazon RDS instances on Amazon Elastic Compute Cloud (Amazon EC2), or use managed container services, such as AWS Fargate, instead of implementing your own container infrastructure.Related documents:AWS FargateAmazon DocumentDBAmazon Elastic Kubernetes Service (EKS)Amazon Managed Streaming for Apache Kafka (Amazon MSK)Amazon RedshiftAmazon Relational Database Service (RDS)\n* Use metrics from the currently running workload to select the right size and type to optimize for cost. Appropriately provision throughput, sizing, and storage for services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Elastic Block Store (Amazon EBS) (PIOPS), Amazon Relational Database Service (Amazon RDS), Amazon EMR, and networking. This can be done with a feedback loop such as automatic scaling or by custom code in the workload.Level of risk exposed if this best practice is not established: LowCreate a feedback loop within the workload that uses active metrics from the running workload to make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you configure to perform the right sizing operations for you. AWS also provides APIs, SDKs, and features that allow resources to be modified with minimal effort. You can program a workload to stop-and-start an Amazon Elastic Compute Cloud(Amazon EC2) instance to allow a change of instance size or instance type. This provides the benefits of right-sizing while removing almost all the operational cost required to make the change.Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage Service(Amazon S3) Intelligent-Tiering. Amazon S3 Intelligent-Tiering automatically moves your data between two access tiers: frequent access and infrequent access, based on your usage patterns.Implementation stepsConfigure workload metrics: Ensure you capture the key metrics for the workload. These metrics provide an indication of the customer experience, such as the workload output, and align to the differences between resource types and sizes, such as CPU and memory usage.View rightsizing recommendations: Use the rightsizing recommendations in AWS Compute Optimizer to make adjustments to your workload.Select resource type and size automatically based on metrics: Using the workload metrics, manually or automatically select your workload resources. Configuring AWS Auto Scaling or implementing code within your application can reduce the effort required if frequent changes are needed, and it can potentially implement changes sooner than a manual process.Related documents:AWS Auto ScalingAWS Compute OptimizerAmazon CloudWatch featuresCloudWatch Getting Set UpCloudWatch Publishing Custom MetricsCost Optimization: Amazon EC2 Right SizingGetting Started with Amazon EC2 Auto ScalingAmazon S3 Intelligent-TieringLaunch an EC2 Instance Using the SDK\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n\nKnowledge Area: Using purpose-built AWS services for workloads\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service should be used to process and analyze large volumes of customer data for targeted advertising campaigns?\", \"answer_choices\": [{\"answer\": \"Amazon EC2\",\"is_correct\": \"false\",\"explanation\": \"Amazon EC2 is a building-block AWS service and is not purpose-built for this task.\"},{\"answer\": \"Amazon EBS\",\"is_correct\": \"false\",\"explanation\": \"Amazon EBS is a building-block AWS service and is not purpose-built for this task.\"},{\"answer\": \"Amazon S3\",\"is_correct\": \"false\",\"explanation\": \"Amazon S3 is a building-block AWS service and is not purpose-built for this task.\"},{\"answer\": \"Amazon RDS or Amazon DynamoDB\",\"is_correct\": \"true\",\"explanation\": \"Amazon RDS and Amazon DynamoDB are managed services and are purpose-built for this task.\"}]}", "docs": ["('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')", "('SUS05-BP03 Use managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a4.html')", "('COST06-BP03 Select resource type, size, and number automatically based on metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_metrics.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')"]}, {"scenario": "A marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 1: Determine high-performing and/or scalable storage solutions", "focus": "Knowledge of", "item": "Hybrid storage solutions to meet business requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.`\"\n\nContext:\n\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n* Identify and document the workload storage needs and define the storage characteristics of each location. Examples of storage characteristics include: shareable access, file size, growth rate, throughput, IOPS, latency, access patterns, and persistence of data. Use these characteristics to evaluate if block, file, object, or instance storage services are the most efficient solution for your storage needs.\n* Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.Best practices:\n* Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file storage services as well as cloud data migration options for your workload.In AWS, storage is available in three forms: object, block, and file:Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems.The following question focuses on these considerations for performance efficiency.When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want.PERF 3: How do you select your storage solution?The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n\nKnowledge Area: Hybrid storage solutions to meet business requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When selecting a storage solution for the marketing company to process and analyze large volumes of customer data for targeted advertising campaigns, what is the best practice for achieving the desired performance?\", \n\"answer_choices\": [\n{\"answer\": \"Monitor your data access patterns.\", \"is_correct\": true, \"explanation\": \"Monitoring your data access patterns and selecting the appropriate storage technology based on them are important for achieving the desired performance.\"}, \n{\"answer\": \"Use Solid State Devices (SSDs).\", \"is_correct\": false, \"explanation\": \"Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases.\"},\n{\"answer\": \"Migrate data to storage designed for that purpose.\", \"is_correct\": false, \"explanation\": \"Migrating archival data to storage designed for that purpose, such as archival-class storage, is important for infrequently accessed data.\"},\n{\"answer\": \"Choose a storage solution that aligns with your access patterns.\", \"is_correct\": true, \"explanation\": \"Ensuring that the storage solution aligns with your access patterns is critical to achieving the desired performance.\"}]\n}", "docs": ["('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')", "('PERF03-BP01 Understand storage characteristics and requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_understand_char.html')", "('SUS 4 How do you take advantage of data access and usage patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-04.html')", "('Storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-storage.html')", "('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 1: Determine high-performing and/or scalable storage solutions", "focus": "Knowledge of", "item": "Storage services with appropriate use cases (for example, Amazon S3, Amazon Elastic File System [Amazon EFS], Amazon Elastic Block Store [Amazon EBS])", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file storage services as well as cloud data migration options for your workload.In AWS, storage is available in three forms: object, block, and file:Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems.The following question focuses on these considerations for performance efficiency.When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want.PERF 3: How do you select your storage solution?The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n\nKnowledge Area: Storage services with appropriate use cases (for example, Amazon S3, Amazon Elastic File System [Amazon EFS], Amazon Elastic Block Store [Amazon EBS])\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When selecting a storage solution for a retail company's e-commerce platform during peak shopping seasons, which storage service should be considered?\", \n\"answer_choices\": [\n{\"answer\": \"Amazon Glacier\",\"is_correct\": \"false\",\"explanation\": \"Amazon Glacier is an archival storage solution and is therefore not suited for a retail company's ecommerce platform during peak shopping seasons, which requires low-latency and high-performance storage.\"},\n{\"answer\": \"Amazon S3\",\"is_correct\": \"true\",\"explanation\": \"Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. It can provide a scalable and durable platform to make data accessible from any internet location during peak shopping seasons.\"},\n{\"answer\": \"Amazon EFS\",\"is_correct\": \"false\",\"explanation\": \"Amazon EFS provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is not suited for a retail company's ecommerce platform during peak shopping seasons, which requires low-latency and high-performance storage.\"},\n{\"answer\": \"Amazon EBS\",\"is_correct\": \"false\",\"explanation\": \"Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. However, it is not suited for a retail company's ecommerce platform during peak shopping seasons, which requires low-latency and high-performance storage.\"}\n]}", "docs": ["('Storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-storage.html')", "('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')"]}, {"scenario": "A company wants to build a mobile application that needs to access and store data in the cloud.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 1: Determine high-performing and/or scalable storage solutions", "focus": "Knowledge of", "item": "Storage types with associated characteristics (for example, object, file, block)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to build a mobile application that needs to access and store data in the cloud.`\"\n\nContext:\n\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n* Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file storage services as well as cloud data migration options for your workload.In AWS, storage is available in three forms: object, block, and file:Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems.The following question focuses on these considerations for performance efficiency.When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want.PERF 3: How do you select your storage solution?The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n\nKnowledge Area: Storage types with associated characteristics (for example, object, file, block)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"When selecting a storage solution for a mobile application that needs to access and store data in the cloud, which storage type should you choose to optimize performance and use resources efficiently?\", \n\"answer_choices\": [\n    {\"answer\": \"Provisioned IOPS for all workloads\",\"is_correct\": \"false\",\"explanation\": \"This is an anti-pattern and is not a recommended approach. Provisioned IOPS should be used after testing against all storage tiers.\"},\n    {\"answer\": \"Object storage, such as Amazon S3\",\"is_correct\": \"true\",\"explanation\": \"Object storage, such as Amazon S3, is ideal for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery.\"},\n    {\"answer\": \"Block storage, such as Amazon EBS\",\"is_correct\": \"false\",\"explanation\": \"Block storage, such as Amazon EBS, is designed for workloads that require persistent storage accessible by EC2 instances.\"},\n    {\"answer\": \"File storage, such as Amazon EFS\",\"is_correct\": \"false\",\"explanation\": \"File storage is ideal for use cases such as large content repositories, development environments, media stores, or user home directories.\"}\n]\n}", "docs": ["('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')", "('Storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-storage.html')", "('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')"]}, {"scenario": "A software development team wants to build and deploy a microservices-based application on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 1: Determine high-performing and/or scalable storage solutions", "focus": "Skills in", "item": "Determining storage services and configurations that meet performance demands", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA software development team wants to build and deploy a microservices-based application on AWS.`\"\n\nContext:\n\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n\nKnowledge Area: Determining storage services and configurations that meet performance demands\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"For a software development team wanting to build and deploy a microservices-based application on AWS, what is the best storage service to use to optimize storage performance and cost?\", \n\"answer_choices\": [\n{\"answer\": \"Amazon S3 with CloudFront\",\"is_correct\": \"False\",\"explanation\": \"Amazon S3 with CloudFront is best used for intensive GET requests and uploading large files. It is not the best option for optimizing storage performance and cost.\"},\n{\"answer\": \"Amazon EBS\",\"is_correct\": \"True\",\"explanation\": \"Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. It is the best service to use for optimizing storage performance and cost.\"},\n{\"answer\": \"Amazon EFS\",\"is_correct\": \"False\",\"explanation\": \"Amazon EFS is a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources, but it is not the best option for optimizing storage performance and cost.\"},\n{\"answer\": \"Amazon FSx\",\"is_correct\": \"False\",\"explanation\": \"Amazon FSx provides four file systems to choose from, but it is not the best option for optimizing storage performance and cost.\"}\n]}", "docs": ["('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 1: Determine high-performing and/or scalable storage solutions", "focus": "Skills in", "item": "Determining storage services that can scale to accommodate future needs", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* A scalable workload provides elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time.Best practices\n* The way users consume your workloads and other resources can help you identify improvements to meet sustainability goals. Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed. Align service levels to customer needs. Position resources to limit the network required for users to consume them. Remove existing, unused assets. Identify created assets that are unused and stop generating them. Provide your team members with devices that support their needs with minimized sustainability impact.Best practices:\n* Cost efficient agreements and terms ensure the cost of these services scales with the benefits they provide. Select agreements and pricing that scale when they provide additional benefits to your organization.Level of risk exposed if this best practice is not established: MediumWhen you utilize third-party solutions or services in the cloud, it is important that the pricing structures are aligned to Cost Optimization outcomes. Pricing should scale with the outcomes and value it provides. An example of this is software that takes a percentage of savings it provides, the more you save (outcome) the more it charges. Agreements that scale with your bill are typically not aligned to Cost Optimization, unless they provide outcomes for every part of your specific bill. For example, a solution that provides recommendations for Amazon Elastic Compute Cloud(Amazon EC2) and charges a percentage of your entire bill will increase if you use other services for which it provides no benefit. Another example is a managed service that is charged at a percentage of the cost of resources that are managed. A larger instance size may not necessarily require more management effort, but will be charged more. Ensure that these service pricing arrangements include a cost optimization program or features in their service to drive efficiency.Implementation stepsAnalyze third-party agreements and terms: Review the pricing in third party agreements. Perform modeling for different levels of your usage, and factor in new costs such as new service usage, or increases in current services due to workload growth. Decide if the additional costs provide the required benefits to your business.Related documents:Accessing Reserved Instance recommendationsInstance purchasing optionsRelated videos:Save up to 90% and run production workloads on Spot\n* To minimize total provisioned storage, create block storage with size allocations that are appropriate for the workload. Use elastic volumes to expand storage as data grows without having to resize storage attached to compute resources. Regularly review elastic volumes and shrink over-provisioned volumes to fit the current data size.Level of risk exposed if this best practice is not established: LowMonitor the utilization of your data volumes.Use elastic volumes and managed block data services to automate allocation of additional storage as your persistent data grows.Set target levels of utilization for your data volumes, and resize volumes outside of expected ranges.Size read-only volumes to fit the data.Migrate data to object stores to avoid provisioning the excess capacity from fixed volume sizes on block storage.Related documents:Amazon EBS Elastic VolumesAmazon FSx DocumentationWhat is Amazon CloudWatch?What is Amazon Elastic File System?\n* Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Update deployed components as your needs change.Level of risk exposed if this best practice is not established: MediumEnable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.Scale using small increments for variable workloads.Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years.Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.Related documents:AWS Compute Optimizer DocumentationOperating Lambda: Performance optimizationAuto Scaling Documentation\n\nKnowledge Area: Determining storage services that can scale to accommodate future needs\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"Which of the following is the best way to accommodate the company\u2019s need for scalable storage to process financial transactions?\",\n\"answer_choices\":\n[{\"answer\":\"Use Amazon EBS Elastic Volumes in order to expand storage as data grows without having to resize storage attached to compute resources.\",\"is_correct\":\"true\",\"explanation\":\"Amazon EBS Elastic Volumes enables elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time. It also allows for storage to expand as data grows without having to resize storage attached to compute resources.\"},\n{\"answer\":\"Choose software that takes a percentage of savings it provides, the more you save (outcome) the more it charges.\",\"is_correct\":\"false\",\"explanation\":\"Software that takes a percentage of savings it provides, the more you save (outcome) the more it charges is not the best way to accommodate the company\u2019s need for scalable storage to process financial transactions.\"},\n{\"answer\":\"Migrate data to object stores to avoid provisioning the excess capacity from fixed volume sizes on block storage.\",\"is_correct\":\"true\",\"explanation\":\"Migrating data to object stores can help minimize total provisioned storage and avoid provisioning the excess capacity from fixed volume sizes on block storage.\"},\n{\"answer\":\"Enable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.\",\"is_correct\":\"false\",\"explanation\":\"Horizontal scaling and the use of automation to scale out as loads increase and to scale in as loads decrease is not the best way to accommodate the company\u2019s need for scalable storage to process financial transactions.\"}]}", "docs": ["('REL 7\\xa0 How do you design your workload to adapt to changes in demand? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-07.html')", "('SUS 2 How do you take advantage of user behavior patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-02.html')", "('COST07-BP03 Select third-party agreements with cost-efficient terms - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_pricing_model_third_party.html')", "('SUS04-BP04 Minimize over-provisioning in block storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a5.html')", "('SUS05-BP01 Use the minimum amount of hardware to meet your needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a2.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Knowledge of", "item": "AWS compute services with appropriate use cases (for example, AWS Batch, Amazon EMR, Fargate)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Selecting compute resources that meet your requirements, performance needs, and provide great efficiency of cost and effort will enable you to accomplish more with the same number of resources. When evaluating compute options, be aware of your requirements for workload performance and cost requirements and use this to make informed decisions.In AWS, compute is available in three forms: instances, containers, and functions:Instances are virtualized servers, allowing you to change their capabilities with a button or an API call. Because resource decisions in the cloud aren\u2019t fixed, you can experiment with different server types. At AWS, these virtual server instances come in different families and sizes, and they offer a wide variety of capabilities, including solid-state drives (SSDs) and graphics processing units (GPUs).Containers are a method of operating system virtualization that allow you to run an application and its dependencies in resource-isolated processes. AWS Fargate is serverless compute for containers or Amazon EC2 can be used if you need control over the installation, configuration, and management of your compute environment. You can also choose from multiple container orchestration platforms: Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS).Functions abstract the execution environment from the code you want to execute. For example, AWS Lambda allows you to execute code without running an instance.The following question focuses on these considerations for performance efficiency.When architecting your use of compute you should take advantage of the elasticity mechanisms available to ensure you have sufficient capacity to sustain performance as demand changes.PERF 2: How do you select your compute solution?The optimal compute solution for a workload varies based on application design, usage patterns, and configuration settings. Architectures can use different compute solutions for various components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency.\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n\nKnowledge Area: AWS compute services with appropriate use cases (for example, AWS Batch, Amazon EMR, Fargate)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When architecting your use of compute for a disaster recovery plan, what should you consider to ensure sufficient capacity to sustain performance as demand changes?\", \n\"answer_choices\": [\n    {\"answer\": \"Using Amazon EC2 for serverless compute to eliminate installation, configuration and management of your compute environment.\",\"is_correct\": \"False\",\"explanation\": \"Amazon EC2 is a virtualized server, not serverless compute.\"}, \n    {\"answer\": \"Using Amazon EBS to monitor resource decisions in the cloud that aren\u2019t fixed.\",\"is_correct\": \"False\",\"explanation\": \"Amazon EBS is a block storage service, not a monitoring service.\"}, \n    {\"answer\": \"Using AWS Fargate to take advantage of elasticity mechanisms.\",\"is_correct\": \"True\",\"explanation\": \"AWS Fargate is a serverless compute service that allows you to take advantage of elasticity mechanisms.\"}, \n    {\"answer\": \"Using Amazon S3 to gain control over the installation configuration and management of your compute environment.\",\"is_correct\": \"False\",\"explanation\": \"Amazon S3 is a file storage service, not a compute service.\"}]}", "docs": ["('Compute - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-compute.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Knowledge of", "item": "Distributed computing concepts supported by AWS global infrastructure and edge services", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* Use the cloud location options available to reduce network latency or improve throughput. Use AWS Regions, Availability Zones, placement groups, and edge locations such as AWS Outposts, AWS Local Zones, and AWS Wavelength, to reduce network latency or improve throughput.The AWS Cloud infrastructure is built around Regions and Availability Zones. A Region is a physical location in the world having multiple Availability Zones.Availability Zones consist of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. These Availability Zones offer you the ability to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data centerChoose the appropriate Region or Regions for your deployment based on the following key elements:Where your users are located: Choosing a Region close to your workload\u2019s users ensures lower latency when they use the workload.Where your data is located: For data-heavy applications, the major bottleneck in latency is data transfer. Application code should execute as close to the data as possible.Other constraints: Consider constraints such as security and compliance.Amazon EC2 provides placement groups for networking. A placement group is a logical grouping of instances to decrease latency or increase reliability. Using placement groups with supported instance types and an Elastic Network Adapter (ENA) enables workloads to participate in a low-latency, 25 Gbps network. Placement groups are recommended for workloads that benefit from low network latency, high network throughput, or both. Using placement groups has the benefit of lowering jitter in network communications.Latency-sensitive services are delivered at the edge using a global network of edge locations. These edge locations commonly provide services such as content delivery network (CDN) and domain name system (DNS). By having these services at the edge, workloads can respond with low latency to requests for content or DNS resolution. These services also provide geographic services such as geo targeting of content (providing different content based on the end users\u2019 location), or latency-based routing to direct end users to the nearest Region (minimum latency).Amazon CloudFront is a global CDN that can be used to accelerate both static content such as images, scripts, and videos, as well as dynamic content such as APIs or web applications. It relies on a global network of edge locations that will cache the content and provide high-performance network connectivity to your users. CloudFront also accelerates many other features such as content uploading and dynamic applications, making it a performance addition to all applications serving traffic over the internet. Lambda@Edge is a feature of Amazon CloudFront that will let you run code closer to users of your workload, which improves performance and reduces latency.Amazon Route 53 is a highly available and scalable cloud DNS web service. It\u2019s designed to give developers and businesses an extremely reliable and cost-effective way to route end users to internet applications by translating names, like www.example.com, into numeric IP addresses, like 192.168.2.1, that computers use to connect to each other. Route 53 is fully compliant with IPv6.AWS Outposts is designed for workloads that need to remain on-premises due to latency requirements, where you want that workload to run seamlessly with the rest of your other workloads in AWS. AWS Outposts are fully managed and configurable compute and storage racks built with AWS-designed hardware that allow you to run compute and storage on-premises, while seamlessly connecting to the broad array of AWS services in in the cloud.AWS Local Zones is designed to run workloads that require single-digit millisecond latency, like video rendering and graphics intensive, virtual desktop applications. Local Zones allow you to gain all the benefits of having compute and storage resources closer to end-users.AWS Wavelength is designed to deliver ultra-low latency applications to 5G devices by extending AWS infrastructure, services, APIs, and tools to 5G networks. Wavelength embeds storage and compute inside telco providers 5G networks to help your 5G workload if it requires single-digit millisecond latency, such as IoT devices, game streaming, autonomous vehicles, and live media production.Use edge services to reduce latency and to enable content caching. Ensure that you have configured cache control correctly for both DNS and HTTP/HTTPS to gain the most benefit from these approaches.Common anti-patterns:You consolidate all workload resources into one geographic location.You chose the closest region to your location but not to the workload end user.Benefits of establishing this best practice: You must ensure that your network is available wherever you want to reach customers. Using the AWS private global network ensures that your customers get the lowest latency experience by deploying workloads into the locations nearest them.Level of risk exposed if this best practice is not established: MediumReduce latency by selecting the correct locations: Identify where your users and data are located. Take advantage of AWS Regions, Availability Zones, placement groups, and edge locations to reduce latency.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* Distribute workload data and resources across multiple Availability Zones or, where necessary, across AWS Regions. These locations can be as diverse as required.\n* There are five design principles for performance efficiency in the cloud:Democratize advanced technologies: Make advanced technology implementation easier for your team by delegating complex tasks to your cloud vendor. Rather than asking your IT team to learn about hosting and running a new technology, consider consuming the technology as a service. For example, NoSQL databases, media transcoding, and machine learning are all technologies that require specialized expertise. In the cloud, these technologies become services that your team can consume, allowing your team to focus on product development rather than resource provisioning and management.Go global in minutes: Deploying your workload in multiple AWS Regions around the world allows you to provide lower latency and a better experience for your customers at minimal cost.Use serverless architectures: Serverless architectures remove the need for you to run and maintain physical servers for traditional compute activities. For example, serverless storage services can act as static websites (removing the need for web servers) and event services can host code. This removes the operational burden of managing physical servers, and can lower transactional costs because managed services operate at cloud scale.Experiment more often: With virtual and automatable resources, you can quickly carry out comparative testing using different types of instances, storage, or configurations.Consider mechanical sympathy: Understand how cloud services are consumed and always use the technology approach that aligns best with your workload goals. For example, consider data access patterns when you select database or storage approaches.\n\nKnowledge Area: Distributed computing concepts supported by AWS global infrastructure and edge services\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best approach for reducing latency for the gaming company's multiplayer game servers?\", \"answer_choices\": [{\"answer\": \"Deploy the infrastructure on multiple Availability Zones in the same AWS Region\",\"is_correct\": \"True\", \"explanation\": \"Deploying the infrastructure on multiple Availability Zones in the same AWS Region will reduce latency and provide higher availability and performance for the game servers.\"},{\"answer\": \"Deploy the infrastructure on a single Availability Zone in the closest AWS Region\",\"is_correct\": \"False\", \"explanation\": \"Deploying the infrastructure on a single Availability Zone in the closest AWS Region may not be the most optimal design for reducing latency and providing higher availability and performance for the game servers.\"},{\"answer\": \"Use AWS Local Zones to deploy the infrastructure\",\"is_correct\": \"False\", \"explanation\": \"AWS Local Zones are used to run workloads that require single-digit millisecond latency, such as video rendering and graphics intensive, virtual desktop applications. However, this may not be the most optimal design for reducing latency and providing higher availability and performance for the game servers.\"},{\"answer\": \"Use AWS Outposts to deploy the infrastructure\",\"is_correct\": \"False\", \"explanation\": \"AWS Outposts are used to run workloads that need to remain on-premises due to latency requirements, where you want that workload to run seamlessly with the rest of your other workloads in AWS. However, this may not be the most optimal design for reducing latency and providing higher availability and performance for the game servers.\"}]}", "docs": ["('PERF05-BP06 Choose your workload\u2019s location based on network requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_location.html')", "('REL10-BP01 Deploy the workload to multiple locations - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_fault_isolation_multiaz_region_system.html')", "('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-dp.html')"]}, {"scenario": "A social media platform wants to use AWS to store and process user-generated content.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Knowledge of", "item": "Queuing and messaging concepts (for example, publish/subscribe)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA social media platform wants to use AWS to store and process user-generated content.`\"\n\nContext:\n\n* Communicate directly with your users (for example, with email or SMS) when the services they use are impacted, and again when the services return to normal operating conditions, to enable users to take appropriate action.Common anti-patterns:Your application is experiencing a distributed denial of service incident and has been unresponsive for days. There is no error message. You have not sent a notification email. You have not sent text notifications. You have not shared information on social media. You customers are frustrated and looking for other vendors who can support them.On Monday, your application had issues following a patch and was down for a couple of hours. On Tuesday, your application had issues following a code deployment and was unreliable for a couple of hours. On Wednesday, your application had issues following a code deployment to mitigate a security vulnerability associated to the failed patch and was unavailable for a couple of hours. On Thursday, your frustrated customers started looking for another vendor who could support them.Your application is going to be down for maintenance this weekend. You don't inform your customers. Some of your customers had scheduled activities involving the use of your application. They are very frustrated upon discovery that your application is not available.Benefits of establishing this best practice: By defining notifications, triggers for notifications, and procedures for notifications you enable your customer to be informed and respond when issues with your workload impact them.Level of risk exposed if this best practice is not established: MediumEnable push notifications: Communicate directly with your users (for example, with email or SMS) when the services they use are impacted, and when the services return to normal operating conditions, to enable users to take appropriate action.Amazon SES featuresWhat is Amazon SES?Set up Amazon SNS notificationsRelated documents:Amazon SES featuresSet up Amazon SNS notificationsWhat is Amazon SES?\n* Use efficient software designs and architectures to minimize the average resources required per unit of work. Implement mechanisms that result in even utilization of components to reduce resources that are idle between tasks and minimize the impact of load spikes.Level of risk exposed if this best practice is not established: LowQueue requests that don\u2019t require immediate processing.Increase serialization to flatten utilization across your pipeline.Modify the capacity of individual components to prevent idling resources waiting for input.Create buffers and establish rate limiting to smooth the consumption of external services.Use the most efficient available hardware for your software optimizations.Use queue-driven architectures, pipeline management, and On-Demand Instance workers to maximize utilization for batch processing.Schedule tasks to avoid load spikes and resource contention from simultaneous execution.Schedule jobs during times of day where carbon intensity for power is lowest.Related documents:What is Amazon Simple Queue Service?What is Amazon MQ?Scaling based on Amazon SQSWhat is AWS Step Functions?What is AWS Lambda?Using AWS Lambda with Amazon SQSWhat is Amazon EventBridge?Related videos:Building Sustainably on AWSMoving to event-driven architectures\n\nKnowledge Area: Queuing and messaging concepts (for example, publish/subscribe)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"In order to ensure that users of a social media platform can take appropriate action when services they use are impacted, and when the services return to normal operating conditions, which concept should be used?\", \n\"answer_choices\": [\n    {\"answer\": \"Queue requests that don't require immediate processing.\",\"is_correct\": \"false\",\"explanation\": \"This would not enable users to take appropriate action when services are impacted.\"},\n    {\"answer\": \"Increase serialization to flatten utilization across your pipeline.\",\"is_correct\": \"false\",\"explanation\": \"This would not enable users to take appropriate action when services are impacted.\"},\n    {\"answer\": \"Set up Amazon SNS notifications.\",\"is_correct\": \"true\",\"explanation\": \"Amazon SNS notifications allow an application to send messages or notifications directly to users via email, SMS, or other messaging platforms.\"},\n    {\"answer\": \"Modify the capacity of individual components to prevent idling resources waiting for input.\",\"is_correct\": \"false\",\"explanation\": \"This would not enable users to take appropriate action when services are impacted.\"}\n]\n}", "docs": ["('OPS10-BP05 Enable push notifications - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_event_response_push_notify.html')", "('SUS03-BP01 Optimize software and architecture for asynchronous and scheduled jobs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a2.html')"]}, {"scenario": "A telecommunications company wants to use AWS to host and manage their network infrastructure.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Knowledge of", "item": "Scalability capabilities with appropriate use cases (for example, Amazon EC2 Auto Scaling, AWS Auto Scaling)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA telecommunications company wants to use AWS to host and manage their network infrastructure.`\"\n\nContext:\n\n* When replacing impaired resources or scaling your workload, automate the process by using managed AWS services, such as Amazon S3 and AWS Auto Scaling. You can also use third-party tools and AWS SDKs to automate scaling.Managed AWS services include Amazon S3, Amazon CloudFront, AWS Auto Scaling, AWS Lambda, Amazon DynamoDB, AWS Fargate, and Amazon Route 53.AWS Auto Scaling lets you detect and replace impaired instances. It also lets you build scaling plans for resources including Amazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon DynamoDB tables and indexes, and Amazon Aurora Replicas.When scaling EC2 instances, ensure that you use multiple Availability Zones (preferably at least three) and add or remove capacity to maintain balance across these Availability Zones. ECS tasks or Kubernetes pods (when using Amazon Elastic Kubernetes Service) should also be distributed across multiple Availability Zones.When using AWS Lambda, instances scale automatically. Every time an event notification is received for your function, AWS Lambda quickly locates free capacity within its compute fleet and runs your code up to the allocated concurrency. You need to ensure that the necessary concurrency is configured on the specific Lambda, and in your Service Quotas.Amazon S3 automatically scales to handle high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.Configure and use Amazon CloudFront or a trusted content delivery network (CDN). A CDN can provide faster end-user response times and can serve requests for content from cache, therefore reducing the need to scale your workload.Common anti-patterns:Implementing Auto Scaling groups for automated healing, but not implementing elasticity.Using automatic scaling to respond to large increases in traffic.Deploying highly stateful applications, eliminating the option of elasticity.Benefits of establishing this best practice: Automation removes the potential for manual error in deploying and decommissioning resources. Automation removes the risk of cost overruns and denial of service due to slow response on needs for deployment or decommissioning.Level of risk exposed if this best practice is not established: HighConfigure and use AWS Auto Scaling. This monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, you can setup application scaling for multiple resources across multiple services.What is AWS Auto Scaling?Configure Auto Scaling on your Amazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon DynamoDB tables and indexes, Amazon Aurora Replicas, and AWS Marketplace appliances as applicable.Managing throughput capacity automatically with DynamoDB Auto ScalingUse service API operations to specify the alarms, scaling policies, warm up times, and cool down times.Use Elastic Load Balancing. Load balancers can distribute load by path or by network connectivity.What is Elastic Load Balancing?Application Load Balancers can distribute load by path.What is an Application Load Balancer?Configure an Application Load Balancer to distribute traffic to different workloads based on the path under the domain name.Application Load Balancers can be used to distribute loads in a manner that integrates with AWS Auto Scaling to manage demand.Using a load balancer with an Auto Scaling groupNetwork Load Balancers can distribute load by connection.What is a Network Load Balancer?Configure a Network Load Balancer to distribute traffic to different workloads using TCP, or to have a constant set of IP addresses for your workload.Network Load Balancers can be used to distribute loads in a manner that integrates with AWS Auto Scaling to manage demand.Use a highly available DNS provider. DNS names allow your users to enter names instead of IP addresses to access your workloads and distributes this information to a defined scope, usually globally for users of the workload.Use Amazon Route 53 or a trusted DNS provider.What is Amazon Route 53?Use Route 53 to manage your CloudFront distributions and load balancers.Determine the domains and subdomains you are going to manage.Create appropriate record sets using ALIAS or CNAME records.Working with recordsUse the AWS global network to optimize the path from your users to your applications. AWS Global Accelerator continually monitors the health of your application endpoints and redirects traffic to healthy endpoints in less than 30 seconds.AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances.What Is AWS Global Accelerator?Configure and use Amazon CloudFront or a trusted content delivery network (CDN). A content delivery network can provide faster end-user response times and can serve requests for content that may cause unnecessary scaling of your workloads.What is Amazon CloudFront?Configure Amazon CloudFront distributions for your workloads, or use a third-party CDN.You can limit access to your workloads so that they are only accessible from CloudFront by using the IP ranges for CloudFront in your endpoint security groups or access policies.Related documents:APN Partner: partners that can help you create automated compute solutionsAWS Auto Scaling: How Scaling Plans WorkAWS Marketplace: products that can be used with auto scalingManaging Throughput Capacity Automatically with DynamoDB Auto ScalingUsing a load balancer with an Auto Scaling groupWhat Is AWS Global Accelerator?What Is Amazon EC2 Auto Scaling?What is AWS Auto Scaling?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?What is a Network Load Balancer?What is an Application Load Balancer?Working with records\n\nKnowledge Area: Scalability capabilities with appropriate use cases (for example, Amazon EC2 Auto Scaling, AWS Auto Scaling)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service should be used by a telecommunications company to host and manage their network infrastructure in order to automate the process of scaling workloads?\",\"answer_choices\": [{\"answer\": \"Amazon S3\",\"is_correct\": \"False\",\"explanation\": \"Amazon S3 is used for object storage, not for hosting and managing network infrastructure.\"},{\"answer\": \"AWS Auto Scaling\",\"is_correct\": \"True\",\"explanation\": \"AWS Auto Scaling is a managed service that can be used to automate the process of scaling workloads.\"},{\"answer\": \"Amazon DynamoDB\",\"is_correct\": \"False\",\"explanation\": \"Amazon DynamoDB is a NoSQL database, not a service for hosting and managing network infrastructure.\"},{\"answer\": \"AWS Lambda\",\"is_correct\": \"False\",\"explanation\": \"AWS Lambda is a serverless compute service, not a service for hosting and managing network infrastructure.\"}]}", "docs": ["('REL07-BP01 Use automation when obtaining or scaling resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_adapt_to_changes_autoscale_adapt.html')"]}, {"scenario": "A social media platform wants to use AWS to store and process user-generated content.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Knowledge of", "item": "Serverless technologies and patterns (for example, Lambda, Fargate)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA social media platform wants to use AWS to store and process user-generated content.`\"\n\nContext:\n\n* There are five design principles for performance efficiency in the cloud:Democratize advanced technologies: Make advanced technology implementation easier for your team by delegating complex tasks to your cloud vendor. Rather than asking your IT team to learn about hosting and running a new technology, consider consuming the technology as a service. For example, NoSQL databases, media transcoding, and machine learning are all technologies that require specialized expertise. In the cloud, these technologies become services that your team can consume, allowing your team to focus on product development rather than resource provisioning and management.Go global in minutes: Deploying your workload in multiple AWS Regions around the world allows you to provide lower latency and a better experience for your customers at minimal cost.Use serverless architectures: Serverless architectures remove the need for you to run and maintain physical servers for traditional compute activities. For example, serverless storage services can act as static websites (removing the need for web servers) and event services can host code. This removes the operational burden of managing physical servers, and can lower transactional costs because managed services operate at cloud scale.Experiment more often: With virtual and automatable resources, you can quickly carry out comparative testing using different types of instances, storage, or configurations.Consider mechanical sympathy: Understand how cloud services are consumed and always use the technology approach that aligns best with your workload goals. For example, consider data access patterns when you select database or storage approaches.\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n* Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Update deployed components as your needs change.Level of risk exposed if this best practice is not established: MediumEnable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.Scale using small increments for variable workloads.Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years.Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.Related documents:AWS Compute Optimizer DocumentationOperating Lambda: Performance optimizationAuto Scaling Documentation\n* The workload has a recovery time objective (RTO) and recovery point objective (RPO).\n* Selecting compute resources that meet your requirements, performance needs, and provide great efficiency of cost and effort will enable you to accomplish more with the same number of resources. When evaluating compute options, be aware of your requirements for workload performance and cost requirements and use this to make informed decisions.In AWS, compute is available in three forms: instances, containers, and functions:Instances are virtualized servers, allowing you to change their capabilities with a button or an API call. Because resource decisions in the cloud aren\u2019t fixed, you can experiment with different server types. At AWS, these virtual server instances come in different families and sizes, and they offer a wide variety of capabilities, including solid-state drives (SSDs) and graphics processing units (GPUs).Containers are a method of operating system virtualization that allow you to run an application and its dependencies in resource-isolated processes. AWS Fargate is serverless compute for containers or Amazon EC2 can be used if you need control over the installation, configuration, and management of your compute environment. You can also choose from multiple container orchestration platforms: Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS).Functions abstract the execution environment from the code you want to execute. For example, AWS Lambda allows you to execute code without running an instance.The following question focuses on these considerations for performance efficiency.When architecting your use of compute you should take advantage of the elasticity mechanisms available to ensure you have sufficient capacity to sustain performance as demand changes.PERF 2: How do you select your compute solution?The optimal compute solution for a workload varies based on application design, usage patterns, and configuration settings. Architectures can use different compute solutions for various components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency.\n\nKnowledge Area: Serverless technologies and patterns (for example, Lambda, Fargate)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When architecting your use of compute you should take advantage of the elasticity mechanisms available to ensure you have sufficient capacity to sustain performance as demand changes. PERF 2: How do you select your compute solution?\",\"answer_choices\": [{\"answer\": \"By evaluating factors such as resource requirements, performance needs, and cost requirements\",\"is_correct\": \"True\",\"explanation\": \"When evaluating compute options, be aware of your requirements for workload performance and cost requirements and use this to make informed decisions.In AWS, compute is available in three forms: instances, containers, and functions:Instances are virtualized servers, allowing you to change their capabilities with a button or an API call. Containers are a method of operating system virtualization that allow you to run an application and its dependencies in resource-isolated processes. AWS Fargate is serverless compute for containers or Amazon EC2 can be used if you need control over the installation, configuration, and management of your compute environment. You can also choose from multiple container orchestration platforms: Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS).Functions abstract the execution environment from the code you want to execute. For example, AWS Lambda allows you to execute code without running an instance.\"},{\"answer\": \"By using different types of instances with comparative testing\",\"is_correct\": \"False\",\"explanation\": \"Comparative testing using different types of instances, storage, or configurations is an important factor to consider when selecting all components; however, it is only one factor to evaluate when selecting a compute solution.\"},{\"answer\": \"By selecting the most efficient computing resource\",\"is_correct\": \"False\",\"explanation\": \"The most efficient computing resource will vary based on the application design, usage patterns, and configuration settings. The optimal compute solution for a workload is the one that best matches the organizational priorities.\"},{\"answer\": \"By using serverless, containers, and open source software\",\"is_correct\": \"False\",\"explanation\": \"Using serverless, containers, and open source software are important considerations when selecting a compute solution; however, they are not the only factors that should be taken into account. It is important to evaluate factors such as resource requirements, performance needs, and cost requirements to make an informed decision.\"}]}", "docs": ["('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-dp.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')", "('SUS05-BP01 Use the minimum amount of hardware to meet your needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a2.html')", "('REL13-BP01 Define recovery objectives for downtime and data loss - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_for_recovery_objective_defined_recovery.html')", "('Compute - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-compute.html')"]}, {"scenario": "A marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Knowledge of", "item": "The orchestration of containers (for example, Amazon ECS, Amazon EKS)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.`\"\n\nContext:\n\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n\nKnowledge Area: The orchestration of containers (for example, Amazon ECS, Amazon EKS)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service should be used to automate the creation, management, and scaling of the customer data processing pipeline?\", \n\"answer_choices\": [\n{\"answer\": \"Amazon RDS\", \"is_correct\": \"false\", \"explanation\": \"Amazon RDS is a relational database service that can be used to set up, operate and scale a relational database. It is not used to automate the creation, management, and scaling of a data processing pipeline.\"},\n{\"answer\": \"AWS Lambda\", \"is_correct\": \"false\", \"explanation\": \"AWS Lambda can be used to run code without provisioning or managing servers. It is not used to automate the creation, management, and scaling of a data processing pipeline.\"}, \n{\"answer\": \"Amazon ECS\",\"is_correct\": \"true\", \"explanation\": \"Amazon ECS is an Elastic Container Service used to automate the creation, management, and scaling of the customer data processing pipeline.\"},\n{\"answer\": \"Amazon EKS\", \"is_correct\": \"false\", \"explanation\": \"Amazon EKS is an Elastic Kubernetes Service used to orchestrate containers such as Docker. It is not used to automate the creation, management, and scaling of a data processing pipeline.\"}\n]\n}", "docs": ["('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')"]}, {"scenario": "A software development team wants to build and deploy a microservices-based application on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Skills in", "item": "Decoupling workloads so that components can scale independently", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA software development team wants to build and deploy a microservices-based application on AWS.`\"\n\nContext:\n\n* Workload segmentation is important when determining the resilience requirements of your application. Monolithic architecture should be avoided whenever possible. Instead, carefully consider which application components can be broken out into microservices. Depending on your application requirements, this may end up being a combination of a service-oriented architecture (SOA) with microservices where possible. Workloads that are capable of statelessness are more capable of being deployed as microservices.Desired outcome: Workloads should be supportable, scalable, and as loosely coupled as possible.When making choices about how to segment your workload, balance the benefits against the complexities. What is right for a new product racing to first launch is different than what a workload built to scale from the start needs. When refactoring an existing monolith, you will need to consider how well the application will support a decomposition towards statelessness. Breaking services into smaller pieces allows small, well-defined teams to develop and manage them. However, smaller services can introduce complexities which include possible increased latency, more complex debugging, and increased operational burden.Common anti-patterns:The microservice Death Star is a situation in which the atomic components become so highly interdependent that a failure of one results in a much larger failure, making the components as rigid and fragile as a monolith.Benefits of establishing this practice:More specific segments lead to greater agility, organizational flexibility, and scalability.Reduced impact of service interruptions.Application components may have different availability requirements, which can be supported by a more atomic segmentation.Well-defined responsibilities for teams supporting the workload.Level of risk exposed if this best practice is not established: HighChoose your architecture type based on how you will segment your workload. Choose an SOA or microservices architecture (or in some rare cases, a monolithic architecture). Even if you choose to start with a monolith architecture, you must ensure that it\u2019s modular and can ultimately evolve to SOA or microservices as your product scales with user adoption. SOA and microservices offer respectively smaller segmentation, which is preferred as a modern scalable and reliable architecture, but there are trade-offs to consider, especially when deploying a microservice architecture.One primary trade-off is that you now have a distributed compute architecture that can make it harder to achieve user latency requirements and there is additional complexity in the debugging and tracing of user interactions. You can use AWS X-Ray to assist you in solving this problem. Another effect to consider is increased operational complexity as you increase the number of applications that you are managing, which requires the deployment of multiple independency components.Monolithic, service-oriented, and microservices architecturesDetermine the appropriate architecture to refactor or build your application. SOA and microservices offer respectively smaller segmentation, which is preferred as a modern scalable and reliable architecture. SOA can be a good compromise for achieving smaller segmentation while avoiding some of the complexities of microservices. For more details, see Microservice Trade-Offs.If your workload is amenable to it, and your organization can support it, you should use a microservices architecture to achieve the best agility and reliability. For more details, see Implementing Microservices on AWS.Consider following the Strangler Fig pattern to refactor a monolith into smaller components. This involves gradually replacing specific application components with new applications and services. AWS Migration Hub Refactor Spaces acts as the starting point for incremental refactoring. For more details, see Seamlessly migrate on-premises legacy workloads using a strangler pattern.Implementing microservices may require a service discovery mechanism to allow these distributed services to communicate with each other. AWS App Mesh can be used with service-oriented architectures to provide reliable discovery and access of services. AWS Cloud Map can also be used for dynamic, DNS-based service discovery.If you\u2019re migrating from a monolith to SOA, Amazon MQ can help bridge the gap as a service bus when redesigning legacy applications in the cloud.For existing monoliths with a single, shared database, choose how to reorganize the data into smaller segments. This could be by business unit, access pattern, or data structure. At this point in the refactoring process, you should choose to move forward with a relational or non-relational (NoSQL) type of database. For more details, see From SQL to NoSQL.Level of effort for the implementation plan: HighRelated best practices:REL03-BP02 Build services focused on specific business domains and functionalityRelated documents:Amazon API Gateway: Configuring a REST API Using OpenAPIWhat is Service-Oriented Architecture?Bounded Context (a central pattern in Domain-Driven Design)Implementing Microservices on AWSMicroservice Trade-OffsMicroservices - a definition of this new architectural termMicroservices on AWSWhat is AWS App Mesh?Related examples:Iterative App Modernization WorkshopRelated videos:Delivering Excellence with Microservices on AWS\n* A scalable workload provides elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time.Best practices\n* Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Update deployed components as your needs change.Level of risk exposed if this best practice is not established: MediumEnable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.Scale using small increments for variable workloads.Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years.Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.Related documents:AWS Compute Optimizer DocumentationOperating Lambda: Performance optimizationAuto Scaling Documentation\n\nKnowledge Area: Decoupling workloads so that components can scale independently\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the most appropriate decoupling method for a software development team building a microservices-based application on AWS?\", \n\"answer_choices\": [\n{\"answer\": \"Use Amazon MQ to refactor a monolith into smaller components\",\"is_correct\": \"false\", \"explanation\": \"Amazon MQ is a service bus that can be used when redesigning legacy applications in the cloud, but it is not an appropriate decoupling method for a microservices-based application\"},\n{\"answer\": \"Use AWS X-Ray for debugging and tracing user interactions\",\"is_correct\": \"false\", \"explanation\": \"AWS X-Ray is a debugging and tracing tool, but it is not an appropriate decoupling method for a microservices-based application\"},\n{\"answer\": \"Use AWS App Mesh for service discovery,\",\"is_correct\": \"true\", \"explanation\": \"AWS App Mesh can be used with service-oriented architectures to provide reliable discovery and access of services and it is an appropriate decoupling method for a microservices-based application\"},\n{\"answer\": \"Use AWS Cloud Map for dynamic DNS-based service discovery\",\"is_correct\": \"false\", \"explanation\": \"AWS Cloud Map can be used for dynamic, DNS-based service discovery, but it is not an appropriate decoupling method for a microservices-based application\"}\n]}", "docs": ["('REL03-BP01 Choose how to segment your workload - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_service_architecture_monolith_soa_microservice.html')", "('REL 7\\xa0 How do you design your workload to adapt to changes in demand? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-07.html')", "('SUS05-BP01 Use the minimum amount of hardware to meet your needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a2.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Skills in", "item": "Identifying metrics and conditions to perform scaling actions", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* Identify periods of low or no utilization and scale down resources to eliminate excess capacity and improve efficiency.Common anti-patterns:You do not scale your infrastructure with user load.You manually scale your infrastructure all the time.You leave increased capacity after a scaling event instead of scaling back down.Benefits of establishing this best practice: Configuring and testing workload elasticity will help reduce workload environmental impact, save money, and maintain performance benchmarks. You can take advantage of elasticity in the cloud to automatically scale capacity during and after user load spikes to make sure you are only using the exact number of resources needed to meet the needs of your customers.Level of risk exposed if this best practice is not established: MediumElasticity matches the supply of resources you have against the demand for those resources. Instances, containers, and functions provide mechanisms for elasticity, either in combination with automatic scaling or as a feature of the service. Use elasticity in your architecture to ensure that workload can scale down quickly and easily during the period of low user load:Amazon EC2 Auto ScalingUse to verify you have the correct number of Amazon EC2 instances available to handle the user load for your application.Application Auto ScalingUse to automatically scale the resources for individual AWS services beyond Amazon EC2, such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.Kubernetes Cluster AutoscalerUse to automatically scale Kubernetes clusters on AWS.Verify that the metrics for scaling up or down are validated against the type of workload being deployed. If you are deploying a video transcoding application, 100% CPU utilization is expected and should not be your primary metric. You can use a customized metric (such as memory utilization) for your scaling policy if required. To choose the right metrics, consider the following guidance for Amazon EC2:The metric should be a valid utilization metric and describe how busy an instance is.The metric value must increase or decrease proportionally to the number of instances in the Auto Scaling group.Use dynamic scaling instead of manual scaling for your Auto Scaling group. We also recommend that you use target tracking scaling policies in your dynamic scaling.Verify that workload deployments can handle both scale-up and scale-down events. Create test scenarios for scale-down events to ensure that the workload behaves as expected. You can use Activity history to test and verify a scaling activity for an Auto Scaling group.Evaluate your workload for predictable patterns and proactively scale as you anticipate predicted and planned changes in demand. Use Predictive Scaling with Amazon EC2 Auto Scaling to eliminate the need to overprove capacity.Related documents:Getting Started with Amazon EC2 Auto ScalingPredictive Scaling for EC2, Powered by Machine LearningAnalyze user behavior using Amazon OpenSearch Service, Amazon Kinesis Data Firehose and KibanaWhat is Amazon CloudWatch?What is AWS X-Ray?VPC Flow LogsMonitoring DB load with Performance Insights on Amazon RDSIntroducing Native Support for Predictive Scaling with Amazon EC2 Auto ScalingHow to create an Amazon EC2 Auto Scaling policy based on a memory utilization metric (Linux)Introducing Karpenter - An Open-Source, High-Performance Kubernetes Cluster AutoscalerRelated videos:Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Related examples:Lab: Amazon EC2 Auto Scaling Group ExamplesLab: Implement Autoscaling with KarpenterAuto-scaling mechanismWhere to useAmazon EC2 Auto ScalingUse to verify you have the correct number of Amazon EC2 instances available to handle the user load for your application.Application Auto ScalingUse to automatically scale the resources for individual AWS services beyond Amazon EC2, such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.Kubernetes Cluster AutoscalerUse to automatically scale Kubernetes clusters on AWS.\n* Use metrics from the currently running workload to select the right size and type to optimize for cost. Appropriately provision throughput, sizing, and storage for services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Elastic Block Store (Amazon EBS) (PIOPS), Amazon Relational Database Service (Amazon RDS), Amazon EMR, and networking. This can be done with a feedback loop such as automatic scaling or by custom code in the workload.Level of risk exposed if this best practice is not established: LowCreate a feedback loop within the workload that uses active metrics from the running workload to make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you configure to perform the right sizing operations for you. AWS also provides APIs, SDKs, and features that allow resources to be modified with minimal effort. You can program a workload to stop-and-start an Amazon Elastic Compute Cloud(Amazon EC2) instance to allow a change of instance size or instance type. This provides the benefits of right-sizing while removing almost all the operational cost required to make the change.Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage Service(Amazon S3) Intelligent-Tiering. Amazon S3 Intelligent-Tiering automatically moves your data between two access tiers: frequent access and infrequent access, based on your usage patterns.Implementation stepsConfigure workload metrics: Ensure you capture the key metrics for the workload. These metrics provide an indication of the customer experience, such as the workload output, and align to the differences between resource types and sizes, such as CPU and memory usage.View rightsizing recommendations: Use the rightsizing recommendations in AWS Compute Optimizer to make adjustments to your workload.Select resource type and size automatically based on metrics: Using the workload metrics, manually or automatically select your workload resources. Configuring AWS Auto Scaling or implementing code within your application can reduce the effort required if frequent changes are needed, and it can potentially implement changes sooner than a manual process.Related documents:AWS Auto ScalingAWS Compute OptimizerAmazon CloudWatch featuresCloudWatch Getting Set UpCloudWatch Publishing Custom MetricsCost Optimization: Amazon EC2 Right SizingGetting Started with Amazon EC2 Auto ScalingAmazon S3 Intelligent-TieringLaunch an EC2 Instance Using the SDK\n\nKnowledge Area: Identifying metrics and conditions to perform scaling actions\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"For a media company wanting to store large amounts of data to be used for their applications, which of the following is the best practice to ensure they are only using the exact number of resources needed to meet customer demand?\",\"answer_choices\": [{\"answer\": \"Use AWS CloudWatch to monitor the load on the applications\",\"is_correct\": \"false\",\"explanation\": \"CloudWatch is used for monitoring, not for scaling.\"},{\"answer\": \"Use Amazon EC2 Auto Scaling to verify you have the correct number of Amazon EC2 instances available to handle the user load for your application\",\"is_correct\": \"true\",\"explanation\": \"EC2 auto scaling automatically provisions and removes instances to ensure you have the correct number of instances to handle customer demand.\"},{\"answer\": \"Use Amazon S3 Intelligent-Tiering to automatically move data between two access tiers\",\"is_correct\": \"false\",\"explanation\": \"S3 Intelligent-Tiering moves data between two access tiers, not instances.\"},{\"answer\": \"Use AWS X-Ray to monitor the load on the applications\",\"is_correct\": \"false\",\"explanation\": \"X-Ray is used for monitoring and debugging, not for scaling.\"}]}", "docs": ["('SUS02-BP01 Scale infrastructure with user load - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_user_a2.html')", "('COST06-BP03 Select resource type, size, and number automatically based on metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_metrics.html')"]}, {"scenario": "A telecommunications company wants to use AWS to host and manage their network infrastructure.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Skills in", "item": "Selecting the appropriate compute options and features (for example, EC2 instance types) to meet business requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA telecommunications company wants to use AWS to host and manage their network infrastructure.`\"\n\nContext:\n\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n\nKnowledge Area: Selecting the appropriate compute options and features (for example, EC2 instance types) to meet business requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best option for a telecommunications company that wants to use AWS to host and manage their network infrastructure?\",\n\"answer_choices\": [{\"answer\": \"The largest instance available for all workloads\",\"is_correct\": \"false\",\"explanation\": \"This is an anti-pattern and will not optimize performance or efficiency.\"},{\"answer\": \"Standardizing all instance types to one type for ease of management\",\"is_correct\": \"false\",\"explanation\": \"This is an anti-pattern and will not optimize performance or efficiency.\"},{\"answer\": \"Determining which resources the workload needs and choosing memory-optimized instances for memory-intensive workloads\",\"is_correct\": \"true\",\"explanation\": \"Right sizing enables the workload to perform as well as possible while only using the required resources.\"},{\"answer\": \"Choosing compute-optimized instances for components that do data processing\",\"is_correct\": \"true\",\"explanation\": \"Compute-optimized instances are best for components that do data processing that is not memory-intensive.\"}]}", "docs": ["('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 2: Design high-performing and elastic compute solutions", "focus": "Skills in", "item": "Selecting the appropriate resource type and size (for example, the amount of Lambda memory) to meet business requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n* Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number.Best practices\n* Use metrics from the currently running workload to select the right size and type to optimize for cost. Appropriately provision throughput, sizing, and storage for services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Elastic Block Store (Amazon EBS) (PIOPS), Amazon Relational Database Service (Amazon RDS), Amazon EMR, and networking. This can be done with a feedback loop such as automatic scaling or by custom code in the workload.Level of risk exposed if this best practice is not established: LowCreate a feedback loop within the workload that uses active metrics from the running workload to make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you configure to perform the right sizing operations for you. AWS also provides APIs, SDKs, and features that allow resources to be modified with minimal effort. You can program a workload to stop-and-start an Amazon Elastic Compute Cloud(Amazon EC2) instance to allow a change of instance size or instance type. This provides the benefits of right-sizing while removing almost all the operational cost required to make the change.Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage Service(Amazon S3) Intelligent-Tiering. Amazon S3 Intelligent-Tiering automatically moves your data between two access tiers: frequent access and infrequent access, based on your usage patterns.Implementation stepsConfigure workload metrics: Ensure you capture the key metrics for the workload. These metrics provide an indication of the customer experience, such as the workload output, and align to the differences between resource types and sizes, such as CPU and memory usage.View rightsizing recommendations: Use the rightsizing recommendations in AWS Compute Optimizer to make adjustments to your workload.Select resource type and size automatically based on metrics: Using the workload metrics, manually or automatically select your workload resources. Configuring AWS Auto Scaling or implementing code within your application can reduce the effort required if frequent changes are needed, and it can potentially implement changes sooner than a manual process.Related documents:AWS Auto ScalingAWS Compute OptimizerAmazon CloudWatch featuresCloudWatch Getting Set UpCloudWatch Publishing Custom MetricsCost Optimization: Amazon EC2 Right SizingGetting Started with Amazon EC2 Auto ScalingAmazon S3 Intelligent-TieringLaunch an EC2 Instance Using the SDK\n\nKnowledge Area: Selecting the appropriate resource type and size (for example, the amount of Lambda memory) to meet business requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"For the gaming company's multiplayer game servers, what is the best practice for selecting the appropriate resource type and size to meet business requirements?\",\n\"answer_choices\": [{\"answer\": \"Choose the largest instance available for all workloads.\",\"is_correct\": \"false\",\"explanation\": \"This would be an inefficient use of resources and not optimize the workload for cost.\"},{\"answer\": \"Standardize all instances types to one type for ease of management.\",\"is_correct\": \"false\",\"explanation\": \"This would not optimize the workload for cost and could lead to unforeseen performance issues.\"},{\"answer\": \"Use metrics from the running workload to select the right size and type to optimize for cost.\",\"is_correct\": \"true\",\"explanation\": \"This enables the workload to perform as well as possible while only using the required resources.\"},{\"answer\": \"Configure a feedback loop within the workload to make changes to the workload based on active metrics.\",\"is_correct\": \"true\",\"explanation\": \"This provides the benefits of right-sizing while removing almost all the operational cost required to make the change.\"}]}", "docs": ["('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')", "('COST 6\\xa0 How do you meet cost targets when you select resource type, size and number? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-06.html')", "('COST06-BP03 Select resource type, size, and number automatically based on metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_metrics.html')"]}, {"scenario": "A startup wants to host their new web application on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Knowledge of", "item": "AWS global infrastructure (for example, Availability Zones, AWS Regions)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA startup wants to host their new web application on AWS.`\"\n\nContext:\n\n* Use the cloud location options available to reduce network latency or improve throughput. Use AWS Regions, Availability Zones, placement groups, and edge locations such as AWS Outposts, AWS Local Zones, and AWS Wavelength, to reduce network latency or improve throughput.The AWS Cloud infrastructure is built around Regions and Availability Zones. A Region is a physical location in the world having multiple Availability Zones.Availability Zones consist of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. These Availability Zones offer you the ability to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data centerChoose the appropriate Region or Regions for your deployment based on the following key elements:Where your users are located: Choosing a Region close to your workload\u2019s users ensures lower latency when they use the workload.Where your data is located: For data-heavy applications, the major bottleneck in latency is data transfer. Application code should execute as close to the data as possible.Other constraints: Consider constraints such as security and compliance.Amazon EC2 provides placement groups for networking. A placement group is a logical grouping of instances to decrease latency or increase reliability. Using placement groups with supported instance types and an Elastic Network Adapter (ENA) enables workloads to participate in a low-latency, 25 Gbps network. Placement groups are recommended for workloads that benefit from low network latency, high network throughput, or both. Using placement groups has the benefit of lowering jitter in network communications.Latency-sensitive services are delivered at the edge using a global network of edge locations. These edge locations commonly provide services such as content delivery network (CDN) and domain name system (DNS). By having these services at the edge, workloads can respond with low latency to requests for content or DNS resolution. These services also provide geographic services such as geo targeting of content (providing different content based on the end users\u2019 location), or latency-based routing to direct end users to the nearest Region (minimum latency).Amazon CloudFront is a global CDN that can be used to accelerate both static content such as images, scripts, and videos, as well as dynamic content such as APIs or web applications. It relies on a global network of edge locations that will cache the content and provide high-performance network connectivity to your users. CloudFront also accelerates many other features such as content uploading and dynamic applications, making it a performance addition to all applications serving traffic over the internet. Lambda@Edge is a feature of Amazon CloudFront that will let you run code closer to users of your workload, which improves performance and reduces latency.Amazon Route 53 is a highly available and scalable cloud DNS web service. It\u2019s designed to give developers and businesses an extremely reliable and cost-effective way to route end users to internet applications by translating names, like www.example.com, into numeric IP addresses, like 192.168.2.1, that computers use to connect to each other. Route 53 is fully compliant with IPv6.AWS Outposts is designed for workloads that need to remain on-premises due to latency requirements, where you want that workload to run seamlessly with the rest of your other workloads in AWS. AWS Outposts are fully managed and configurable compute and storage racks built with AWS-designed hardware that allow you to run compute and storage on-premises, while seamlessly connecting to the broad array of AWS services in in the cloud.AWS Local Zones is designed to run workloads that require single-digit millisecond latency, like video rendering and graphics intensive, virtual desktop applications. Local Zones allow you to gain all the benefits of having compute and storage resources closer to end-users.AWS Wavelength is designed to deliver ultra-low latency applications to 5G devices by extending AWS infrastructure, services, APIs, and tools to 5G networks. Wavelength embeds storage and compute inside telco providers 5G networks to help your 5G workload if it requires single-digit millisecond latency, such as IoT devices, game streaming, autonomous vehicles, and live media production.Use edge services to reduce latency and to enable content caching. Ensure that you have configured cache control correctly for both DNS and HTTP/HTTPS to gain the most benefit from these approaches.Common anti-patterns:You consolidate all workload resources into one geographic location.You chose the closest region to your location but not to the workload end user.Benefits of establishing this best practice: You must ensure that your network is available wherever you want to reach customers. Using the AWS private global network ensures that your customers get the lowest latency experience by deploying workloads into the locations nearest them.Level of risk exposed if this best practice is not established: MediumReduce latency by selecting the correct locations: Identify where your users and data are located. Take advantage of AWS Regions, Availability Zones, placement groups, and edge locations to reduce latency.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n\nKnowledge Area: AWS global infrastructure (for example, Availability Zones, AWS Regions)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS feature should an organization use to reduce network latency for a new web application hosted on AWS?\",\n\"answer_choices\": [{\"answer\": \"Use AWS Route 53 to translate names into numeric IP addresses\",\"is_correct\": \"False\",\"explanation\": \"Route 53 is a highly available and scalable cloud DNS web service, not a feature to reduce network latency\"},\n{\"answer\": \"Use AWS Regions and Availability Zones to deploy the application close to the users\",\"is_correct\": \"True\",\"explanation\": \"Regions and Availability Zones offer the ability to reduce network latency by deploying the application close to the users\"},\n{\"answer\": \"Use AWS Outposts to embed storage and compute inside telco providers' 5G networks\",\"is_correct\": \"False\",\"explanation\": \"AWS Outposts is designed to remain on-premises due to latency requirements, not inside telco providers' 5G networks\"},\n{\"answer\": \"Use Amazon CloudFront to accelerate content and dynamic applications\",\"is_correct\": \"False\",\"explanation\": \"CloudFront is a global CDN that can be used to accelerate content, but not to reduce network latency\"}]}", "docs": ["('PERF05-BP06 Choose your workload\u2019s location based on network requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_location.html')"]}, {"scenario": "A company wants to build a mobile application that needs to access and store data in the cloud.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Knowledge of", "item": "Caching strategies and services (for example, Amazon ElastiCache)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to build a mobile application that needs to access and store data in the cloud.`\"\n\nContext:\n\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS), AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate, and scale a relational database, automates administration tasks such as hardware provisioning, database setup, patching, and backups. This means you have more free time to focus on securing your application in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security at the code level\u2013not the infrastructure or operating system.Level of risk exposed if this best practice is not established: MediumExplore available services: Explore, test, and implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS.Related documents:AWS WebsiteAWS Systems ManagerReplacing a Bastion Host with Amazon EC2 Systems ManagerSecurity Overview of AWS LambdaRelated videos:Running high-security workloads on Amazon EKSSecuring Serverless and Container ServicesSecurity best practices for the Amazon EC2 instance metadata serviceRelated examples:Lab: AWS Certificate Manager Request Public Certificate\n\nKnowledge Area: Caching strategies and services (for example, Amazon ElastiCache)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best caching strategy to use for the mobile application that needs to access and store data in the cloud?\", \n\"answer_choices\": [{\"answer\": \"Host the application on Amazon EC2 and store data on Amazon S3\",\"is_correct\": \"false\",\"explanation\": \"This would not provide any caching to improve performance of the application\"}, \n{\"answer\": \"Implement Amazon ElastiCache to cache frequently accessed data\",\"is_correct\": \"true\",\"explanation\": \"Amazon ElastiCache is a managed caching service which could be used to improve performance of the application by caching frequently accessed data\"}, \n{\"answer\": \"Implement Amazon DynamoDB to store data and cache it on the application's servers\",\"is_correct\": \"false\",\"explanation\": \"This would not provide any caching to improve performance of the application\"},\n{\"answer\": \"Host the application on Amazon ECS and store data on Amazon EBS\",\"is_correct\": \"false\",\"explanation\": \"This would not provide any caching to improve performance of the application\"}]}", "docs": ["('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('SEC06-BP03 Implement managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_compute_implement_managed_services.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Knowledge of", "item": "Data access patterns (for example, read-intensive compared with write-intensive)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n* Use performance characteristics and access patterns that optimize how data is stored or queried to achieve the best possible performance. Measure how optimizations such as indexing, key distribution, data warehouse design, or caching strategies impact system performance or overall efficiency.Common anti-patterns:You only use manual log file searching for metrics.You only publish metrics to internal tools.Benefits of establishing this best practice: In order to ensure you are meeting the metrics required for the workload, you must monitor database performance metrics related to both reads and writes. You can use this data to add new optimizations for both reads and writes to the data storage layer.Level of risk exposed if this best practice is not established: LowOptimize data storage based on metrics and patterns: Use reported metrics to identify any underperforming areas in your workload and optimize your database components. Each database system has different performance related characteristics to evaluate, such as how data is indexed, cached, or distributed among multiple systems. Measure the impact of your optimizations.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAnalyzing performance anomalies with DevOps Guru for RDSRead/Write Capacity Mode for DynamoDBRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Hands-on Labs for Amazon DynamoDB\n\nKnowledge Area: Data access patterns (for example, read-intensive compared with write-intensive)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best data access pattern for the financial services company's highly available and secure infrastructure to process financial transactions?\", \n\"answer_choices\": [\n{\"answer\": \"Read-intensive\", \"is_correct\": \"True\", \"explanation\": \"A read-intensive pattern is well suited for a financial services company's highly available and secure infrastructure to process financial transactions, as it allows for fast retrieval of data from the database.\"},\n{\"answer\": \"Write-intensive\", \"is_correct\": \"False\", \"explanation\": \"A write-intensive pattern is not well suited for a financial services company's highly available and secure infrastructure to process financial transactions, as it is more resource-intensive and not optimal for quickly retrieving data from the database.\"},\n{\"answer\": \"Eventual consistency\", \"is_correct\": \"False\", \"explanation\": \"Eventual consistency is not well suited for a financial services company's highly available and secure infrastructure to process financial transactions, as it ensures that all replicas eventually reach a consistent state without requiring transactions or locking.\"},\n{\"answer\": \"ACID compliance\", \"is_correct\": \"False\", \"explanation\": \"ACID compliance is not well suited for a financial services company's highly available and secure infrastructure to process financial transactions, as it is an atomicity, consistency, isolation, and durability compliance model that ensures data integrity and consistency across multiple operations on a database.\"}\n]}", "docs": ["('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')", "('PERF04-BP05 Optimize data storage based on access patterns and metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_optimize_metrics.html')"]}, {"scenario": "A startup wants to host their new web application on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Knowledge of", "item": "Database capacity planning (for example, capacity units, instance types, Provisioned IOPS)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA startup wants to host their new web application on AWS.`\"\n\nContext:\n\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n\nKnowledge Area: Database capacity planning (for example, capacity units, instance types, Provisioned IOPS)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"Which of the following is a best practice when capacity planning for a database on AWS?\",\n \"answer_choices\": [{\"answer\":\"Choose the largest instance available for all workloads.\",\"is_correct\":\"false\",\"explanation\":\"This is a common anti-pattern and can lead to inefficient resource utilization.\"},\n{\"answer\":\"Identify and document the anticipated growth of the data and traffic.\",\"is_correct\":\"true\",\"explanation\":\"Identifying and documenting the anticipated growth of the data and traffic is an important best practice for capacity planning for a database on AWS.\"},\n{\"answer\":\"Standardize all instance types to one type for ease of management.\",\"is_correct\":\"false\",\"explanation\":\"This is a common anti-pattern and can lead to inefficient resource utilization.\"},\n{\"answer\":\"Use the access patterns of the workload to decide which services and technologies to use.\",\"is_correct\":\"false\",\"explanation\":\"This is an important part of the capacity planning process, but it is not a best practice.\"}]}", "docs": ["('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')", "('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')"]}, {"scenario": "A software development team wants to build and deploy a microservices-based application on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Knowledge of", "item": "Database connections and proxies", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA software development team wants to build and deploy a microservices-based application on AWS.`\"\n\nContext:\n\n* For high availability, always (when possible) deploy your workload components to multiple Availability Zones (AZs), as shown in Figure 10. For workloads with extreme resilience requirements, carefully evaluate the options for a multi-Region architecture.\n* Implement services to reduce data transfer. For example, using a content delivery network (CDN) such as Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using AWS Direct Connect instead of VPN for connectivity to AWS.Level of risk exposed if this best practice is not established: LowAmazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources. By using CloudFront, you can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network. It is ideal for small offices or business partners because it provides quick and easy connectivity, and it is a fully managed and elastic service.VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs. Gateway VPC endpoints have no hourly charges, and support Amazon Simple Storage Service(Amazon S3) and Amazon DynamoDB. Interface VPC endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost.Implementation stepsImplement services: Using the data transfer modeling, look at where the largest costs and highest volume flows are. Review the AWS services and assess whether there is a service that reduces or removes the transfer, specifically networking and content delivery. Also look for caching services where there is repeated access to data, or large amounts of data.Related documents:AWS Direct ConnectAWS Explore Our ProductsAWS caching solutionsAmazon CloudFrontDeliver content faster with Amazon CloudFront\n\nKnowledge Area: Database connections and proxies\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service should be used to establish a secure and private connection between the software development team's private network and the AWS global network?\", \n\"answer_choices\": [\n    {\"answer\": \"Amazon CloudFront\", \"is_correct\": \"false\", \"explanation\": \"Amazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds.\"}, \n    {\"answer\": \"AWS Direct Connect\", \"is_correct\": \"false\", \"explanation\": \"AWS Direct Connect allows you to establish a dedicated network connection to AWS.\"},\n    {\"answer\": \"VPC Endpoints\", \"is_correct\": \"false\", \"explanation\": \"VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs.\"},\n    {\"answer\": \"AWS VPN\", \"is_correct\": \"true\", \"explanation\": \"AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network.\"}\n]\n}", "docs": ["('REL10-BP02 Select the appropriate locations for your multi-location deployment - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_fault_isolation_select_location.html')", "('COST08-BP03 Implement services to reduce data transfer costs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_implement_services.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Knowledge of", "item": "Database engines with appropriate use cases (for example, heterogeneous migrations, homogeneous migrations)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* The cloud offers purpose-built database services that address different problems presented by your workload. You can choose from many purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. By picking the best database to solve a specific problem (or a group of problems), you can break away from restrictive one-size-fits-all monolithic databases and focus on building applications to meet the performance needs of your customers.In AWS you can choose from multiple purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. With AWS databases, you don\u2019t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running with self-healing storage and automated scaling, so that you can focus on higher value application development.The following question focuses on these considerations for performance efficiency.Your workload's database approach has a significant impact on performance efficiency. It's often an area that is chosen according to organizational defaults rather than through a data-driven approach. As with storage, it is critical to consider the access patterns of your workload, and also to consider if other non-database solutions could solve the problem more efficiently (such as using graph, time series, or in-memory storage database).PERF 4: How do you select your database solution?The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n* Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.Best practices\n\nKnowledge Area: Database engines with appropriate use cases (for example, heterogeneous migrations, homogeneous migrations)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"For the manufacturing company's supply chain and production processes, which database engine is the most appropriate to use?\", \"answer_choices\": [{\"answer\": \"Relational database engine\",\"is_correct\": \"true\",\"explanation\": \"Relational databases provide a good option for applications that require transactions and lookup of data by specific attributes.\"},{\"answer\": \"Key-value database engine\",\"is_correct\": \"false\",\"explanation\": \"Key-value databases are designed for applications that require quick lookup of data, but not transactions or complex lookups.\"},{\"answer\": \"Graph database engine\",\"is_correct\": \"false\",\"explanation\": \"Graph databases are best used for applications that require complex data relationships, and may not be the best option for the manufacturing company's supply chain and production processes.\"},{\"answer\": \"In-memory database engine\",\"is_correct\": \"false\",\"explanation\": \"In-memory databases offer high performance, but are best used for applications that require real-time lookups of data, and may not be the best option for the manufacturing company's supply chain and production processes.\"}]}", "docs": ["('Database - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-db.html')", "('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')", "('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')", "('PERF 1\\xa0 How do you select the best performing architecture? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-01.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Knowledge of", "item": "Database replication (for example, read replicas)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Understand the available database options and how it can optimize your performance before you select your data management solution. Use load testing to identify database metrics that matter for your workload. While you explore the database options, take into consideration various aspects such as the parameter groups, storage options, memory, compute, read replica, eventual consistency, connection pooling, and caching options. Experiment with these various configuration options to improve the metrics.\n\nKnowledge Area: Database replication (for example, read replicas)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which configuration option allows for improved scalability and high availability for the disaster recovery plan for the critical systems on AWS?\",\n\"answer_choices\": [{\"answer\": \"Parameter groups\",\"is_correct\": \"False\",\"explanation\": \"Parameter groups control the settings of a database instance, and are not related to scalability and high availability.\"},{\"answer\": \"Storage options\",\"is_correct\": \"False\",\"explanation\": \"Storage options determine how data is stored, and are not related to scalability and high availability.\"},{\"answer\": \"Read replica\",\"is_correct\": \"True\",\"explanation\": \"Read replicas are copies of the master database that can be used to spread the read operations across multiple nodes, and allows for improved scalability and high availability.\"},{\"answer\": \"Eventual consistency\",\"is_correct\": \"False\",\"explanation\": \"Eventual consistency is a property of data storage systems, and is not related to scalability and high availability.\"}]}", "docs": ["('PERF04-BP02 Evaluate the available options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_evaluate_options.html')"]}, {"scenario": "A company has a critical application that needs to be up at all times", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Knowledge of", "item": "Database types and services (for example, serverless, relational compared with non-relational, in-memory)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company has a critical application that needs to be up at all times`\"\n\nContext:\n\n* The cloud offers purpose-built database services that address different problems presented by your workload. You can choose from many purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. By picking the best database to solve a specific problem (or a group of problems), you can break away from restrictive one-size-fits-all monolithic databases and focus on building applications to meet the performance needs of your customers.In AWS you can choose from multiple purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. With AWS databases, you don\u2019t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running with self-healing storage and automated scaling, so that you can focus on higher value application development.The following question focuses on these considerations for performance efficiency.Your workload's database approach has a significant impact on performance efficiency. It's often an area that is chosen according to organizational defaults rather than through a data-driven approach. As with storage, it is critical to consider the access patterns of your workload, and also to consider if other non-database solutions could solve the problem more efficiently (such as using graph, time series, or in-memory storage database).PERF 4: How do you select your database solution?The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n\nKnowledge Area: Database types and services (for example, serverless, relational compared with non-relational, in-memory)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best way to select an optimal database solution for an application with critical uptime requirements?\",\n\"answer_choices\": [{\"answer\": \"Select the database that is most familiar to you and your team\",\"is_correct\": false,\"explanation\": \"Choosing the most familiar database may not provide the most optimal solution for the application's requirements.\"},{\"answer\": \"Scale the database to support potential high traffic bursts\",\"is_correct\": false,\"explanation\": \"Scaling the database to support potential high traffic bursts may not be necessary and can waste resources.\"},{\"answer\": \"Identify and document the data access patterns and use this information to choose the optimal database solution\",\"is_correct\": true,\"explanation\": \"By identifying and documenting data access patterns, you can identify the optimal database solution for the application's needs.\"},{\"answer\": \"Select the most cost-effective database solution\",\"is_correct\": false,\"explanation\": \"Cost-effectiveness is only one factor to consider when selecting a database solution; performance and scalability should also be taken into account.\"}]}", "docs": ["('Database - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-db.html')", "('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')"]}, {"scenario": "A research institution wants to perform data analysis on a large dataset using AWS compute resources.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Skills in", "item": "Configuring read replicas to meet business requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA research institution wants to perform data analysis on a large dataset using AWS compute resources.`\"\n\nContext:\n\n* Choose Regions where you will implement your workloads based on both your business requirements and sustainability goals.Best Practice:\n* Having backups and redundant workload components in place is the start of your DR strategy. RTO and RPO are your objectives for restoration of your workload. Set these based on business needs. Implement a strategy to meet these objectives, considering locations and function of workload resources and data. The probability of disruption and cost of recovery are also key factors that help to inform the business value of providing disaster recovery for a workload.Best practices\n* If components of the workload can only run in a single Availability Zone or in an on-premises data center, you must implement the capability to do a complete rebuild of the workload within your defined recovery objectives.If the best practice to deploy the workload to multiple locations is not possible due to technological constraints, you must implement an alternate path to resiliency. You must automate the ability to recreate necessary infrastructure, redeploy applications, and recreate necessary data for these cases.For example, Amazon EMR launches all nodes for a given cluster in the same Availability Zone because running a cluster in the same zone improves performance of the jobs flows as it provides a higher data access rate. If this component is required for workload resilience, then you must have a way to redeploy the cluster and its data. Also for Amazon EMR, you should provision redundancy in ways other than using Multi-AZ. You can provision multiple nodes. Using EMR File System (EMRFS), data in EMR can be stored in Amazon S3, which in turn can be replicated across multiple Availability Zones or AWS Regions.Similarly, for Amazon Redshift, by default it provisions your cluster in a randomly selected Availability Zone within the AWS Region that you select. All the cluster nodes are provisioned in the same zone.Level of risk exposed if this best practice is not established: MediumImplement self-healing. Deploy your instances or containers using automatic scaling when possible. If you cannot use automatic scaling, use automatic recovery for EC2 instances or implement self-healing automation based on Amazon EC2 or ECS container lifecycle events.Use Auto Scaling groups for instances and container workloads that have no requirements for a single instance IP address, private IP address, Elastic IP address, and instance metadata.What Is EC2 Auto Scaling?Service automatic scalingThe launch template user data can be used to implement automation that can self-heal most workloads.Use automatic recovery of EC2 instances for workloads that require a single instance ID address, private IP address, Elastic IP address, and instance metadata.Recover your instance.Automatic Recovery will send recovery status alerts to a SNS topic as the instance failure is detected.Use EC2 instance lifecycle events or ECS events to automate self-healing where automatic scaling or EC2 recovery cannot be used.EC2 Auto Scaling lifecycle hooksAmazon ECS eventsUse the events to invoke automation that will heal your component according to the process logic you require.Related documents:Amazon ECS eventsEC2 Auto Scaling lifecycle hooksRecover your instance.Service automatic scalingWhat Is EC2 Auto Scaling?\n* Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO).Best practices\n* A scalable workload provides elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time.Best practices\n* When you architect solutions, think about tradeoffs to ensure an optimal approach. Depending on your situation, you could trade consistency, durability, and space for time or latency, to deliver higher performance.Using AWS, you can go global in minutes and deploy resources in multiple locations across the globe to be closer to your end users. You can also dynamically add readonly replicas to information stores (such as database systems) to reduce the load on the primary database.The following question focuses on these considerations for performance efficiency.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-o\ufb00s impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.PERF 8: How do you use tradeoffs to improve performance?When architecting solutions, determining tradeoffs enables you to select an optimal approach. Often you can improve performance by trading consistency, durability, and space for time and latency.\n* Where applicable, use multiple strategies to improve performance. For example, using strategies like caching data to prevent excessive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Common anti-patterns:You assume that workload performance is adequate if customers are not complaining.You only collect data on performance after you have made performance-related changes.Benefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and historical reference. You can create dashboards and perform metric math on your data to derive operational and utilization insights for your workloads as they change over time.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Related documents:Amazon Builders\u2019 LibraryBest Practices for Implementing Amazon ElastiCacheAWS Database CachingAmazon CloudWatch RUMDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)AWS purpose-built databases (DAT209-L)Optimize applications through Amazon CloudWatch RUMRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n\nKnowledge Area: Configuring read replicas to meet business requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which strategy can be used to improve performance of a research institution's large dataset using AWS compute resources?\", \"answer_choices\": [{\"answer\": \"Automatic scaling of EC2 instances\",\"is_correct\": \"False\", \"explanation\": \"Automatic scaling of EC2 instances is used to provide elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time. It is not used to improve performance.\"},{\"answer\": \"Deploying workloads to multiple locations\",\"is_correct\": \"True\", \"explanation\": \"Deploying workloads to multiple locations is a best practice that can be used to improve performance. It can increase data access rate, provide redundancy, and reduce the load on the primary database.\"},{\"answer\": \"Using EC2 instance lifecycle events\",\"is_correct\": \"False\", \"explanation\": \"Using EC2 instance lifecycle events or ECS events can be used to automate self-healing where automatic scaling or EC2 recovery cannot be used. It is not used to improve performance.\"},{\"answer\": \"Using Amazon EMR File System (EMRFS)\",\"is_correct\": \"True\", \"explanation\": \"Using Amazon EMR File System (EMRFS) can be used to store data in Amazon S3, which can be replicated across multiple Availability Zones or AWS Regions. This can improve performance of the jobs flows by providing a higher data access rate.\"}]}", "docs": ["('SUS 1 How do you select Regions to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-01.html')", "('REL 13\\xa0 How do you plan for disaster recovery (DR)? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-13.html')", "('REL10-BP03 Automate recovery for components constrained to a single location - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_fault_isolation_single_az_system.html')", "('REL 9\\xa0 How do you back up data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-09.html')", "('REL 7\\xa0 How do you design your workload to adapt to changes in demand? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-07.html')", "('Tradeoffs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-tradeoffs.html')", "('PERF08-BP05 Use various performance-related strategies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_implement_strategy.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Skills in", "item": "Designing database architectures", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n* The optimal solution for a particular workload varies, and solutions often combine multiple approaches. Well-architected workloads use multiple solutions and enable different features to improve performance.AWS resources are available in many types and configurations, which makes it easier to find an approach that closely matches your workload needs. You can also find options that are not easily achievable with on-premises infrastructure. For example, a managed service such as Amazon DynamoDB provides a fully managed NoSQL database with single-digit millisecond latency at any scale.The following question focuses on these considerations for performance efficiency. (For a list of performance efficiency questions and best practices, see the Appendix.).Use a data-driven approach to select the patterns and implementation for your architecture and achieve a cost effective solution. AWS Solutions Architects, AWS Reference Architectures, and AWS Partner Network (APN) partners can help you select an architecture based on industry knowledge, but data obtained through benchmarking or load testing will be required to optimize your architecture.Your architecture will likely combine a number of different architectural approaches (for example, event-driven, ETL, or pipeline). The implementation of your architecture will use the AWS services that are specific to the optimization of your architecture's performance. In the following sections we discuss the four main resource types to consider (compute, storage, database, and network).PERF 1: How do you select the best performing architecture?Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n\nKnowledge Area: Designing database architectures\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice for selecting the best performing architecture?\", \n\"answer_choices\": [\n    {\"answer\": \"Use a single database vendor to simplify operations management\", \"is_correct\": false, \"explanation\": \"Using a single database vendor carries the risk of not selecting the best performing architecture and creating lower performance efficiency.\"},\n    {\"answer\": \"Assume data access patterns will stay consistent over time\", \"is_correct\": false, \"explanation\": \"Data access patterns may vary over time, so it is important to select an architecture that is dynamic and can support changing access patterns.\"},\n    {\"answer\": \"Use the access patterns of the workload to decide which services and technologies to use\", \"is_correct\": true, \"explanation\": \"Using the access patterns of the workload is the best practice for selecting the best performing architecture. This includes looking at the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.\"},\n    {\"answer\": \"Configure the database to support a potential high traffic burst\", \"is_correct\": false, \"explanation\": \"Configuring the database to support a potential high traffic burst carries the risk of the database resources remaining idle most of the time, resulting in lower performance efficiency.\"}\n]}", "docs": ["('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')", "('Selection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-sel.html')", "('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Skills in", "item": "Determining an appropriate database engine (for example, MySQL compared with PostgreSQL)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* The cloud offers purpose-built database services that address different problems presented by your workload. You can choose from many purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. By picking the best database to solve a specific problem (or a group of problems), you can break away from restrictive one-size-fits-all monolithic databases and focus on building applications to meet the performance needs of your customers.In AWS you can choose from multiple purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. With AWS databases, you don\u2019t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running with self-healing storage and automated scaling, so that you can focus on higher value application development.The following question focuses on these considerations for performance efficiency.Your workload's database approach has a significant impact on performance efficiency. It's often an area that is chosen according to organizational defaults rather than through a data-driven approach. As with storage, it is critical to consider the access patterns of your workload, and also to consider if other non-database solutions could solve the problem more efficiently (such as using graph, time series, or in-memory storage database).PERF 4: How do you select your database solution?The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n* Evaluate the impact of tradeoffs between competing interests or alternative approaches, to help make informed decisions when determining where to focus efforts or choosing a course of action. For example, accelerating speed to market for new features may be emphasized over cost optimization, or you may choose a relational database for non-relational data to simplify the effort to migrate a system, rather than migrating to a database optimized for your data type and updating your application.AWS can help you educate your teams about AWS and its services to increase their understanding of how their choices can have an impact on your workload. You should use the resources provided by AWS Support (AWS Knowledge Center, AWS Discussion Forums, and AWS Support Center) and AWS Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS questions.AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog and The Official AWS Podcast.Common anti-patterns:You are using a relational database to manage time series and non-relational data. There are database options that are optimized to support the data types you are using but you are unaware of the benefits because you have not evaluated the tradeoffs between solutions.Your investors request that you demonstrate compliance with Payment Card Industry Data Security Standards (PCI DSS). You do not consider the tradeoffs between satisfying their request and continuing with your current development efforts. Instead you proceed with your development efforts without demonstrating compliance. Your investors stop their support of your company over concerns about the security of your platform and their investments.Benefits of establishing this best practice: Understanding the implications and consequences of your choices enables you to prioritize your options.Level of risk exposed if this best practice is not established: MediumEvaluate tradeoffs: Evaluate the impact of tradeoffs between competing interests, to help make informed decisions when determining where to focus efforts. For example, accelerating speed to market for new features might be emphasized over cost optimization.AWS can help you educate your teams about AWS and its services to increase their understanding of how their choices can have an impact on your workload. You should use the resources provided by AWS Support (AWS Knowledge Center, AWS Discussion Forums, and AWS Support Center) and AWS Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS questions.AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog and The Official AWS Podcast.Related documents:AWS BlogAWS Cloud ComplianceAWS Discussion ForumsAWS DocumentationAWS Knowledge CenterAWS SupportAWS Support CenterThe Amazon Builders' LibraryThe Official AWS Podcast\n\nKnowledge Area: Determining an appropriate database engine (for example, MySQL compared with PostgreSQL)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which database engine is the most appropriate for the manufacturing company's needs to optimize their supply chain and production processes?\",\n\"answer_choices\": [{\"answer\": \"MySQL\",\"is_correct\": \"False\",\"explanation\": \"MySQL is a relational database engine, which is not appropriate for this application.\"},{\"answer\": \"PostgreSQL\",\"is_correct\": \"False\",\"explanation\": \"PostgreSQL is a relational database engine, which is not appropriate for this application.\"},{\"answer\": \"In-memory database\",\"is_correct\": \"False\",\"explanation\": \"In-memory databases are optimized for high performance and low latency, but are not suitable for all applications.\"},{\"answer\": \"Graph database\",\"is_correct\": \"True\",\"explanation\": \"Graph databases are optimized to handle many-to-many relationships and can be used to optimize supply chain and production processes.\"}]}", "docs": ["('Database - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-db.html')", "('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')", "('OPS01-BP06 Evaluate tradeoffs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_priorities_eval_tradeoffs.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Skills in", "item": "Determining an appropriate database type (for example, Amazon Aurora, Amazon DynamoDB)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* The cloud offers purpose-built database services that address different problems presented by your workload. You can choose from many purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. By picking the best database to solve a specific problem (or a group of problems), you can break away from restrictive one-size-fits-all monolithic databases and focus on building applications to meet the performance needs of your customers.In AWS you can choose from multiple purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. With AWS databases, you don\u2019t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running with self-healing storage and automated scaling, so that you can focus on higher value application development.The following question focuses on these considerations for performance efficiency.Your workload's database approach has a significant impact on performance efficiency. It's often an area that is chosen according to organizational defaults rather than through a data-driven approach. As with storage, it is critical to consider the access patterns of your workload, and also to consider if other non-database solutions could solve the problem more efficiently (such as using graph, time series, or in-memory storage database).PERF 4: How do you select your database solution?The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n\nKnowledge Area: Determining an appropriate database type (for example, Amazon Aurora, Amazon DynamoDB)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best way for a gaming company to select an appropriate database type to host and scale their multiplayer game servers on AWS?\", \"answer_choices\": [{\"answer\": \"Use the most common database vendor to simplify operations management.\", \"is_correct\": \"false\", \"explanation\": \"This may not be the most efficient or cost-effective solution. It is important to consider the access patterns of the workload, such as transactions, consistency, latency and access requirements, in order to identify the optimal storage solution.\"},{\"answer\": \"Identify and document the anticipated growth of the data and traffic, and select a database type accordingly.\", \"is_correct\": \"true\", \"explanation\": \"Optimizing the data storage based on access patterns will help decrease development complexity and maximize performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.\"},{\"answer\": \"Use a shared database for transactional and analytical uses.\", \"is_correct\": \"false\", \"explanation\": \"Using a shared database for transactional and analytical uses is an anti-pattern and can lead to lower performance efficiency.\"},{\"answer\": \"Assume that data access patterns will stay consistent over time.\", \"is_correct\": \"false\", \"explanation\": \"Data access patterns can change over time, and it is important to regularly review and update the data storage configuration accordingly.\"}]}", "docs": ["('Database - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-db.html')", "('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')"]}, {"scenario": "A marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 3: Determine high-performing database solutions", "focus": "Skills in", "item": "Integrating caching to meet business requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.`\"\n\nContext:\n\n* When evaluating performance-related improvements, determine which choices will impact your customers and workload efficiency. For example, if using a key-value data store increases system performance, it is important to evaluate how the eventually consistent nature of it will impact customers.Identify areas of poor performance in your system through metrics and monitoring. Determine how you can make improvements, what trade-offs those improvements bring, and how they impact the system and the user experience. For example, implementing caching data can help dramatically improve performance but requires a clear strategy for how and when to update or invalidate cached data to prevent incorrect system behavior.Common anti-patterns:You assume that all performance gains should be implemented, even if there are tradeoffs for implementation such as eventual consistency.You only evaluate changes to workloads when a performance issue has reached a critical point.Benefits of establishing this best practice: When you are evaluating potential performance-related improvements, you must decide if the tradeoffs for the changes are consistent with the workload requirements. In some cases, you may have to implement additional controls to compensate for the tradeoffs.Level of risk exposed if this best practice is not established: HighIdentify tradeoffs: Use metrics and monitoring to identify areas of poor performance in your system. Determine how to make improvements, and how tradeoffs will impact the system and the user experience. For example, implementing caching data can help dramatically improve performance, but it requires a clear strategy for how and when to update or invalidate cached data to prevent incorrect system behavior.Related documents:Amazon Builders\u2019 LibraryAmazon QuickSight KPIsAmazon CloudWatch RUMX-Ray DocumentationRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)Build a Monitoring PlanOptimize applications through Amazon CloudWatch RUMDemo of Amazon CloudWatch SyntheticsRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web Client\n* Where applicable, use multiple strategies to improve performance. For example, using strategies like caching data to prevent excessive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Common anti-patterns:You assume that workload performance is adequate if customers are not complaining.You only collect data on performance after you have made performance-related changes.Benefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and historical reference. You can create dashboards and perform metric math on your data to derive operational and utilization insights for your workloads as they change over time.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Related documents:Amazon Builders\u2019 LibraryBest Practices for Implementing Amazon ElastiCacheAWS Database CachingAmazon CloudWatch RUMDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)AWS purpose-built databases (DAT209-L)Optimize applications through Amazon CloudWatch RUMRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n* Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload. Fetch data from shared storage only as needed. Detach unused volumes to make more resources available.Level of risk exposed if this best practice is not established: LowMigrate data to shared storage when the data has multiple consumers.Fetch data from shared storage only as needed.Delete data as appropriate for your usage patterns, and implement time-to-live (TTL) functionality to manage cached data.Detach volumes from clients that are not actively using them.Related documents:Amazon FSxCaching strategiesWhat is Amazon Elastic File System?What is Amazon S3?\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n\nKnowledge Area: Integrating caching to meet business requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following options is the best way to improve the performance of a marketing company's customer data processing system on AWS while taking into account trade-offs with eventual consistency?\", \"answer_choices\": [{\"answer\": \"Implement caching data to store frequently accessed data in memory, but update or invalidate the cached data regularly to avoid incorrect system behavior\",\"is_correct\": \"true\",\"explanation\": \"Caching data can improve the performance of the system while ensuring eventual consistency by regularly updating or invalidating the cached data.\"},{\"answer\": \"Migrate data to shared storage to reduce total storage requirements\",\"is_correct\": \"false\",\"explanation\": \"Migrating data to shared storage can help reduce the total storage requirements, but it will not improve the performance of the system.\"},{\"answer\": \"Evaluate potential performance-related improvements when a performance issue has reached a critical point\",\"is_correct\": \"false\",\"explanation\": \"Evaluating performance-related improvements when a performance issue has reached a critical point is an anti-pattern, and should be avoided.\"},{\"answer\": \"Measure the impacts to the system and the user experience from trade-offs\",\"is_correct\": \"true\",\"explanation\": \"Measuring the impacts to the system and the user experience from trade-offs is important to ensure that the trade-offs are consistent with the workload requirements.\"}]}", "docs": ["('PERF08-BP03 Identify how tradeoffs impact customers and efficiency - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_understand_impact.html')", "('PERF08-BP05 Use various performance-related strategies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_implement_strategy.html')", "('SUS04-BP06 Use shared file systems or object storage to access common data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a7.html')", "('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 4: Determine high-performing and/or scalable network architectures", "focus": "Knowledge of", "item": "Edge networking services with appropriate use cases (for example, Amazon CloudFront, AWS Global Accelerator)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies.Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public endpoints. You might also choose to evaluate AWS Marketplace software appliances for load balancing and proxying.Consumers of the service your workload provides, whether they are end-users or other services, make requests on these service endpoints. Several AWS resources are available to enable you to provide highly available endpoints.Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.Distributed Denial of Service (DDoS) attacks risk shutting out legitimate traffic and lowering availability for your users. AWS Shield provides automatic protection against these attacks at no extra cost for AWS service endpoints on your workload. You can augment these features with virtual appliances from APN Partners and the AWS Marketplace to meet your needs.Common anti-patterns:Using public internet addresses on instances or containers and managing the connectivity to them via DNS.Using Internet Protocol addresses instead of domain names for locating services.Providing content (web pages, static assets, media files) to a large geographic area and not using a content delivery network.Benefits of establishing this best practice: By implementing highly available services in your workload, you know that your workload will be available to your users.Level of risk exposed if this best practice is not established: HighEnsure that you have highly available connectivity for users of the workload Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public facing endpoints. You may also choose to evaluate AWS Marketplace software appliances for load-balancing and proxying.Ensure that you have a highly available connection to your users.Ensure that you are using a highly available DNS to manage the domain names of your application endpoints.If your users access your application via the internet, use service API operations to confirm the correct usage of Internet Gateways. Also confirm that the route tables entries for the subnets hosting your application endpoints are correct.DescribeInternetGatewaysDescribeRouteTablesEnsure that you are using a highly available reverse proxy or load balancer in front of your application.If your users access your application via your on-premises environment, ensure that your connectivity between AWS and your on-premises environment is highly available.Use Route 53 to manage your domain names.What is Amazon Route 53?Use a third-party DNS provider that meets your requirements.Use Elastic Load Balancing.What is Elastic Load Balancing?Use an AWS Marketplace appliance that meets your requirements.Related documents:APN Partner: partners that can help plan your networkingAWS Direct Connect Resiliency RecommendationsAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivityUsing the Direct Connect Resiliency Toolkit to get startedVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is AWS Global Accelerator?What Is Amazon VPC?What Is a Transit Gateway?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?Working with Direct Connect GatewaysRelated videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n* Implement services to reduce data transfer. For example, using a content delivery network (CDN) such as Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using AWS Direct Connect instead of VPN for connectivity to AWS.Level of risk exposed if this best practice is not established: LowAmazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources. By using CloudFront, you can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network. It is ideal for small offices or business partners because it provides quick and easy connectivity, and it is a fully managed and elastic service.VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs. Gateway VPC endpoints have no hourly charges, and support Amazon Simple Storage Service(Amazon S3) and Amazon DynamoDB. Interface VPC endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost.Implementation stepsImplement services: Using the data transfer modeling, look at where the largest costs and highest volume flows are. Review the AWS services and assess whether there is a service that reduces or removes the transfer, specifically networking and content delivery. Also look for caching services where there is repeated access to data, or large amounts of data.Related documents:AWS Direct ConnectAWS Explore Our ProductsAWS caching solutionsAmazon CloudFrontDeliver content faster with Amazon CloudFront\n\nKnowledge Area: Edge networking services with appropriate use cases (for example, Amazon CloudFront, AWS Global Accelerator)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"Which AWS service can be used to reduce data transfer and reduce costs while ensuring high availability?\",\n\"answer_choices\": [\n{\"answer\":\"Amazon Simple Storage Service (Amazon S3)\",\"is_correct\":false,\"explanation\":\"Amazon S3 is used for storing data, but not for reducing data transfer.\"},\n{\"answer\":\"AWS Direct Connect\",\"is_correct\":true,\"explanation\":\"AWS Direct Connect allows you to establish a dedicated network connection to AWS which can reduce network costs, increase bandwidth and provide a more consistent network experience than internet-based connections.\"},\n{\"answer\":\"Amazon CloudFront\",\"is_correct\":false,\"explanation\":\"Amazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds, but it is not used for reducing data transfer.\"},\n{\"answer\":\"AWS VPN\",\"is_correct\":false,\"explanation\":\"AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network, but it is not used for reducing data transfer.\"}\n]}", "docs": ["('REL02-BP01 Use highly available network connectivity for your workload public endpoints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ha_conn_users.html')", "('COST08-BP03 Implement services to reduce data transfer costs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_implement_services.html')"]}, {"scenario": "A marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 4: Determine high-performing and/or scalable network architectures", "focus": "Knowledge of", "item": "How to design network architecture (for example, subnet tiers, routing, IP addressing)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.`\"\n\nContext:\n\n* Group components that share reachability requirements into layers. For example, a database cluster in a virtual private cloud (VPC) with no need for internet access should be placed in subnets with no route to or from the internet. In a serverless workload operating without a VPC, similar layering and segmentation with microservices can achieve the same goal.Components such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Relational Database Service (Amazon RDS) database clusters, and AWS Lambda functions that share reachability requirements can be segmented into layers formed by subnets. For example, an Amazon RDS database cluster in a VPC with no need for internet access should be placed in subnets with no route to or from the internet. This layered approach for the controls mitigates the impact of a single layer misconfiguration, which could allow unintended access. For Lambda, you can run your functions in your VPC to take advantage of VPC-based controls.For network connectivity that can include thousands of VPCs, AWS accounts, and on-premises networks, you should use AWS Transit Gateway. It acts as a hub that controls how traffic is routed among all the connected networks, which act like spokes. Traffic between an Amazon Virtual Private Cloud and AWS Transit Gateway remains on the AWS private network, which reduces external threat vectors such as distributed denial of service (DDoS) attacks and common exploits, such as SQL injection, cross-site scripting, cross-site request forgery, or abuse of broken authentication code. AWS Transit Gateway inter-region peering also encrypts inter-region traffic with no single point of failure or bandwidth bottleneck.Level of risk exposed if this best practice is not established: HighCreate subnets in VPC: Create subnets for each layer (in groups that include multiple Availability Zones), and associate route tables to control routing.VPCs and subnetsRoute tablesRelated documents:AWS Firewall ManagerAmazon InspectorAmazon VPC SecurityGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n* When architecting your network topology, you should examine the connectivity requirements of each component. For example, if a component requires internet accessibility (inbound and outbound), connectivity to VPCs, edge services, and external data centers.A VPC allows you to define your network topology that spans an AWS Region with a private IPv4 address range that you set, or an IPv6 address range AWS selects. You should apply multiple controls with a defense in depth approach for both inbound and outbound traffic, including the use of security groups (stateful inspection firewall), Network ACLs, subnets, and route tables. Within a VPC, you can create subnets in an Availability Zone. Each subnet can have an associated route table that defines routing rules for managing the paths that traffic takes within the subnet. You can define an internet routable subnet by having a route that goes to an internet or NAT gateway attached to the VPC, or through another VPC.When an instance, Amazon Relational Database Service(Amazon RDS) database, or other service is launched within a VPC, it has its own security group per network interface. This firewall is outside the operating system layer and can be used to define rules for allowed inbound and outbound traffic. You can also define relationships between security groups. For example, instances within a database tier security group only accept traffic from instances within the application tier, by reference to the security groups applied to the instances involved. Unless you are using non-TCP protocols, it shouldn\u2019t be necessary to have an Amazon Elastic Compute Cloud(Amazon EC2) instance directly accessible by the internet (even with ports restricted by security groups) without a load balancer, or CloudFront. This helps protect it from unintended access through an operating system or application issue. A subnet can also have a network ACL attached to it, which acts as a stateless firewall. You should configure the network ACL to narrow the scope of traffic allowed between layers, note that you need to define both inbound and outbound rules.Some AWS services require components to access the internet for making API calls, where AWS API endpoints are located. Other AWS services use VPC endpoints within your Amazon VPCs. Many AWS services, including Amazon S3 and Amazon DynamoDB, support VPC endpoints, and this technology has been generalized in AWS PrivateLink. We recommend you use this approach to access AWS services, third-party services, and your own services hosted in other VPCs securely. All network traffic on AWS PrivateLink stays on the global AWS backbone and never traverses the internet. Connectivity can only be initiated by the consumer of the service, and not by the provider of the service. Using AWS PrivateLink for external service access allows you to create air-gapped VPCs with no internet access and helps protect your VPCs from external threat vectors. Third-party services can use AWS PrivateLink to allow their customers to connect to the services from their VPCs over private IP addresses. For VPC assets that need to make outbound connections to the internet, these can be made outbound only (one-way) through an AWS managed NAT gateway, outbound only internet gateway, or web proxies that you create and manage.Level of risk exposed if this best practice is not established: HighControl network traffic in a VPC: Implement VPC best practices to control traffic.Amazon VPC securityVPC endpointsAmazon VPC security groupNetwork ACLsControl traffic at the edge: Implement edge services, such as Amazon CloudFront, to provide an additional layer of protection and other features.Amazon CloudFront use casesAWS Global AcceleratorAWS Web Application Firewall (AWS WAF)Amazon Route 53Amazon VPC Ingress RoutingControl private network traffic: Implement services that protect your private traffic for your workload.Amazon VPC PeeringAmazon VPC Endpoint Services (AWS PrivateLink)Amazon VPC Transit GatewayAWS Direct ConnectAWS Site-to-Site VPNAWS Client VPNAmazon S3 Access PointsRelated documents:AWS Firewall ManagerAmazon InspectorGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n\nKnowledge Area: How to design network architecture (for example, subnet tiers, routing, IP addressing)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best practice the company should employ to control network traffic for the customer data processing and analysis project?\", \"answer_choices\": [{\"answer\": \"Create a single VPC and configure routing to the internet gateway\",\"is_correct\": false,\"explanation\": \"This option does not provide multiple controls with a defense in depth approach, or segment networks into different layers.\"},{\"answer\": \"Create multiple subnets within one VPC with associated route tables to control routing\",\"is_correct\": true,\"explanation\": \"Creating multiple subnets within a VPC with associated route tables to control routing is a best practice for controlling network traffic.\"},{\"answer\": \"Create a single subnet within one VPC with a single route table\",\"is_correct\": false,\"explanation\": \"This option does not provide multiple controls with a defense in depth approach, or segment networks into different layers.\"},{\"answer\": \"Create multiple VPCs and configure routing to internet gateways\",\"is_correct\": false,\"explanation\": \"This option does not provide multiple controls with a defense in depth approach, or segment networks into different layers.\"}]}", "docs": ["('SEC05-BP01 Create network layers - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_create_layers.html')", "('SEC05-BP02 Control traffic at all layers - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_layered.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 4: Determine high-performing and/or scalable network architectures", "focus": "Knowledge of", "item": "Load balancing concepts (for example, Application Load Balancer)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* Distribute traffic across multiple resources or services to allow your workload to take advantage of the elasticity that the cloud provides. You can also use load balancing for offloading encryption termination to improve performance and to manage and route traffic effectively.When implementing a scale-out architecture where you want to use multiple instances for service content, you can use load balancers inside your Amazon VPC. AWS provides multiple models for your applications in the ELB service. Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers.Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. It is capable of handling millions of requests per second while maintaining ultra-low latencies, and it is optimized to handle sudden and volatile traffic patterns.Elastic Load Balancing provides integrated certificate management and SSL/TLS decryption, allowing you the flexibility to centrally manage the SSL settings of the load balancer and offload CPU intensive work from your workload.Common anti-patterns:You route all internet traffic through existing load balancers.You use generic TCP load balancing and making each compute node handle SSL encryption.Benefits of establishing this best practice: A load balancer handles the varying load of your application traffic in a single Availability Zone, or across multiple Availability Zones. Load balancers feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant.Level of risk exposed if this best practice is not established: HighUse the appropriate load balancer for your workload: Select the appropriate load balancer for your workload. If you must load balance HTTP requests, we recommend Application Load Balancer. For network and transport protocols (layer 4 \u2013 TCP, UDP) load balancing, and for extreme performance and low latency applications, we recommend Network Load Balancer. Application Load Balancers support HTTPS and Network Load Balancers support TLS encryption offloading.Enable offload of HTTPS or TLS encryption: Elastic Load Balancing includes integrated certificate management, user-authentication, and SSL/TLS decryption. It provides the flexibility to centrally manage TLS settings and offload CPU intensive workloads from your applications. Encrypt all HTTPS traffic as part of your load balancer deployment.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies.Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public endpoints. You might also choose to evaluate AWS Marketplace software appliances for load balancing and proxying.Consumers of the service your workload provides, whether they are end-users or other services, make requests on these service endpoints. Several AWS resources are available to enable you to provide highly available endpoints.Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.Distributed Denial of Service (DDoS) attacks risk shutting out legitimate traffic and lowering availability for your users. AWS Shield provides automatic protection against these attacks at no extra cost for AWS service endpoints on your workload. You can augment these features with virtual appliances from APN Partners and the AWS Marketplace to meet your needs.Common anti-patterns:Using public internet addresses on instances or containers and managing the connectivity to them via DNS.Using Internet Protocol addresses instead of domain names for locating services.Providing content (web pages, static assets, media files) to a large geographic area and not using a content delivery network.Benefits of establishing this best practice: By implementing highly available services in your workload, you know that your workload will be available to your users.Level of risk exposed if this best practice is not established: HighEnsure that you have highly available connectivity for users of the workload Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public facing endpoints. You may also choose to evaluate AWS Marketplace software appliances for load-balancing and proxying.Ensure that you have a highly available connection to your users.Ensure that you are using a highly available DNS to manage the domain names of your application endpoints.If your users access your application via the internet, use service API operations to confirm the correct usage of Internet Gateways. Also confirm that the route tables entries for the subnets hosting your application endpoints are correct.DescribeInternetGatewaysDescribeRouteTablesEnsure that you are using a highly available reverse proxy or load balancer in front of your application.If your users access your application via your on-premises environment, ensure that your connectivity between AWS and your on-premises environment is highly available.Use Route 53 to manage your domain names.What is Amazon Route 53?Use a third-party DNS provider that meets your requirements.Use Elastic Load Balancing.What is Elastic Load Balancing?Use an AWS Marketplace appliance that meets your requirements.Related documents:APN Partner: partners that can help plan your networkingAWS Direct Connect Resiliency RecommendationsAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivityUsing the Direct Connect Resiliency Toolkit to get startedVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is AWS Global Accelerator?What Is Amazon VPC?What Is a Transit Gateway?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?Working with Direct Connect GatewaysRelated videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n\nKnowledge Area: Load balancing concepts (for example, Application Load Balancer)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service should an organization use to ensure highly available public facing endpoints for their workload?\", \"answer_choices\": [{\"answer\": \"Amazon Route 53\",\"is_correct\": \"true\",\"explanation\": \"Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.\"},{\"answer\": \"AWS Global Accelerator\",\"is_correct\": \"true\",\"explanation\": \"AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.\"},{\"answer\": \"Amazon CloudFront\",\"is_correct\": \"true\",\"explanation\": \"Amazon CloudFront is a content delivery network (CDN) which provides a highly available public endpoint for web content.\"},{\"answer\": \"Elastic Load Balancing\",\"is_correct\": \"true\",\"explanation\": \"Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.\"}]}", "docs": ["('PERF05-BP04 Leverage load-balancing and encryption offloading - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_encryption_offload.html')", "('REL02-BP01 Use highly available network connectivity for your workload public endpoints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ha_conn_users.html')"]}, {"scenario": "A company wants to build a mobile application that needs to access and store data in the cloud.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 4: Determine high-performing and/or scalable network architectures", "focus": "Knowledge of", "item": "Network connection options (for example, AWS VPN, Direct Connect, AWS PrivateLink", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to build a mobile application that needs to access and store data in the cloud.`\"\n\nContext:\n\n* When a common network is required to connect on-premises and cloud resources in AWS, ensure that you have adequate bandwidth to meet your performance requirements. Estimate the bandwidth and latency requirements for your hybrid workload. These numbers will drive the sizing requirements for AWS Direct Connect or your VPN endpoints.Desired outcome: When deploying a workload that will need hybrid network connectivity, you have multiple configuration options for connectivity, such as managed and non-managed VPNs or Direct Connect. Select the appropriate connection type for each workload while ensuring you have adequate bandwidth and encryption requirements between your location and the cloud.Common anti-patterns:You only evaluate VPN solutions for your network encryption requirements.You don\u2019t evaluate backup or parallel connectivity options.You use default configurations for routers, tunnels, and BGP sessions.You fail to understand or identify all workload requirements (encryption, protocol, bandwidth and traffic needs).Benefits of establishing this best practice: Selecting and configuring appropriately sized hybrid network solutions will increase the reliability of your workload and maximize performance opportunities. By identifying workload requirements, planning ahead, and evaluating hybrid solutions you will minimize expensive physical network changes and operational overhead while increasing your time to market.Level of risk exposed if this best practice is not established: HighDevelop a hybrid networking architecture based on your bandwidth requirements: Estimate the bandwidth and latency requirements of your hybrid applications. Based on your bandwidth requirements, a single VPN or Direct Connect connection might not be enough, and you must architect a hybrid setup to enable traffic load balancing across multiple connections. Direct connect may be required which offers more predictable and consistent performance due to its private network connectivity. It is great for production workloads that require consistent latency and almost zero jitter.AWS Direct Connect provides dedicated connectivity to the AWS environment, from 50 Mbps up to 10 Gbps. This gives you managed and controlled latency and provisioned bandwidth so your workload can connect easily and in a performant way to other environments. Using one of the AWS Direct Connect partners, you can have end-to-end connectivity from multiple environments, thus providing an extended network with consistent performance.The AWS Site-to-Site VPN is a managed VPN service for VPCs. When a VPN connection is created, AWS provides tunnels to two different VPN endpoints. With AWS Transit Gateway, you can simplify the connectivity between multiple VPCs and also connect to any VPC attached to AWS Transit Gateway with a single VPN connection. AWS Transit Gateway also enables you to scale beyond the 1.25Gbps IPsec VPN throughput limit by enabling equal cost multi-path (ECMP) routing support over multiple VPN tunnels.Level of effort for the implementation plan: There is a high level of effort to evaluate workload needs for hybrid networks and to implement hybrid networking solutions.Related documents:Network Load BalancerNetworking Products with AWSTransit GatewayTransitioning to latency-based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsSite-to-Site VPNBuilding a Scalable and Secure Multi-VPC AWS Network InfrastructureDirect ConnectClient VPNRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)AWS Global AcceleratorDirect ConnectTransit Gateway ConnectVPN SolutionsSecurity with VPN SolutionsRelated examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n\nKnowledge Area: Network connection options (for example, AWS VPN, Direct Connect, AWS PrivateLink\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which method of network connection should be used for a mobile application that needs to access and store data in the cloud?\", \n\"answer_choices\": [\n{\"answer\": \"AWS Site-to-Site VPN\",\"is_correct\": \"False\",\"explanation\": \"AWS Site-to-Site VPN is a managed VPN service for VPCs, and is not suitable for mobile application that need to access and store data in the cloud.\"},\n{\"answer\": \"AWS Direct Connect\",\"is_correct\": \"True\",\"explanation\": \"AWS Direct Connect provides dedicated connectivity to the AWS environment, from 50 Mbps up to 10 Gbps and is suitable for mobile applications that need to access and store data in the cloud.\"},\n{\"answer\": \"AWS PrivateLink\",\"is_correct\": \"False\",\"explanation\": \"AWS PrivateLink is a networking technology that provides private connectivity between VPCs, AWS services, and on-premises applications, and is not suitable for mobile applications that need to access and store data in the cloud.\"},\n{\"answer\": \"AWS Transmit Gateway\",\"is_correct\": \"False\",\"explanation\": \"AWS Transmit Gateway is a managed service that provides a single gateway to connect multiple VPCs, but is not suitable for mobile applications that need to access and store data in the cloud.\"}\n]}", "docs": ["('PERF05-BP03 Choose appropriately sized dedicated connectivity or VPN for hybrid workloads - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_hybrid.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 4: Determine high-performing and/or scalable network architectures", "focus": "Skills in", "item": "Creating a network topology for various architectures (for example, global, hybrid, multi-tier)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* Workloads often exist in multiple environments. These include multiple cloud environments (both publicly accessible and private) and possibly your existing data center infrastructure. Plans must include network considerations such as intra- and inter-system connectivity, public IP address management, private IP address management, and domain name resolution.Best practices\n* The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.Best practices\n* Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.Best practices\n* All components are selected, and architecture is designed to reduce data transfer costs. This includes using components such as wide-area-network (WAN) optimization and Multi-Availability Zone (AZ) configurationsLevel of risk exposed if this best practice is not established: LowArchitecting for data transfer ensures that you minimize data transfer costs. This may involve using content delivery networks to locate data closer to users, or using dedicated network links from your premises to AWS. You can also use WAN optimization and application optimization to reduce the amount of data that is transferred between components.Implementation stepsSelect components for data transfer: Using the data transfer modeling, focus on where the largest data transfer costs are or where they would be if the workload usage changes. Look for alternative architectures, or additional components that remove or reduce the need for data transfer, or lower its cost.Related documents:AWS caching solutionsDeliver content faster with Amazon CloudFront\n* Since the network is between all workload components, it can have great impacts, both positive and negative, on workload performance and behavior. There are also workloads that are heavily dependent on network performance such as High Performance Computing (HPC) where deep network understanding is important to increase cluster performance. You must determine the workload requirements for bandwidth, latency, jitter, and throughput.On AWS, networking is virtualized and is available in a number of different types and configurations. This makes it easier to match your networking methods with your needs. AWS offers product features (for example, Enhanced Networking, Amazon EBS-optimized instances, Amazon S3 transfer acceleration, and dynamic Amazon CloudFront) to optimize network traffic. AWS also offers networking features (for example, Amazon Route 53 latency routing, Amazon VPC endpoints, AWS Direct Connect, and AWS Global Accelerator) to reduce network distance or jitter.The following question focuses on these considerations for performance efficiency.You must consider location when deploying your network. You can choose to place resources close to where they will be used to reduce distance. Use networking metrics to make changes to networking configuration as the workload evolves. By taking advantage of Regions, placement groups, and edge services, you can significantly improve performance. Cloud based networks can be quickly re-built or modified, so evolving your network architecture over time is necessary to maintain performance efficiency.PERF 5: How do you configure your networking solution?The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.\n* Analyze and understand how network-related decisions impact workload performance. The network is responsible for the connectivity between application components, cloud services, edge networks and on-premises data and therefor it can highly impact workload performance. In addition to workload performance, user experience is also impacted by network latency, bandwidth, protocols, location, network congestion, jitter, throughput, and routing rules.Desired outcome: Have a documented list of networking requirements from the workload including latency, packet size, routing rules, protocols, and supporting traffic patterns. Review the available networking solutions and identify which service meets your workload networking characteristics. Cloud-based networks can be quickly rebuilt, so evolving your network architecture over time is necessary to improve performance efficiency.Common anti-patterns:All traffic flows through your existing data centers.You overbuild Direct Connect sessions without understanding the actual usage requirements.You don\u2019t consider workload characteristics and encryption overhead when defining your networking solutions.You use on-premises concepts and strategies for networking solutions in the cloud.Benefits of establishing this best practice: Understanding how networking impacts workload performance will help you identify potential bottlenecks, improve user experience, increase reliability, and lower operational maintenance as the workload changes.Level of risk exposed if this best practice is not established: HighIdentify important network performance metrics of your workload and capture its networking characteristics. Define and document requirements as part of a data-driven approach, using benchmarking or load testing. Use this data to identify where your network solution is constrained, and examine configuration options that could improve the workload. Understand the cloud-native networking features and options available and how they can impact your workload performance based on the requirements. Each networking feature has advantages and disadvantages and can be configured to meet your workload characteristics and scale based on your needs.Implementation steps:Define and document networking performance requirements:Include metrics such as network latency, bandwidth, protocols, locations, traffic patterns (spikes and frequency), throughput, encryption, inspection, and routing rulesCapture your foundational networking characteristics:VPC Flow LogsAWS Transit Gateway metricsAWS PrivateLink metricsCapture your application networking characteristics:Elastic Network AdaptorAWS App Mesh metricsAmazon API Gateway metricsCapture your edge networking characteristics:Amazon CloudFront metricsAmazon Route 53 metricsAWS Global Accelerator metricsCapture your hybrid networking characteristics:Direct Connect metricsAWS Site-to-Site VPN metricsAWS Client VPN metricsAWS Cloud WAN metricsCapture your security networking characteristics:AWS Shield, WAF, and Network Firewall metricsCapture end-to-end performance metrics with tracing tools:AWS X-RayAmazon CloudWatch RUMBenchmark and test network performance:Benchmark network throughput: Some factors that can affect EC2 network performance when the instances are in the same VPC. Measure the network bandwidth between EC2 Linux instances in the same VPC.Perform load tests to experiment with networking solutions and optionsLevel of effort for the implementation plan: There is a medium level of effort to document workload networking requirements, options, and available solutions.Related documents:Application Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to latency-based routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Improve Global Network Performance for ApplicationsEC2 Instances and Performance Optimization Best PracticesOptimizing Network Performance for Amazon EC2 InstancesNetworking best practices and tips with the Well-Architected FrameworkAWS networking best practices in large-scale migrationsRelated examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n\nKnowledge Area: Creating a network topology for various architectures (for example, global, hybrid, multi-tier)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"A gaming company wants to use AWS to host and scale their multiplayer game servers. In order to optimize network performance, what should be done?\",\n\"answer_choices\": [\n    {\"answer\": \"Use EC2 Placement Groups to place resources close to where they will be used to reduce distance\",\"is_correct\": \"False\",\"explanation\": \"EC2 Placement Groups are used to ensure that instances are placed in logical groupings to reduce latency and increase performance\"},\n    {\"answer\": \"Use AWS Global Accelerator to reduce network distance or jitter\",\"is_correct\": \"True\",\"explanation\": \"AWS Global Accelerator can be used to reduce network distance or jitter, improving network performance\"},\n    {\"answer\": \"Use Amazon Route 53 latency routing to optimize network traffic\",\"is_correct\": \"False\",\"explanation\": \"Amazon Route 53 latency routing can be used to route requests to the endpoint which provides the lowest latency\"},\n    {\"answer\": \"Use enhanced networking on Windows Instances\",\"is_correct\": \"False\",\"explanation\": \"Enhanced networking on Windows Instances can be used to improve network performance, but is not the best option for this scenario\"}\n]\n}", "docs": ["('REL 2\\xa0 How do you plan your network topology? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-02.html')", "('PERF 5\\xa0 How do you configure your networking solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-05.html')", "('PERF 1\\xa0 How do you select the best performing architecture? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-01.html')", "('COST08-BP02 Select components to optimize data transfer cost - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_optimized_components.html')", "('Network - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-network.html')", "('PERF05-BP01 Understand how networking impacts performance - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_understand_impact.html')"]}, {"scenario": "A telecommunications company wants to use AWS to host and manage their network infrastructure.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 4: Determine high-performing and/or scalable network architectures", "focus": "Skills in", "item": "Determining network configurations that can scale to accommodate future needs", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA telecommunications company wants to use AWS to host and manage their network infrastructure.`\"\n\nContext:\n\n* A scalable workload provides elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time.Best practices\n* The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.Best practices\n* The way users consume your workloads and other resources can help you identify improvements to meet sustainability goals. Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed. Align service levels to customer needs. Position resources to limit the network required for users to consume them. Remove existing, unused assets. Identify created assets that are unused and stop generating them. Provide your team members with devices that support their needs with minimized sustainability impact.Best practices:\n* Workloads often exist in multiple environments. These include multiple cloud environments (both publicly accessible and private) and possibly your existing data center infrastructure. Plans must include network considerations such as intra- and inter-system connectivity, public IP address management, private IP address management, and domain name resolution.Best practices\n* Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Update deployed components as your needs change.Level of risk exposed if this best practice is not established: MediumEnable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.Scale using small increments for variable workloads.Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years.Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.Related documents:AWS Compute Optimizer DocumentationOperating Lambda: Performance optimizationAuto Scaling Documentation\n* Since the network is between all workload components, it can have great impacts, both positive and negative, on workload performance and behavior. There are also workloads that are heavily dependent on network performance such as High Performance Computing (HPC) where deep network understanding is important to increase cluster performance. You must determine the workload requirements for bandwidth, latency, jitter, and throughput.On AWS, networking is virtualized and is available in a number of different types and configurations. This makes it easier to match your networking methods with your needs. AWS offers product features (for example, Enhanced Networking, Amazon EBS-optimized instances, Amazon S3 transfer acceleration, and dynamic Amazon CloudFront) to optimize network traffic. AWS also offers networking features (for example, Amazon Route 53 latency routing, Amazon VPC endpoints, AWS Direct Connect, and AWS Global Accelerator) to reduce network distance or jitter.The following question focuses on these considerations for performance efficiency.You must consider location when deploying your network. You can choose to place resources close to where they will be used to reduce distance. Use networking metrics to make changes to networking configuration as the workload evolves. By taking advantage of Regions, placement groups, and edge services, you can significantly improve performance. Cloud based networks can be quickly re-built or modified, so evolving your network architecture over time is necessary to maintain performance efficiency.PERF 5: How do you configure your networking solution?The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.\n* Foundational requirements are those whose scope extends beyond a single workload or project. Before architecting any system, foundational requirements that influence reliability should be in place. For example, you must have sufficient network bandwidth to your data center.With AWS, most of these foundational requirements are already incorporated or can be addressed as needed. The cloud is designed to be nearly limitless, so it\u2019s the responsibility of AWS to satisfy the requirement for sufficient networking and compute capacity, leaving you free to change resource size and allocations on demand.The following questions focus on these considerations for reliability. (For a list of reliability questions and best practices, see the Appendix.).For cloud-based workload architectures, there are service quotas (which are also referred to as service limits). These quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations to protect services from abuse. Workloads often exist in multiple environments. You must monitor and manage these quotas for all workload environments. These include multiple cloud environments (both publicly accessible and private) and may include your existing data center infrastructure. Plans must include network considerations, such as intrasystem and intersystem connectivity, public IP address management, private IP address management, and domain name resolution.REL 1: How do you manage service quotas and constraints?For cloud-based workload architectures, there are service quotas (which are also referred to as service limits). These quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations so as to protect services from abuse. There are also resource constraints, for example, the rate that you can push bits down a fiber-optic cable, or the amount of storage on a physical disk.REL 2: How do you plan your network topology?Workloads often exist in multiple environments. These include multiple cloud environments (both publicly accessible and private) and possibly your existing data center infrastructure. Plans must include network considerations such as intra- and inter-system connectivity, public IP address management, private IP address management, and domain name resolution.\n\nKnowledge Area: Determining network configurations that can scale to accommodate future needs\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best way to plan a network topology that can scale to accommodate future needs?\",\n\"answer_choices\": [\n    {\"answer\": \"Use horizontal scaling to match user demand and deploy only the minimum resources required.\",\"is_correct\": \"true\",\"explanation\": \"Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Enable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease. Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years. Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.\"},\n    {\"answer\": \"Manually provision additional resources to meet increased demand.\",\"is_correct\": \"false\",\"explanation\": \"Manually provisioning additional resources is not an effective way to scale a network topology to meet future needs. Automation is the best way to scale a network topology to meet future needs.\"},\n    {\"answer\": \"Identify unused assets and remove them to free up resources.\",\"is_correct\": \"false\",\"explanation\": \"Removing existing, unused assets is one way to increase resource efficiency. It is not, however, the best way to scale a network topology to meet future needs. Automation is the best way to scale a network topology to meet future needs.\"},\n    {\"answer\": \"Align service levels to customer needs.\",\"is_correct\": \"false\",\"explanation\": \"Aligning service levels to customer needs is one way to increase resource efficiency. It is not, however, the best way to scale a network topology to meet future needs. Automation is the best way to scale a network topology to meet future needs.\"}\n]\n}", "docs": ["('REL 7\\xa0 How do you design your workload to adapt to changes in demand? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-07.html')", "('PERF 5\\xa0 How do you configure your networking solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-05.html')", "('SUS 2 How do you take advantage of user behavior patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-02.html')", "('REL 2\\xa0 How do you plan your network topology? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-02.html')", "('SUS05-BP01 Use the minimum amount of hardware to meet your needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a2.html')", "('Network - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-network.html')", "('Foundations - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-found.html')"]}, {"scenario": "A large corporation wants to move their on-premises compute to the cloud for increased scalability and security.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 4: Determine high-performing and/or scalable network architectures", "focus": "Skills in", "item": "Determining the appropriate placement of resources to meet business requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA large corporation wants to move their on-premises compute to the cloud for increased scalability and security.`\"\n\nContext:\n\n* The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.Best practices\n* The way users consume your workloads and other resources can help you identify improvements to meet sustainability goals. Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed. Align service levels to customer needs. Position resources to limit the network required for users to consume them. Remove existing, unused assets. Identify created assets that are unused and stop generating them. Provide your team members with devices that support their needs with minimized sustainability impact.Best practices:\n* Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number.Best practices\n* Manage benefits and risks to make informed decisions when determining where to focus efforts. For example, it may be beneficial to deploy a workload with unresolved issues so that significant new features can be made available to customers. It may be possible to mitigate associated risks, or it may become unacceptable to allow a risk to remain, in which case you will take action to address the risk.You might find that you want to emphasize a small subset of your priorities at some point in time. Use a balanced approach over the long term to ensure the development of needed capabilities and management of risk. Update your priorities as needs changeCommon anti-patterns:You have decided to include a library that does everything you need that one of your developers found on the internet. You have not evaluated the risks of adopting this library from an unknown source and do not know if it contains vulnerabilities or malicious code.You have decided to develop and deploy a new feature instead of fixing an existing issue. You have not evaluated the risks of leaving the issue in place until the feature is deployed and do not know what the impact will be on your customers.You have decided to not deploy a feature frequently requested by customers because of unspecified concerns from your compliance team.Benefits of establishing this best practice: Identifying the available benefits of your choices, and being aware of the risks to your organization, enables you to make informed decisions.Level of risk exposed if this best practice is not established: LowManage benefits and risks: Balance the benefits of decisions against the risks involved.Identify benefits: Identify benefits based on business goals, needs, and priorities. Examples include time-to-market, security, reliability, performance, and cost.Identify risks: Identify risks based on business goals, needs, and priorities. Examples include time-to-market, security, reliability, performance, and cost.Assess benefits against risks and make informed decisions: Determine the impact of benefits and risks based on goals, needs, and priorities of your key stakeholders, including business, development, and operations. Evaluate the value of the benefit against the probability of the risk being realized and the cost of its impact. For example, emphasizing speed-to-market over reliability might provide competitive advantage. However, it may result in reduced uptime if there are reliability issues.\n* Workloads often have cost requirements for operation. Use internal cost controls to select resource types and sizes based on predicted resource need.Determine which workload components could be replaced with fully managed services, such as managed databases, in-memory caches, and ETL services. Reducing your operational workload allows you to focus resources on business outcomes.For cost requirement best practices, refer to the Cost-Effective Resources section of the Cost Optimization Pillar whitepaper.Common anti-patterns:You only use one family of instances.You do not evaluate licensed solutions versus open-source solutionsYou only use block storage.You deploy common software on EC2 instances and Amazon EBS or ephemeral volumes that are available as a managed service.Benefits of establishing this best practice: Considering cost when making your selections will allow you to enable other investments.Level of risk exposed if this best practice is not established: MediumOptimize workload components to reduce cost: Right size workload components and enable elasticity to reduce cost and maximize component efficiency. Determine which workload components can be replaced with managed services when appropriate, such as managed databases, in-memory caches, and reverse proxies.Related documents:AWS Architecture CenterAWS Partner NetworkAWS Solutions LibraryAWS Knowledge CenterAWS Compute OptimizerRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)This is my ArchitectureOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:AWS SamplesAWS SDK ExamplesRightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n* Choose Regions where you will implement your workloads based on both your business requirements and sustainability goals.Best Practice:\n* Involve key stakeholders, including business, development, and operations teams, when determining where to focus efforts on internal customer needs. This will ensure that you have a thorough understanding of the operations support that is required to achieve business outcomes.Use your established priorities to focus your improvement efforts where they will have the greatest impact (for example, developing team skills, improving workload performance, reducing costs, automating runbooks, or enhancing monitoring). Update your priorities as needs change.Common anti-patterns:You have decided to change IP address allocations for your product teams, without consulting them, to make managing your network easier. You do not know the impact this will have on your product teams.You are implementing a new development tool but have not engaged your internal customers to find out if it is needed or if it is compatible with their existing practices.You are implementing a new monitoring system but have not contacted your internal customers to find out if they have monitoring or reporting needs that should be considered.Benefits of establishing this best practice: Evaluating and understanding internal customer needs will inform how you prioritize your efforts to deliver business value.Level of risk exposed if this best practice is not established: HighUnderstand business needs: Business success is enabled by shared goals and understanding across stakeholders including business, development, and operations teams.Review business goals, needs, and priorities of internal customers: Engage key stakeholders, including business, development, and operations teams, to discuss goals, needs, and priorities of internal customers. This ensures that you have a thorough understanding of the operational support that is required to achieve business and customer outcomes.Establish shared understanding: Establish shared understanding of the business functions of the workload, the roles of each of the teams in operating the workload, and how these factors support shared business goals across internal and external customers.Related documents:AWS Well-Architected Framework Concepts \u2013 Feedback loop\n\nKnowledge Area: Determining the appropriate placement of resources to meet business requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"A large corporation wants to move their on-premises compute to the cloud for increased scalability and security. Which of the following would be the best approach to determine the appropriate placement of resources to meet the company's business requirements?\", \"answer_choices\": [{\"answer\": \"Deploy resources on the cloud closest to the user base to reduce latency and improve performance.\",\"is_correct\": \"false\",\"explanation\": \"Although reducing latency is important, it should not be the only factor when considering the placement of resources. Other factors such as cost, throughput requirements, compliance requirements, and resource availability should also be taken into consideration.\"},{\"answer\": \"Deploy resources on the cloud that offers the least expensive services.\",\"is_correct\": \"false\",\"explanation\": \"Cost should be considered when determining the placement of resources, however it should not be the only factor. Other factors such as throughput requirements, latency, compliance requirements, and resource availability should also be taken into consideration.\"},{\"answer\": \"Involve key stakeholders, including business, development, and operations teams, to discuss goals, needs, and priorities of the company.\",\"is_correct\": \"true\",\"explanation\": \"Involving key stakeholders, such as the business, development, and operations teams, is critical when determining the placement of resources to meet business requirements. This will ensure that all factors, such as cost, throughput requirements, latency, compliance requirements, and resource availability, are taken into consideration.\"},{\"answer\": \"Deploy resources to the cloud that offers the most reliable services.\",\"is_correct\": \"false\",\"explanation\": \"Reliability is an important factor when considering the placement of resources, however it should not be the only factor. Other factors such as cost, throughput requirements, latency, compliance requirements, and resource availability should also be taken into consideration.\"}]}", "docs": ["('PERF 5\\xa0 How do you configure your networking solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-05.html')", "('SUS 2 How do you take advantage of user behavior patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-02.html')", "('COST 6\\xa0 How do you meet cost targets when you select resource type, size and number? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-06.html')", "('OPS01-BP07 Manage benefits and risks - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_priorities_manage_risk_benefit.html')", "('PERF01-BP03 Factor cost requirements into decisions - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_performing_architecture_cost.html')", "('SUS 1 How do you select Regions to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-01.html')", "('OPS01-BP02 Evaluate internal customer needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_priorities_int_cust_needs.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 4: Determine high-performing and/or scalable network architectures", "focus": "Skills in", "item": "Selecting the appropriate load balancing strategy", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* Distribute traffic across multiple resources or services to allow your workload to take advantage of the elasticity that the cloud provides. You can also use load balancing for offloading encryption termination to improve performance and to manage and route traffic effectively.When implementing a scale-out architecture where you want to use multiple instances for service content, you can use load balancers inside your Amazon VPC. AWS provides multiple models for your applications in the ELB service. Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers.Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. It is capable of handling millions of requests per second while maintaining ultra-low latencies, and it is optimized to handle sudden and volatile traffic patterns.Elastic Load Balancing provides integrated certificate management and SSL/TLS decryption, allowing you the flexibility to centrally manage the SSL settings of the load balancer and offload CPU intensive work from your workload.Common anti-patterns:You route all internet traffic through existing load balancers.You use generic TCP load balancing and making each compute node handle SSL encryption.Benefits of establishing this best practice: A load balancer handles the varying load of your application traffic in a single Availability Zone, or across multiple Availability Zones. Load balancers feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant.Level of risk exposed if this best practice is not established: HighUse the appropriate load balancer for your workload: Select the appropriate load balancer for your workload. If you must load balance HTTP requests, we recommend Application Load Balancer. For network and transport protocols (layer 4 \u2013 TCP, UDP) load balancing, and for extreme performance and low latency applications, we recommend Network Load Balancer. Application Load Balancers support HTTPS and Network Load Balancers support TLS encryption offloading.Enable offload of HTTPS or TLS encryption: Elastic Load Balancing includes integrated certificate management, user-authentication, and SSL/TLS decryption. It provides the flexibility to centrally manage TLS settings and offload CPU intensive workloads from your applications. Encrypt all HTTPS traffic as part of your load balancer deployment.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Where applicable, use multiple strategies to improve performance. For example, using strategies like caching data to prevent excessive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Common anti-patterns:You assume that workload performance is adequate if customers are not complaining.You only collect data on performance after you have made performance-related changes.Benefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and historical reference. You can create dashboards and perform metric math on your data to derive operational and utilization insights for your workloads as they change over time.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Related documents:Amazon Builders\u2019 LibraryBest Practices for Implementing Amazon ElastiCacheAWS Database CachingAmazon CloudWatch RUMDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)AWS purpose-built databases (DAT209-L)Optimize applications through Amazon CloudWatch RUMRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n\nKnowledge Area: Selecting the appropriate load balancing strategy\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best load balancing strategy for the retail company's e-commerce platform during peak shopping seasons?\", \n\"answer_choices\": [{\"answer\": \"Route all internet traffic through existing load balancers.\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern, since routing all traffic through existing load balancers can lead to performance degradation and increased latency.\"},{\"answer\": \"Use generic TCP load balancing and make each compute node handle SSL encryption.\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern, since it can lead to increased latency, CPU intensive load, and decreased performance.\"},{\"answer\": \"Application Load Balancer for HTTP and HTTPS traffic.\",\"is_correct\": \"True\",\"explanation\": \"Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers.\"},{\"answer\": \"Network Load Balancer for TCP traffic.\",\"is_correct\": \"True\",\"explanation\": \"Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required.\"}]}", "docs": ["('PERF05-BP04 Leverage load-balancing and encryption offloading - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_encryption_offload.html')", "('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('PERF08-BP05 Use various performance-related strategies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_implement_strategy.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Knowledge of", "item": "Data analytics and visualization services with appropriate use cases (for example, Amazon Athena, AWS Lake Formation, Amazon QuickSight)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Provide dashboards tailored to their target audiences (for example, internal technical teams, leadership, and customers) to communicate the current operating status of the business and provide metrics of interest.You can create dashboards using Amazon CloudWatch Dashboards on customizable home pages in the CloudWatch console. Using business intelligence services such as Amazon QuickSight you can create and publish interactive dashboards of your workload and operational health (for example, order rates, connected users, and transaction times). Create Dashboards that present system and business-level views of your metrics.Common anti-patterns:Upon request, you run a report on the current utilization of your application for management.During an incident, you are contacted every twenty minutes by a concerned system owner wanting to know if it is fixed yet.Benefits of establishing this best practice: By creating dashboards, you enable self-service access to information enabling your customers to informed themselves and determine if they need to take action.Level of risk exposed if this best practice is not established: MediumCommunicate status through dashboards: Provide dashboards tailored to their target audiences (for example, internal technical teams, leadership, and customers) to communicate the current operating status of the business and provide metrics of interest. Providing a self-service option for status information reduces the disruption of fielding requests for status by the operations team. Examples include Amazon CloudWatch dashboards, and AWS Health Dashboard.CloudWatch dashboards create and use customized metrics viewsRelated documents:Amazon QuickSightCloudWatch dashboards create and use customized metrics views\n* Collect log files and metrics histories and analyze these for broader trends and workload insights.Amazon CloudWatch Logs Insights supports a simple yet powerful query language that you can use to analyze log data. Amazon CloudWatch Logs also supports subscriptions that allow data to flow seamlessly to Amazon S3 where you can use or Amazon Athena to query the data. It also supports queries on a large array of formats. See Supported SerDes and Data Formats in the Amazon Athena User Guide for more information. For analysis of huge log file sets, you can run an Amazon EMR cluster to run petabyte-scale analyses.There are a number of tools provided by AWS Partners and third parties that allow for aggregation, processing, storage, and analytics. These tools include New Relic, Splunk, Loggly, Logstash, CloudHealth, and Nagios. However, outside generation of system and application logs is unique to each cloud provider, and often unique to each service.An often-overlooked part of the monitoring process is data management. You need to determine the retention requirements for monitoring data, and then apply lifecycle policies accordingly. Amazon S3 supports lifecycle management at the S3 bucket level. This lifecycle management can be applied differently to different paths in the bucket. Toward the end of the lifecycle, you can transition data to Amazon S3 Glacier for long-term storage, and then expiration after the end of the retention period is reached. The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.Level of risk exposed if this best practice is not established: MediumCloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs.Analyzing Log Data with CloudWatch Logs InsightsAmazon CloudWatch Logs Insights Sample QueriesUse Amazon CloudWatch Logs send logs to Amazon S3 where you can use or Amazon Athena to query the data.How do I analyze my Amazon S3 server access logs using Athena?Create an S3 lifecycle policy for your server access logs bucket. Configure the lifecycle policy to periodically remove log files. Doing so reduces the amount of data that Athena analyzes for each query.How Do I Create a Lifecycle Policy for an S3 Bucket?Related documents:Amazon CloudWatch Logs Insights Sample QueriesAnalyzing Log Data with CloudWatch Logs InsightsDebugging with Amazon CloudWatch Synthetics and AWS X-RayHow Do I Create a Lifecycle Policy for an S3 Bucket?How do I analyze my Amazon S3 server access logs using Athena?One Observability WorkshopThe Amazon Builders' Library: Instrumenting distributed systems for operational visibility\n* Perform regular, proactive reviews of metrics to identify trends and determine where appropriate responses are needed.You should aggregate log data from the execution of your operations activities and operations API calls, into a service such as CloudWatch Logs. Generate metrics from observations of necessary log content to gain insight into the performance of operations activities.On AWS, you can export your log data to Amazon S3 or send logs directly to Amazon S3 for long-term storage. Using AWS Glue, you can discover and prepare your log data in Amazon S3 for analytics, storing associated metadata in the AWSAWS Glue Data Catalog. Amazon Athena, through its native integration with AWS Glue, can then be used to analyze your log data, querying it using standard SQL. Using a business intelligence tool like Amazon QuickSight you can visualize, explore, and analyze your data.Common anti-patterns:Consistent delivery of new features is considered a key performance indicator. You have no method to measure how frequently deployments occur.You log deployments, rolled back deployments, patches, and rolled back patches to track you operations activities, but no one reviews the metrics.You have a recovery time objective to restore a lost database within fifteen minutes that was defined when the system was deployed and had no users. You now have ten thousand users and have been operating for two years. A recent restore took over two hours. This was not recorded and no one is aware.Benefits of establishing this best practice: By collecting and analyzing your operations metrics, you gain understanding of the health of your operations and can gain insight to trends that have may an impact on your operations or the achievement of your business outcomes.Level of risk exposed if this best practice is not established: HighCollect and analyze operations metrics: Perform regular proactive reviews of metrics to identify trends and determine where appropriate responses are needed.Using Amazon CloudWatch metricsAmazon CloudWatch metrics and dimensions referenceCollect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch AgentRelated documents:Amazon AthenaAmazon CloudWatch metrics and dimensions referenceAmazon QuickSightAWS GlueAWSAWS Glue Data CatalogCollect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch AgentUsing Amazon CloudWatch metrics\n\nKnowledge Area: Data analytics and visualization services with appropriate use cases (for example, Amazon Athena, AWS Lake Formation, Amazon QuickSight)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"In order to monitor the current operating status of its critical systems, a company wants to create dashboards tailored to their target audiences. Which AWS service should the company use to create these dashboards?\", \"answer_choices\": [{\"answer\": \"Amazon CloudWatch Logs Insights\",\"is_correct\": \"false\",\"explanation\": \"Amazon CloudWatch Logs Insights enables you to interactively search and analyze log data in Amazon CloudWatch Logs, but does not allow for the creation of dashboards.\"},{\"answer\": \"Amazon EMR cluster\",\"is_correct\": \"false\",\"explanation\": \"Amazon EMR cluster provides a managed Hadoop framework to analyze huge log file sets, but does not allow for the creation of dashboards.\"},{\"answer\": \"Amazon QuickSight\",\"is_correct\": \"false\",\"explanation\": \"Amazon QuickSight allows you to create and publish interactive dashboards of your workload and operational health, but would not be the primary service to use for creating dashboards.\"},{\"answer\": \"Amazon CloudWatch Dashboards\",\"is_correct\": \"true\",\"explanation\": \"Amazon CloudWatch Dashboards can be used to create and use customized metrics views tailored to their target audiences, such as internal technical teams, leadership, and customers, to communicate the current operating status of the business and provide metrics of interest.\"}]}", "docs": ["('OPS10-BP06 Communicate status through dashboards - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_event_response_dashboards.html')", "('REL06-BP05 Analytics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_monitor_aws_resources_storage_analytics.html')", "('OPS09-BP03 Collect and analyze operations metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_operations_health_collect_analyze_ops_metrics.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Knowledge of", "item": "Data ingestion patterns (for example, frequency)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Establish patterns of workload activity to identify anomalous behavior so that you can respond appropriately if required.CloudWatch through the CloudWatch Anomaly Detection feature applies statistical and machine learning algorithms to generate a range of expected values that represent normal metric behavior.Amazon DevOps Guru can be used to identify anomalous behavior through event correlation, log analysis, and applying machine learning to analyze your workload telemetry. When unexpected behaviors are detected, it provides the related metrics and events with recommendations to address the behavior.Common anti-patterns:You are reviewing network utilization logs and see that network utilization increased between 11:30am and 1:30pm and then again at 4:30pm through 6:00pm. You are unaware if this should be considered normal or not.Your web servers reboot every night at 3:00am. You are unaware if this is an expected behavior.Benefits of establishing this best practice: By learning patterns of behavior you can recognize unexpected behavior and take action if necessary.Level of risk exposed if this best practice is not established: MediumLearn expected patterns of activity for workload: Establish patterns of workload activity to determine when behavior is outside of the expected values so that you can respond appropriately if required.Related documents:Amazon DevOps GuruCloudWatch Anomaly Detection\n* Use performance characteristics and access patterns that optimize how data is stored or queried to achieve the best possible performance. Measure how optimizations such as indexing, key distribution, data warehouse design, or caching strategies impact system performance or overall efficiency.Common anti-patterns:You only use manual log file searching for metrics.You only publish metrics to internal tools.Benefits of establishing this best practice: In order to ensure you are meeting the metrics required for the workload, you must monitor database performance metrics related to both reads and writes. You can use this data to add new optimizations for both reads and writes to the data storage layer.Level of risk exposed if this best practice is not established: LowOptimize data storage based on metrics and patterns: Use reported metrics to identify any underperforming areas in your workload and optimize your database components. Each database system has different performance related characteristics to evaluate, such as how data is indexed, cached, or distributed among multiple systems. Measure the impact of your optimizations.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAnalyzing performance anomalies with DevOps Guru for RDSRead/Write Capacity Mode for DynamoDBRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Hands-on Labs for Amazon DynamoDB\n\nKnowledge Area: Data ingestion patterns (for example, frequency)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following would be the best approach to identify anomalous behavior related to data ingestion patterns in the healthcare company's cloud environment?\",\"answer_choices\": [{\"answer\": \"Use manual log file searching for metrics\",\"is_correct\": \"false\",\"explanation\": \"Manual log file searching does not provide enough information to identify anomalous behavior related to data ingestion patterns.\"},{\"answer\": \"Utilize CloudWatch Anomaly Detection feature\",\"is_correct\": \"true\",\"explanation\": \"The CloudWatch Anomaly Detection feature applies statistical and machine learning algorithms to generate a range of expected values that represent normal metric behavior and can be used to identify anomalous behavior.\"},{\"answer\": \"Publish metrics to internal tools\",\"is_correct\": \"false\",\"explanation\": \"Publishing metrics to an internal tool does not provide enough information to identify anomalous behavior related to data ingestion patterns.\"},{\"answer\": \"Use the Amazon DevOps Guru feature\",\"is_correct\": \"false\",\"explanation\": \"Amazon DevOps Guru can be used to identify anomalous behavior, but it is not specifically tailored to data ingestion patterns.\"}]}", "docs": ["('OPS08-BP05 Learn expected patterns of activity for workload - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_workload_health_learn_workload_usage_patterns.html')", "('PERF04-BP05 Optimize data storage based on access patterns and metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_optimize_metrics.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Knowledge of", "item": "Data transfer services with appropriate use cases (for example, AWS DataSync, AWS Storage Gateway)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Implement services to reduce data transfer. For example, using a content delivery network (CDN) such as Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using AWS Direct Connect instead of VPN for connectivity to AWS.Level of risk exposed if this best practice is not established: LowAmazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources. By using CloudFront, you can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network. It is ideal for small offices or business partners because it provides quick and easy connectivity, and it is a fully managed and elastic service.VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs. Gateway VPC endpoints have no hourly charges, and support Amazon Simple Storage Service(Amazon S3) and Amazon DynamoDB. Interface VPC endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost.Implementation stepsImplement services: Using the data transfer modeling, look at where the largest costs and highest volume flows are. Review the AWS services and assess whether there is a service that reduces or removes the transfer, specifically networking and content delivery. Also look for caching services where there is repeated access to data, or large amounts of data.Related documents:AWS Direct ConnectAWS Explore Our ProductsAWS caching solutionsAmazon CloudFrontDeliver content faster with Amazon CloudFront\n* Use shared storage and access data from regional data stores to minimize the total networking resources required to support data movement for your workload.Level of risk exposed if this best practice is not established: LowStore data as close to the consumer as possible.Partition regionally consumed services so that their Region-specific data is stored within the Region where it is consumed.Use block-level duplication instead of file or object-level duplication when copying changes across the network.Compress data before moving it over the network.Related documents:Optimizing your AWS Infrastructure for Sustainability, Part III: NetworkingAWS Global InfrastructureAmazon CloudFront Key Features including the CloudFront Global Edge NetworkCompressing HTTP requests in Amazon OpenSearch ServiceIntermediate data compression with Amazon EMRLoading compressed data files from Amazon S3 into Amazon RedshiftServing compressed files with Amazon CloudFront\n\nKnowledge Area: Data transfer services with appropriate use cases (for example, AWS DataSync, AWS Storage Gateway)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which services should a healthcare company use to reduce data transfer for storing and processing sensitive patient information in the cloud?\", \n\"answer_choices\": [\n{\"answer\": \"AWS DataSync\", \"is_correct\": \"False\", \"explanation\": \"AWS DataSync is used to automate and accelerate data transfer between on-premises storage systems and AWS Storage Gateway, not reduce data transfer.\"},\n{\"answer\": \"Amazon CloudFront\", \"is_correct\": \"True\", \"explanation\": \"Amazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources.\"}, \n{\"answer\": \"AWS Direct Connect\", \"is_correct\": \"True\", \"explanation\": \"AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.\"},\n{\"answer\": \"VPC Endpoints\", \"is_correct\": \"True\", \"explanation\": \"VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs.\" }\n]}", "docs": ["('COST08-BP03 Implement services to reduce data transfer costs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_implement_services.html')", "('SUS04-BP07 Minimize data movement across networks - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a8.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Knowledge of", "item": "Data transformation services with appropriate use cases (for example, AWS Glue)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.Level of risk exposed if this best practice is not established: LowAnalyze your data access and storage patterns.Store data files in efficient file formats such as Parquet to prevent unnecessary processing (for example, when running analytics) and to reduce the total storage provisioned.Use technologies that work natively with compressed data.Use the database engine that best supports your dominant query pattern.Manage your database indexes to ensure index designs support efficient query execution.Select network protocols that reduce the amount of network capacity consumed.Related documents:Athena Compression Support file formatsCOPY from columnar data formats with Amazon RedshiftConverting Your Input Record Format in Kinesis Data FirehoseFormat Options for ETL Inputs and Outputs in AWS GlueImprove query performance on Amazon Athena by Converting to Columnar FormatsLoading compressed data files from Amazon S3 with Amazon RedshiftMonitoring DB load with Performance Insights on Amazon AuroraMonitoring DB load with Performance Insights on Amazon RDSAWS IoT FleetWiseRelated videos:Building Sustainably on AWS\n* Perform regular, proactive reviews of metrics to identify trends and determine where appropriate responses are needed.You should aggregate log data from the execution of your operations activities and operations API calls, into a service such as CloudWatch Logs. Generate metrics from observations of necessary log content to gain insight into the performance of operations activities.On AWS, you can export your log data to Amazon S3 or send logs directly to Amazon S3 for long-term storage. Using AWS Glue, you can discover and prepare your log data in Amazon S3 for analytics, storing associated metadata in the AWSAWS Glue Data Catalog. Amazon Athena, through its native integration with AWS Glue, can then be used to analyze your log data, querying it using standard SQL. Using a business intelligence tool like Amazon QuickSight you can visualize, explore, and analyze your data.Common anti-patterns:Consistent delivery of new features is considered a key performance indicator. You have no method to measure how frequently deployments occur.You log deployments, rolled back deployments, patches, and rolled back patches to track you operations activities, but no one reviews the metrics.You have a recovery time objective to restore a lost database within fifteen minutes that was defined when the system was deployed and had no users. You now have ten thousand users and have been operating for two years. A recent restore took over two hours. This was not recorded and no one is aware.Benefits of establishing this best practice: By collecting and analyzing your operations metrics, you gain understanding of the health of your operations and can gain insight to trends that have may an impact on your operations or the achievement of your business outcomes.Level of risk exposed if this best practice is not established: HighCollect and analyze operations metrics: Perform regular proactive reviews of metrics to identify trends and determine where appropriate responses are needed.Using Amazon CloudWatch metricsAmazon CloudWatch metrics and dimensions referenceCollect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch AgentRelated documents:Amazon AthenaAmazon CloudWatch metrics and dimensions referenceAmazon QuickSightAWS GlueAWSAWS Glue Data CatalogCollect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch AgentUsing Amazon CloudWatch metrics\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Perform regular proactive reviews of metrics to identify trends and determine where appropriate responses are needed.You should aggregate log data from your application, workload components, services, and API calls to a service such as CloudWatch Logs. Generate metrics from observations of necessary log content to enable insight into the performance of operations activities.On AWS, you can analyze workload metrics and identify operational issues using the machine learning capabilities of Amazon DevOps Guru. AWS DevOps Guru provides notification of operational issues with targeted and proactive recommendations to resolve issues and maintain application health.In the AWS Shared Responsibility Model, portions of monitoring are delivered to you through the AWS Health Dashboard. This dashboard provides alerts and remediation guidance when AWS is experiencing events that might affect you. Customers with Business and Enterprise Support subscriptions also get access to the AWS Health API, enabling integration to their event management systems.On AWS, you can export your log data to Amazon S3 or send logs directly to Amazon S3 for long-term storage. Using AWS Glue, you can discover and prepare your log data in Amazon S3 for analytics, storing associated metadata in the AWSAWS Glue Data Catalog. Amazon Athena, through its native integration with AWS Glue, can then be used to analyze your log data, querying it using standard SQL. Using a business intelligence tool like Amazon QuickSight you can visualize, explore, and analyze your data.An alternative solution would be to use the Amazon OpenSearch Service and OpenSearch Dashboards to collect, analyze, and display logs on AWS across multiple accounts and AWS Regions.Common anti-patterns:You are asked by the network design team for current network bandwidth utilization rates. You provide the current metrics, network utilization is at 35%. They reduce circuit capacity as a cost savings measure causing widespread connectivity issues as your point-in-time measurement did not reflect the trend in utilization rates.Your router has failed. It has been logging non-critical memory errors with greater and greater frequency up until its complete failure. You did not detect this trend and as a result did not replace the faulty memory before the router caused a service interruption.Benefits of establishing this best practice: By collecting and analyzing your workload metrics you gain understanding of the health of your workload and can gain insight to trends that may have an impact on your workload or the achievement of your business outcomes.Level of risk exposed if this best practice is not established: HighCollect and analyze workload metrics: Perform regular proactive reviews of metrics to identify trends and determine where appropriate responses are needed.Using Amazon CloudWatch metricsAmazon CloudWatch metrics and dimensions referenceCollect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch AgentRelated documents:Amazon AthenaAmazon CloudWatch metrics and dimensions referenceAmazon DevOps GuruAWS GlueAWSAWS Glue Data CatalogAmazon OpenSearch ServiceAWS Health DashboardAmazon QuickSightCollect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch AgentUsing Amazon CloudWatch metrics\n\nKnowledge Area: Data transformation services with appropriate use cases (for example, AWS Glue)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"In order to store large amounts of data for their applications, which AWS service should the media company use most effectively?\",\n\"answer_choices\": [{\"answer\": \"Amazon S3\",\"is_correct\": \"true\",\"explanation\": \"Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. It is the most suitable service for the media company to store large amounts of data for their applications.\"},\n{\"answer\": \"Amazon EC2\",\"is_correct\": \"false\",\"explanation\": \"Amazon EC2 is a web service that provides secure, resizable compute capacity in the cloud. It is not the most suitable service for the media company to store large amounts of data for their applications.\"},{\"answer\": \"AWS Glue\",\"is_correct\": \"false\",\"explanation\": \"AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load data for analytics. It is not the most suitable service for the media company to store large amounts of data for their applications.\"},{\"answer\": \"Amazon CloudWatch\",\"is_correct\": \"false\",\"explanation\": \"Amazon CloudWatch is a monitoring and observability service that provides you with data and actionable insights to monitor your applications, understand and respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. It is not the most suitable service for the media company to store large amounts of data for their applications.\"}]}", "docs": ["('SUS03-BP05 Use software patterns and architectures that best support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a6.html')", "('OPS09-BP03 Collect and analyze operations metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_operations_health_collect_analyze_ops_metrics.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('OPS08-BP03 Collect and analyze workload metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_workload_health_collect_analyze_workload_metrics.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Knowledge of", "item": "Secure access to ingestion access points", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* Keep all users away from directly accessing sensitive data and systems under normal operational circumstances. For example, use a change management workflow to manage Amazon Elastic Compute Cloud (Amazon EC2) instances using tools instead of allowing direct access or a bastion host. This can be achieved using AWS Systems Manager Automation, which uses automation documents that contain steps you use to perform tasks. These documents can be stored in source control, be peer reviewed before running, and tested thoroughly to minimize risk compared to shell access. Business users could have a dashboard instead of direct access to a data store to run queries. Where CI/CD pipelines are not used, determine which controls and processes are required to adequately provide a normally disabled break-glass access mechanism.Level of risk exposed if this best practice is not established: LowImplement mechanisms to keep people away from data: Mechanisms include using dashboards, such as Amazon QuickSight, to display data to users instead of directly querying.Amazon QuickSightAutomate configuration management: Perform actions at a distance, enforce and validate secure configurations automatically by using a configuration management service or tool. Avoid use of bastion hosts or directly accessing EC2 instances.AWS Systems ManagerAWS CloudFormationCI/CD Pipeline for AWS CloudFormation templates on AWSRelated documents:AWS KMS Cryptographic Details WhitepaperRelated videos:How Encryption Works in AWSSecuring Your Block Storage on AWS\n* Each component or resource of your workload needs to be accessed by administrators, end users, or other components. Have a clear definition of who or what should have access to each component, choose the appropriate identity type and method of authentication and authorization.Common anti-patterns:Hard-coding or storing secrets in your application.Granting custom permissions for each user.Using long-lived credentials.Level of risk exposed if this best practice is not established: HighEach component or resource of your workload needs to be accessed by administrators, end users, or other components. Have a clear definition of who or what should have access to each component, choose the appropriate identity type and method of authentication and authorization.Regular access to AWS accounts within the organization should be provided using federated access or a centralized identity provider. You should also centralize your identity management and ensure that there is an established practice to integrate AWS access to your employee access lifecycle. For example, when an employee changes to a job role with a different access level, their group membership should also change to reflect their new access requirements.When defining access requirements for non-human identities, determine which applications and components need access and how permissions are granted. Using IAM roles built with the least privilege access model is a recommended approach. AWS Managed policies provide predefined IAM policies that cover most common use cases.AWS services, such as AWS Secrets Manager and AWS Systems Manager Parameter Store, can help decouple secrets from the application or workload securely in cases where it's not feasible to use IAM roles. In Secrets Manager, you can establish automatic rotation for your credentials. You can use Systems Manager to reference parameters in your scripts, commands, SSM documents, configuration, and automation workflows by using the unique name that you specified when you created the parameter.You can use AWS Identity and Access Management Roles Anywhere to obtain temporary security credentials in IAM for workloads that run outside of AWS. Your workloads can use the same IAM policies and IAM roles that you use with AWS applications to access AWS resources.Where possible, prefer short-term temporary credentials over long-term static credentials. For scenarios in which you need IAM users with programmatic access and long-term credentials, use access key last used information to rotate and remove access keys.Related documents:Attribute-based access control (ABAC)AWS IAM Identity Center (successor to AWS Single Sign-On)IAM Roles AnywhereAWS Managed policies for IAM Identity CenterAWS IAM policy conditionsIAM use casesRemove unnecessary credentialsWorking with PoliciesHow to control access to AWS resources based on AWS account, OU, or organizationIdentify, arrange, and manage secrets easily using enhanced search in AWS Secrets ManagerRelated videos:Become an IAM Policy Master in 60 Minutes or LessSeparation of Duties, Least Privilege, Delegation, and CI/CDStreamlining identity and access management for innovation\n* Enforce access control with least privileges and mechanisms, including backups, isolation, and versioning, to help protect your data at rest. Prevent operators from granting public access to your data.Different controls including access (using least privilege), backups (see Reliability whitepaper), isolation, and versioning can all help protect your data at rest. Access to your data should be audited using detective mechanisms covered earlier in this paper including CloudTrail, and service level log, such as Amazon Simple Storage Service (Amazon S3) access logs. You should inventory what data is publicly accessible, and plan for how you can reduce the amount of data available over time. Amazon S3 Glacier Vault Lock and Amazon S3 Object Lock are capabilities providing mandatory access control\u2014once a vault policy is locked with the compliance option, not even the root user can change it until the lock expires. The mechanism meets the Books and Records Management requirements of the SEC, CFTC, and FINRA. For more details, see this whitepaper.Level of risk exposed if this best practice is not established: LowEnforce access control: Enforce access control with least privileges, including access to encryption keys.Introduction to Managing Access Permissions to Your Amazon S3 ResourcesSeparate data based on different classification levels: Use different AWS accounts for data classification levels managed by AWS Organizations.AWS OrganizationsReview AWS KMS policies: Review the level of access granted in AWS KMS policies.Overview of managing access to your AWS KMS resourcesReview Amazon S3 bucket and object permissions: Regularly review the level of access granted in Amazon S3 bucket policies. Best practice is to not have publicly readable or writeable buckets. Consider using AWS Config to detect buckets that are publicly available, and Amazon CloudFront to serve content from Amazon S3.AWS Config RulesAmazon S3 + Amazon CloudFront: A Match Made in the CloudEnable Amazon S3 versioning and object lock.Using versioningLocking Objects Using Amazon S3 Object LockUse Amazon S3 Inventory: Amazon S3 inventory is one of the tools you can use to audit and report on the replication and encryption status of your objects.Amazon S3 InventoryReview Amazon EBS and AMI sharing permissions: Sharing permissions can allow images and volumes to be shared to AWS accounts external to your workload.Sharing an Amazon EBS SnapshotShared AMIsRelated documents:AWS KMS Cryptographic Details WhitepaperRelated videos:Securing Your Block Storage on AWS\n\nKnowledge Area: Secure access to ingestion access points\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best approach to secure access to ingestion access points and keep users away from directly accessing sensitive data and systems under normal operational circumstances for the retail company in the scenario?\", \n\"answer_choices\": [{\"answer\": \"Use Amazon QuickSight to display data to users instead of directly querying.\",\"is_correct\": \"False\",\"explanation\": \"Amazon QuickSight is a dashboard to display data to users, but it does not secure access to ingestion access points.\"},\n{\"answer\": \"Perform actions at a distance and enforce and validate secure configurations automatically using a configuration management service or tool.\",\"is_correct\": \"True\",\"explanation\": \"Using a configuration management service or tool is the best approach to secure access to ingestion access points and keep users away from directly accessing sensitive data and system.\"},\n{\"answer\": \"Grant custom permissions for each user.\",\"is_correct\": \"False\",\"explanation\": \"Granting custom permissions for each user is an anti-pattern and a low level of risk is exposed if this best practice is not established.\"},\n{\"answer\": \"Use long-lived credentials.\",\"is_correct\": \"False\",\"explanation\": \"Using long-lived credentials is an anti-pattern and a high level of risk is exposed if this best practice is not established.\"}]}", "docs": ["('SEC08-BP05 Use mechanisms to keep people away from data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_use_people_away.html')", "('SEC03-BP01 Define access requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_define.html')", "('SEC08-BP04 Enforce access control - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_access_control.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Knowledge of", "item": "Sizes and speeds needed to meet business requirements", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.Best practices\n* A scalable workload provides elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time.Best practices\n* The way users consume your workloads and other resources can help you identify improvements to meet sustainability goals. Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed. Align service levels to customer needs. Position resources to limit the network required for users to consume them. Remove existing, unused assets. Identify created assets that are unused and stop generating them. Provide your team members with devices that support their needs with minimized sustainability impact.Best practices:\n* Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO).Best practices\n* Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number.Best practices\n* Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time.Best practices\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Use metrics from the currently running workload to select the right size and type to optimize for cost. Appropriately provision throughput, sizing, and storage for services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Elastic Block Store (Amazon EBS) (PIOPS), Amazon Relational Database Service (Amazon RDS), Amazon EMR, and networking. This can be done with a feedback loop such as automatic scaling or by custom code in the workload.Level of risk exposed if this best practice is not established: LowCreate a feedback loop within the workload that uses active metrics from the running workload to make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you configure to perform the right sizing operations for you. AWS also provides APIs, SDKs, and features that allow resources to be modified with minimal effort. You can program a workload to stop-and-start an Amazon Elastic Compute Cloud(Amazon EC2) instance to allow a change of instance size or instance type. This provides the benefits of right-sizing while removing almost all the operational cost required to make the change.Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage Service(Amazon S3) Intelligent-Tiering. Amazon S3 Intelligent-Tiering automatically moves your data between two access tiers: frequent access and infrequent access, based on your usage patterns.Implementation stepsConfigure workload metrics: Ensure you capture the key metrics for the workload. These metrics provide an indication of the customer experience, such as the workload output, and align to the differences between resource types and sizes, such as CPU and memory usage.View rightsizing recommendations: Use the rightsizing recommendations in AWS Compute Optimizer to make adjustments to your workload.Select resource type and size automatically based on metrics: Using the workload metrics, manually or automatically select your workload resources. Configuring AWS Auto Scaling or implementing code within your application can reduce the effort required if frequent changes are needed, and it can potentially implement changes sooner than a manual process.Related documents:AWS Auto ScalingAWS Compute OptimizerAmazon CloudWatch featuresCloudWatch Getting Set UpCloudWatch Publishing Custom MetricsCost Optimization: Amazon EC2 Right SizingGetting Started with Amazon EC2 Auto ScalingAmazon S3 Intelligent-TieringLaunch an EC2 Instance Using the SDK\n* Foundational requirements are those whose scope extends beyond a single workload or project. Before architecting any system, foundational requirements that influence reliability should be in place. For example, you must have sufficient network bandwidth to your data center.With AWS, most of these foundational requirements are already incorporated or can be addressed as needed. The cloud is designed to be nearly limitless, so it\u2019s the responsibility of AWS to satisfy the requirement for sufficient networking and compute capacity, leaving you free to change resource size and allocations on demand.The following questions focus on these considerations for reliability. (For a list of reliability questions and best practices, see the Appendix.).For cloud-based workload architectures, there are service quotas (which are also referred to as service limits). These quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations to protect services from abuse. Workloads often exist in multiple environments. You must monitor and manage these quotas for all workload environments. These include multiple cloud environments (both publicly accessible and private) and may include your existing data center infrastructure. Plans must include network considerations, such as intrasystem and intersystem connectivity, public IP address management, private IP address management, and domain name resolution.REL 1: How do you manage service quotas and constraints?For cloud-based workload architectures, there are service quotas (which are also referred to as service limits). These quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations so as to protect services from abuse. There are also resource constraints, for example, the rate that you can push bits down a fiber-optic cable, or the amount of storage on a physical disk.REL 2: How do you plan your network topology?Workloads often exist in multiple environments. These include multiple cloud environments (both publicly accessible and private) and possibly your existing data center infrastructure. Plans must include network considerations such as intra- and inter-system connectivity, public IP address management, private IP address management, and domain name resolution.\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n\nKnowledge Area: Sizes and speeds needed to meet business requirements\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What steps are required to select the optimal resource size or type based on a workload's requirements in order to minimize cost?\",\n\"answer_choices\": [\n{\"answer\": \"Use metrics from the currently running workload to select the right size and type to optimize for cost.\",\"is_correct\": \"true\",\"explanation\": \"Using active metrics from the running workload to make changes to that workload is the most effective way to select the optimal resource size or type for a workload's requirements.\"},\n{\"answer\": \"Use cost modeling to select the required resource type and size.\",\"is_correct\": \"false\",\"explanation\": \"Cost modeling can be used to select the expected workload usage level, but it is not the best way to select the optimal resource size or type.\"},\n{\"answer\": \"Use a feedback loop such as automatic scaling.\",\"is_correct\": \"true\",\"explanation\": \"A feedback loop can be used to ensure that resources closely match the current demand at any given point in time, providing the benefits of right-sizing while removing almost all the operational cost required to make the change.\"},\n{\"answer\": \"Select resources based on documentation.\",\"is_correct\": \"false\",\"explanation\": \"Documentation can be used to gain information about the workload, but it is not the best way to select the optimal resource size or type.\"}\n]}", "docs": ["('PERF 5\\xa0 How do you configure your networking solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-05.html')", "('REL 7\\xa0 How do you design your workload to adapt to changes in demand? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-07.html')", "('SUS 2 How do you take advantage of user behavior patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-02.html')", "('REL 9\\xa0 How do you back up data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-09.html')", "('COST 6\\xa0 How do you meet cost targets when you select resource type, size and number? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-06.html')", "('COST 8\\xa0 How do you plan for data transfer charges? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-08.html')", "('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('COST06-BP03 Select resource type, size, and number automatically based on metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_metrics.html')", "('Foundations - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-found.html')", "('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Knowledge of", "item": "Streaming data services with appropriate use cases (for example, Amazon Kinesis)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.Level of risk exposed if this best practice is not established: LowAnalyze your data access and storage patterns.Store data files in efficient file formats such as Parquet to prevent unnecessary processing (for example, when running analytics) and to reduce the total storage provisioned.Use technologies that work natively with compressed data.Use the database engine that best supports your dominant query pattern.Manage your database indexes to ensure index designs support efficient query execution.Select network protocols that reduce the amount of network capacity consumed.Related documents:Athena Compression Support file formatsCOPY from columnar data formats with Amazon RedshiftConverting Your Input Record Format in Kinesis Data FirehoseFormat Options for ETL Inputs and Outputs in AWS GlueImprove query performance on Amazon Athena by Converting to Columnar FormatsLoading compressed data files from Amazon S3 with Amazon RedshiftMonitoring DB load with Performance Insights on Amazon AuroraMonitoring DB load with Performance Insights on Amazon RDSAWS IoT FleetWiseRelated videos:Building Sustainably on AWS\n* Collect log files and metrics histories and analyze these for broader trends and workload insights.Amazon CloudWatch Logs Insights supports a simple yet powerful query language that you can use to analyze log data. Amazon CloudWatch Logs also supports subscriptions that allow data to flow seamlessly to Amazon S3 where you can use or Amazon Athena to query the data. It also supports queries on a large array of formats. See Supported SerDes and Data Formats in the Amazon Athena User Guide for more information. For analysis of huge log file sets, you can run an Amazon EMR cluster to run petabyte-scale analyses.There are a number of tools provided by AWS Partners and third parties that allow for aggregation, processing, storage, and analytics. These tools include New Relic, Splunk, Loggly, Logstash, CloudHealth, and Nagios. However, outside generation of system and application logs is unique to each cloud provider, and often unique to each service.An often-overlooked part of the monitoring process is data management. You need to determine the retention requirements for monitoring data, and then apply lifecycle policies accordingly. Amazon S3 supports lifecycle management at the S3 bucket level. This lifecycle management can be applied differently to different paths in the bucket. Toward the end of the lifecycle, you can transition data to Amazon S3 Glacier for long-term storage, and then expiration after the end of the retention period is reached. The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.Level of risk exposed if this best practice is not established: MediumCloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs.Analyzing Log Data with CloudWatch Logs InsightsAmazon CloudWatch Logs Insights Sample QueriesUse Amazon CloudWatch Logs send logs to Amazon S3 where you can use or Amazon Athena to query the data.How do I analyze my Amazon S3 server access logs using Athena?Create an S3 lifecycle policy for your server access logs bucket. Configure the lifecycle policy to periodically remove log files. Doing so reduces the amount of data that Athena analyzes for each query.How Do I Create a Lifecycle Policy for an S3 Bucket?Related documents:Amazon CloudWatch Logs Insights Sample QueriesAnalyzing Log Data with CloudWatch Logs InsightsDebugging with Amazon CloudWatch Synthetics and AWS X-RayHow Do I Create a Lifecycle Policy for an S3 Bucket?How do I analyze my Amazon S3 server access logs using Athena?One Observability WorkshopThe Amazon Builders' Library: Instrumenting distributed systems for operational visibility\n* Managed services shift responsibility for maintaining high-average utilization, and sustainability optimization of the deployed hardware to AWS. Use managed services to distribute the sustainability impact of the service across all tenants of the service, reducing your individual contribution.Level of risk exposed if this best practice is not established: LowMigrate from self-hosted services to managed services. For example, use managed Amazon Relational Database Service (Amazon RDS) instances instead of maintaining your own Amazon RDS instances on Amazon Elastic Compute Cloud (Amazon EC2), or use managed container services, such as AWS Fargate, instead of implementing your own container infrastructure.Related documents:AWS FargateAmazon DocumentDBAmazon Elastic Kubernetes Service (EKS)Amazon Managed Streaming for Apache Kafka (Amazon MSK)Amazon RedshiftAmazon Relational Database Service (RDS)\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n\nKnowledge Area: Streaming data services with appropriate use cases (for example, Amazon Kinesis)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS managed service should be used to store log files from an application for analysis and compliance purposes?\",\n\"answer_choices\": [\n    {\"answer\": \"Amazon Kinesis\",\"is_correct\": \"False\",\"explanation\": \"Amazon Kinesis is a streaming data service and is not suitable for storing log files.\"},\n    {\"answer\": \"Amazon S3\",\"is_correct\": \"True\",\"explanation\": \"Amazon S3 is an object storage service and is suitable for storing log files for analysis and compliance purposes.\"},\n    {\"answer\": \"Amazon Glacier\",\"is_correct\": \"False\",\"explanation\": \"Amazon Glacier is an archival storage service and is suitable for infrequently accessed data, but not suitable for storing log files.\"},\n    {\"answer\": \"Amazon Redshift\",\"is_correct\": \"False\",\"explanation\": \"Amazon Redshift is a data warehousing service and is not suitable for storing log files.\"}\n]}", "docs": ["('SUS03-BP05 Use software patterns and architectures that best support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a6.html')", "('REL06-BP05 Analytics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_monitor_aws_resources_storage_analytics.html')", "('SUS05-BP03 Use managed services - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a4.html')", "('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Skills in", "item": "Building and securing data lakes", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* Before architecting any system, foundational practices that influence security should be in place. For example, data classification provides a way to categorize organizational data based on levels of sensitivity, and encryption protects data by way of rendering it unintelligible to unauthorized access. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.In AWS, the following practices facilitate protection of data:As an AWS customer you maintain full control over your data.AWS makes it easier for you to encrypt your data and manage keys, including regular key rotation, which can be easily automated by AWS or maintained by you.Detailed logging that contains important content, such as file access and changes, is available.AWS has designed storage systems for exceptional resiliency. For example, Amazon S3 Standard, S3 Standard\u2013IA, S3 One Zone-IA, and Amazon Glacier are all designed to provide 99.999999999% durability of objects over a given year. This durability level corresponds to an average annual expected loss of 0.000000001% of objects.Versioning, which can be part of a larger data lifecycle management process, can protect against accidental overwrites, deletes, and similar harm.AWS never initiates the movement of data between Regions. Content placed in a Region will remain in that Region unless you explicitly enable a feature or leverage a service that provides that functionality.The following questions focus on these considerations for security.AWS provides multiple means for encrypting data at rest and in transit. We build features into our services that make it easier to encrypt your data. For example, we have implemented server-side encryption (SSE) for Amazon S3 to make it easier for you to store your data in an encrypted form. You can also arrange for the entire HTTPS encryption and decryption process (generally known as SSL termination) to be handled by Elastic Load Balancing (ELB).SEC 7: How do you classify your data?Classification provides a way to categorize data, based on criticality and sensitivity in order to help you determine appropriate protection and retention controls.SEC 8: How do you protect your data at rest?Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access or mishandling.SEC 9: How do you protect your data in transit?Protect your data in transit by implementing multiple controls to reduce the risk of unauthorized access or loss.\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n* Your defined lifecycle strategy should be based on sensitivity level as well as legal and organization requirements. Aspects including the duration for which you retain data, data destruction processes, data access management, data transformation, and data sharing should be considered. When choosing a data classification methodology, balance usability versus access. You should also accommodate the multiple levels of access and nuances for implementing a secure, but still usable, approach for each level. Always use a defense in depth approach and reduce human access to data and mechanisms for transforming, deleting, or copying data. For example, require users to strongly authenticate to an application, and give the application, rather than the users, the requisite access permission to perform action at a distance. In addition, ensure that users come from a trusted network path and require access to the decryption keys. Use tools, such as dashboards and automated reporting, to give users information from the data rather than giving them direct access to the data.Level of risk exposed if this best practice is not established: LowIdentify data types: Identify the types of data that you are storing or processing in your workload. That data could be text, images, binary databases, and so forth.Related documents:Data Classification WhitepaperGetting started with Amazon MacieRelated videos:Introducing the New Amazon Macie\n* Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.Best practices:\n* Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access or mishandling.Best practices\n* Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.Level of risk exposed if this best practice is not established: LowAnalyze your data access and storage patterns.Store data files in efficient file formats such as Parquet to prevent unnecessary processing (for example, when running analytics) and to reduce the total storage provisioned.Use technologies that work natively with compressed data.Use the database engine that best supports your dominant query pattern.Manage your database indexes to ensure index designs support efficient query execution.Select network protocols that reduce the amount of network capacity consumed.Related documents:Athena Compression Support file formatsCOPY from columnar data formats with Amazon RedshiftConverting Your Input Record Format in Kinesis Data FirehoseFormat Options for ETL Inputs and Outputs in AWS GlueImprove query performance on Amazon Athena by Converting to Columnar FormatsLoading compressed data files from Amazon S3 with Amazon RedshiftMonitoring DB load with Performance Insights on Amazon AuroraMonitoring DB load with Performance Insights on Amazon RDSAWS IoT FleetWiseRelated videos:Building Sustainably on AWS\n* Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload. Fetch data from shared storage only as needed. Detach unused volumes to make more resources available.Level of risk exposed if this best practice is not established: LowMigrate data to shared storage when the data has multiple consumers.Fetch data from shared storage only as needed.Delete data as appropriate for your usage patterns, and implement time-to-live (TTL) functionality to manage cached data.Detach volumes from clients that are not actively using them.Related documents:Amazon FSxCaching strategiesWhat is Amazon Elastic File System?What is Amazon S3?\n\nKnowledge Area: Building and securing data lakes\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"In order to securely store log files, which of the following best practices should be established?\", \n\"answer_choices\": [\n{\"answer\": \"Migrate data to the appropriate technology based on access pattern.\",\"is_correct\": \"False\",\"explanation\": \"Data should be migrated to shared storage to avoid data duplication and reduce total storage requirements.\"},\n{\"answer\": \"Use storage that best supports how your data is accessed and stored.\",\"is_correct\": \"True\",\"explanation\": \"Using storage that best supports how data is accessed and stored can minimize the resources provisioned while supporting the workload.\"},\n{\"answer\": \"Identify data types.\",\"is_correct\": \"True\",\"explanation\": \"Identifying data types helps you understand your data and use storage technologies and configurations that support the business value of the data and how it is used.\"},\n{\"answer\": \"Migrate archival data to storage designed for that purpose.\",\"is_correct\": \"True\",\"explanation\": \"Archival data should be migrated to storage designed for that purpose, such as Amazon S3 Glacier, to minimize the resources provisioned while supporting the workload.\"}\n]}", "docs": ["('Data protection - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-dataprot.html')", "('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')", "('SEC07-BP04 Define data lifecycle management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_data_classification_lifecycle_management.html')", "('SUS 4 How do you take advantage of data access and usage patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-04.html')", "('SEC 8\\xa0 How do you protect your data at rest? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-08.html')", "('SUS03-BP05 Use software patterns and architectures that best support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a6.html')", "('SUS04-BP06 Use shared file systems or object storage to access common data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a7.html')"]}, {"scenario": "A startup wants to host their new web application on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Skills in", "item": "Designing data streaming architectures", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA startup wants to host their new web application on AWS.`\"\n\nContext:\n\n* All components are selected, and architecture is designed to reduce data transfer costs. This includes using components such as wide-area-network (WAN) optimization and Multi-Availability Zone (AZ) configurationsLevel of risk exposed if this best practice is not established: LowArchitecting for data transfer ensures that you minimize data transfer costs. This may involve using content delivery networks to locate data closer to users, or using dedicated network links from your premises to AWS. You can also use WAN optimization and application optimization to reduce the amount of data that is transferred between components.Implementation stepsSelect components for data transfer: Using the data transfer modeling, focus on where the largest data transfer costs are or where they would be if the workload usage changes. Look for alternative architectures, or additional components that remove or reduce the need for data transfer, or lower its cost.Related documents:AWS caching solutionsDeliver content faster with Amazon CloudFront\n* Where applicable, use multiple strategies to improve performance. For example, using strategies like caching data to prevent excessive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Common anti-patterns:You assume that workload performance is adequate if customers are not complaining.You only collect data on performance after you have made performance-related changes.Benefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and historical reference. You can create dashboards and perform metric math on your data to derive operational and utilization insights for your workloads as they change over time.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Related documents:Amazon Builders\u2019 LibraryBest Practices for Implementing Amazon ElastiCacheAWS Database CachingAmazon CloudWatch RUMDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)AWS purpose-built databases (DAT209-L)Optimize applications through Amazon CloudWatch RUMRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n* Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time.Best practices\n* Use efficient software designs and architectures to minimize the average resources required per unit of work. Implement mechanisms that result in even utilization of components to reduce resources that are idle between tasks and minimize the impact of load spikes.Level of risk exposed if this best practice is not established: LowQueue requests that don\u2019t require immediate processing.Increase serialization to flatten utilization across your pipeline.Modify the capacity of individual components to prevent idling resources waiting for input.Create buffers and establish rate limiting to smooth the consumption of external services.Use the most efficient available hardware for your software optimizations.Use queue-driven architectures, pipeline management, and On-Demand Instance workers to maximize utilization for batch processing.Schedule tasks to avoid load spikes and resource contention from simultaneous execution.Schedule jobs during times of day where carbon intensity for power is lowest.Related documents:What is Amazon Simple Queue Service?What is Amazon MQ?Scaling based on Amazon SQSWhat is AWS Step Functions?What is AWS Lambda?Using AWS Lambda with Amazon SQSWhat is Amazon EventBridge?Related videos:Building Sustainably on AWSMoving to event-driven architectures\n* As changes are made to improve performance, evaluate the collected metrics and data. Use this information to determine impact that the performance improvement had on the workload, the workload\u2019s components, and your customers. This measurement helps you understand the improvements that result from the tradeoff, and helps you determine if any negative side-effects were introduced.A well-architected system uses a combination of performance related strategies. Determine which strategy will have the largest positive impact on a given hotspot or bottleneck. For example, sharding data across multiple relational database systems could improve overall throughput while retaining support for transactions and, within each shard, caching can help to reduce the load.Common anti-patterns:You deploy and manage technologies manually that are available as managed services.You focus on just one component, such as networking, when multiple components could be used to increase performance of the workload.You rely on customer feedback and perceptions as your only benchmark.Benefits of establishing this best practice: For implementing performance strategies, you must select multiple services and features that, taken together, will allow you to meet your workload requirements for performance.Level of risk exposed if this best practice is not established: MediumA well-architected system uses a combination of performance-related strategies. Determine which strategy will have the largest positive impact on a given hotspot or bottleneck. For example, sharding data across multiple relational database systems could improve overall throughput while retaining support for transactions and, within each shard, caching can help to reduce the load.Related documents:Amazon Builders\u2019 LibraryAmazon CloudWatch RUMAmazon CloudWatch SyntheticsDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)Optimize applications through Amazon CloudWatch RUMDemo of Amazon CloudWatch SyntheticsRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n\nKnowledge Area: Designing data streaming architectures\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"What type of architecture should the startup use to minimize data transfer costs for their web application hosted on AWS?\",\"answer_choices\": [{\"answer\": \"Use a dedicated network link from your premises to AWS\",\"is_correct\": \"False\",\"explanation\": \"This does not minimize data transfer costs, it only reduces the cost of transferring data from the premises to AWS.\"},{\"answer\": \"Using WAN optimization and application optimization\",\"is_correct\": \"True\",\"explanation\": \"Using WAN optimization and application optimization will reduce the amount of data that is transferred between components, resulting in a reduction of data transfer costs.\"},{\"answer\": \"Using content delivery networks to locate data closer to users\",\"is_correct\": \"True\",\"explanation\": \"Using content delivery networks to locate data closer to users will reduce the amount of data that has to be transferred, resulting in a reduction of data transfer costs.\"},{\"answer\": \"Use an On-Demand Instance worker to maximize utilization for batch processing\",\"is_correct\": \"False\",\"explanation\": \"This does not reduce data transfer costs, but it does optimize resource utilization, which can reduce costs in other areas.\"}]}", "docs": ["('COST08-BP02 Select components to optimize data transfer cost - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_optimized_components.html')", "('PERF08-BP05 Use various performance-related strategies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_implement_strategy.html')", "('COST 8\\xa0 How do you plan for data transfer charges? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-08.html')", "('SUS03-BP01 Optimize software and architecture for asynchronous and scheduled jobs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a2.html')", "('PERF08-BP04 Measure the impact of performance improvements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_measure.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Skills in", "item": "Designing data transfer solutions", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time.Best practices\n* All components are selected, and architecture is designed to reduce data transfer costs. This includes using components such as wide-area-network (WAN) optimization and Multi-Availability Zone (AZ) configurationsLevel of risk exposed if this best practice is not established: LowArchitecting for data transfer ensures that you minimize data transfer costs. This may involve using content delivery networks to locate data closer to users, or using dedicated network links from your premises to AWS. You can also use WAN optimization and application optimization to reduce the amount of data that is transferred between components.Implementation stepsSelect components for data transfer: Using the data transfer modeling, focus on where the largest data transfer costs are or where they would be if the workload usage changes. Look for alternative architectures, or additional components that remove or reduce the need for data transfer, or lower its cost.Related documents:AWS caching solutionsDeliver content faster with Amazon CloudFront\n* Gather organization requirements and perform data transfer modeling of the workload and each of its components. This identifies the lowest cost point for its current data transfer requirements.Level of risk exposed if this best practice is not established: HighUnderstand where the data transfer occurs in your workload, the cost of the transfer, and its associated benefit. This allows you to make an informed decision to modify or accept the architectural decision. For example, you may have a Multi-Availability Zone configuration where you replicate data between the Availability Zones. You model the cost of structure and decide that this is an acceptable cost (similar to paying for compute and storage in both Availability Zone) to achieve the required reliability and resilience.Model the costs over different usage levels. Workload usage can change over time, and different services may be more cost effective at different levels.Use AWS Cost Explorer or the AWS Cost and Usage Report (CUR) to understand and model your data transfer costs. Configure a proof of concept (PoC) or test your workload, and run a test with a realistic simulated load. You can model your costs at different workload demands.Implementation stepsCalculate data transfer costs: Use the AWS pricing pages and calculate the data transfer costs for the workload. Calculate the data transfer costs at different usage levels, for both increases and reductions in workload usage. Where there are multiple options for the workload architecture, calculate the cost for each option for comparison.Link costs to outcomes: For each data transfer cost incurred, specify the outcome that it achieves for the workload. If it is transfer between components, it may be for decoupling, if it is between Availability Zones it may be for redundancy.Related documents:AWS caching solutionsAWS PricingAmazon EC2 PricingAmazon VPC pricingDeliver content faster with Amazon CloudFront\n* Protect your data in transit by implementing multiple controls to reduce the risk of unauthorized access or loss.Best practices\n* Implement services to reduce data transfer. For example, using a content delivery network (CDN) such as Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using AWS Direct Connect instead of VPN for connectivity to AWS.Level of risk exposed if this best practice is not established: LowAmazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources. By using CloudFront, you can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network. It is ideal for small offices or business partners because it provides quick and easy connectivity, and it is a fully managed and elastic service.VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs. Gateway VPC endpoints have no hourly charges, and support Amazon Simple Storage Service(Amazon S3) and Amazon DynamoDB. Interface VPC endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost.Implementation stepsImplement services: Using the data transfer modeling, look at where the largest costs and highest volume flows are. Review the AWS services and assess whether there is a service that reduces or removes the transfer, specifically networking and content delivery. Also look for caching services where there is repeated access to data, or large amounts of data.Related documents:AWS Direct ConnectAWS Explore Our ProductsAWS caching solutionsAmazon CloudFrontDeliver content faster with Amazon CloudFront\n* Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.Level of risk exposed if this best practice is not established: LowAnalyze your data access and storage patterns.Store data files in efficient file formats such as Parquet to prevent unnecessary processing (for example, when running analytics) and to reduce the total storage provisioned.Use technologies that work natively with compressed data.Use the database engine that best supports your dominant query pattern.Manage your database indexes to ensure index designs support efficient query execution.Select network protocols that reduce the amount of network capacity consumed.Related documents:Athena Compression Support file formatsCOPY from columnar data formats with Amazon RedshiftConverting Your Input Record Format in Kinesis Data FirehoseFormat Options for ETL Inputs and Outputs in AWS GlueImprove query performance on Amazon Athena by Converting to Columnar FormatsLoading compressed data files from Amazon S3 with Amazon RedshiftMonitoring DB load with Performance Insights on Amazon AuroraMonitoring DB load with Performance Insights on Amazon RDSAWS IoT FleetWiseRelated videos:Building Sustainably on AWS\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n* Distributed systems rely on communications networks to interconnect components, such as servers or services. Your workload must operate reliably despite data loss or latency in these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices prevent failures and improve mean time between failures (MTBF).Best practices\n\nKnowledge Area: Designing data transfer solutions\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best way to reduce data transfer costs for a company running a disaster recovery plan for their critical systems on AWS?\", \"answer_choices\": [{\"answer\": \"Using a content delivery network (CDN) such as Amazon CloudFront to deliver content to end users\",\"is_correct\": \"True\",\"explanation\": \"Using a CDN such as Amazon CloudFront to deliver content to end users can reduce data transfer costs by caching data at edge locations across the world, reducing load on resources.\"},{\"answer\": \"Using AWS Direct Connect instead of VPN for connectivity to AWS\",\"is_correct\": \"True\",\"explanation\": \"Using AWS Direct Connect instead of VPN for connectivity to AWS can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.\"},{\"answer\": \"Using caching layers using Amazon ElastiCache\",\"is_correct\": \"True\",\"explanation\": \"Using caching layers using Amazon ElastiCache can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.\"},{\"answer\": \"Using AWS Pricing to calculate the data transfer costs for the workload\",\"is_correct\": \"False\",\"explanation\": \"AWS Pricing can be used to calculate data transfer costs, but is not the best way to reduce data transfer costs.\"}]}", "docs": ["('COST 8\\xa0 How do you plan for data transfer charges? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-08.html')", "('COST08-BP02 Select components to optimize data transfer cost - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_optimized_components.html')", "('COST08-BP01 Perform data transfer modeling - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_modeling.html')", "('SEC 9\\xa0 How do you protect your data in transit? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-09.html')", "('COST08-BP03 Implement services to reduce data transfer costs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_implement_services.html')", "('SUS03-BP05 Use software patterns and architectures that best support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a6.html')", "('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')", "('REL 4\\xa0 How do you design interactions in a distributed system to prevent failures? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-04.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Skills in", "item": "Implementing visualization strategies", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Where applicable, use multiple strategies to improve performance. For example, using strategies like caching data to prevent excessive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Common anti-patterns:You assume that workload performance is adequate if customers are not complaining.You only collect data on performance after you have made performance-related changes.Benefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and historical reference. You can create dashboards and perform metric math on your data to derive operational and utilization insights for your workloads as they change over time.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Related documents:Amazon Builders\u2019 LibraryBest Practices for Implementing Amazon ElastiCacheAWS Database CachingAmazon CloudWatch RUMDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)AWS purpose-built databases (DAT209-L)Optimize applications through Amazon CloudWatch RUMRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n* As changes are made to improve performance, evaluate the collected metrics and data. Use this information to determine impact that the performance improvement had on the workload, the workload\u2019s components, and your customers. This measurement helps you understand the improvements that result from the tradeoff, and helps you determine if any negative side-effects were introduced.A well-architected system uses a combination of performance related strategies. Determine which strategy will have the largest positive impact on a given hotspot or bottleneck. For example, sharding data across multiple relational database systems could improve overall throughput while retaining support for transactions and, within each shard, caching can help to reduce the load.Common anti-patterns:You deploy and manage technologies manually that are available as managed services.You focus on just one component, such as networking, when multiple components could be used to increase performance of the workload.You rely on customer feedback and perceptions as your only benchmark.Benefits of establishing this best practice: For implementing performance strategies, you must select multiple services and features that, taken together, will allow you to meet your workload requirements for performance.Level of risk exposed if this best practice is not established: MediumA well-architected system uses a combination of performance-related strategies. Determine which strategy will have the largest positive impact on a given hotspot or bottleneck. For example, sharding data across multiple relational database systems could improve overall throughput while retaining support for transactions and, within each shard, caching can help to reduce the load.Related documents:Amazon Builders\u2019 LibraryAmazon CloudWatch RUMAmazon CloudWatch SyntheticsDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)Optimize applications through Amazon CloudWatch RUMDemo of Amazon CloudWatch SyntheticsRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n* Provide dashboards tailored to their target audiences (for example, internal technical teams, leadership, and customers) to communicate the current operating status of the business and provide metrics of interest.You can create dashboards using Amazon CloudWatch Dashboards on customizable home pages in the CloudWatch console. Using business intelligence services such as Amazon QuickSight you can create and publish interactive dashboards of your workload and operational health (for example, order rates, connected users, and transaction times). Create Dashboards that present system and business-level views of your metrics.Common anti-patterns:Upon request, you run a report on the current utilization of your application for management.During an incident, you are contacted every twenty minutes by a concerned system owner wanting to know if it is fixed yet.Benefits of establishing this best practice: By creating dashboards, you enable self-service access to information enabling your customers to informed themselves and determine if they need to take action.Level of risk exposed if this best practice is not established: MediumCommunicate status through dashboards: Provide dashboards tailored to their target audiences (for example, internal technical teams, leadership, and customers) to communicate the current operating status of the business and provide metrics of interest. Providing a self-service option for status information reduces the disruption of fielding requests for status by the operations team. Examples include Amazon CloudWatch dashboards, and AWS Health Dashboard.CloudWatch dashboards create and use customized metrics viewsRelated documents:Amazon QuickSightCloudWatch dashboards create and use customized metrics views\n\nKnowledge Area: Implementing visualization strategies\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which strategy should be implemented to optimize performance when making changes to a healthcare company\u2019s workload that involves sensitive patient information in the cloud?\", \"answer_choices\": [{\"answer\": \"Deploy and manage technologies manually \", \"is_correct\": \"False\", \"explanation\": \"Deploying and managing technologies manually is an anti-pattern and will not optimize performance. Instead, use a data-driven approach to evolve the architecture and measure the impacts to the system and end-user to understand how tradeoffs impact the workload.\"},{\"answer\": \"Focus on just one component, such as networking\", \"is_correct\": \"False\", \"explanation\": \"Focusing on just one component, such as networking, is an anti-pattern and will not optimize performance. Instead, use a combination of performance-related strategies that will allow you to meet workload requirements for performance.\"}, {\"answer\": \"Rely on customer feedback and perceptions as the only benchmark\", \"is_correct\": \"False\", \"explanation\": \"Relying on customer feedback and perceptions as the only benchmark is an anti-pattern and will not optimize performance. Instead, use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.\"},{\"answer\": \"Use a data-driven approach to evaluate metrics and data\", \"is_correct\": \"True\", \"explanation\": \"Using a data-driven approach to evaluate metrics and data is the best practice for optimizing performance. This involves collecting data on performance after changes have been made and measuring the impacts to the system and to the end-user to understand how tradeoffs impact the workload.\"}]}", "docs": ["('PERF08-BP05 Use various performance-related strategies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_implement_strategy.html')", "('PERF08-BP04 Measure the impact of performance improvements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_measure.html')", "('OPS10-BP06 Communicate status through dashboards - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_event_response_dashboards.html')"]}, {"scenario": "A company wants to build a mobile application that needs to access and store data in the cloud.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Skills in", "item": "Selecting appropriate compute options for data processing (for example, Amazon EMR)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to build a mobile application that needs to access and store data in the cloud.`\"\n\nContext:\n\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Selecting compute resources that meet your requirements, performance needs, and provide great efficiency of cost and effort will enable you to accomplish more with the same number of resources. When evaluating compute options, be aware of your requirements for workload performance and cost requirements and use this to make informed decisions.In AWS, compute is available in three forms: instances, containers, and functions:Instances are virtualized servers, allowing you to change their capabilities with a button or an API call. Because resource decisions in the cloud aren\u2019t fixed, you can experiment with different server types. At AWS, these virtual server instances come in different families and sizes, and they offer a wide variety of capabilities, including solid-state drives (SSDs) and graphics processing units (GPUs).Containers are a method of operating system virtualization that allow you to run an application and its dependencies in resource-isolated processes. AWS Fargate is serverless compute for containers or Amazon EC2 can be used if you need control over the installation, configuration, and management of your compute environment. You can also choose from multiple container orchestration platforms: Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS).Functions abstract the execution environment from the code you want to execute. For example, AWS Lambda allows you to execute code without running an instance.The following question focuses on these considerations for performance efficiency.When architecting your use of compute you should take advantage of the elasticity mechanisms available to ensure you have sufficient capacity to sustain performance as demand changes.PERF 2: How do you select your compute solution?The optimal compute solution for a workload varies based on application design, usage patterns, and configuration settings. Architectures can use different compute solutions for various components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency.\n\nKnowledge Area: Selecting appropriate compute options for data processing (for example, Amazon EMR)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When architecting a compute solution to process data, what should you consider to ensure you have sufficient capacity to sustain performance as demand changes?\",\n\"answer_choices\": [{\"answer\": \"Standardize all instances types to one type for ease of management\",\"is_correct\": false,\"explanation\": \"Standardizing all instance types does not improve performance, and only ensures ease of management.\"},{\"answer\": \"Choose the largest instance available for all workloads\",\"is_correct\": false,\"explanation\": \"Choosing the largest instance available for all workloads does not guarantee the best performance, and can lead to increased costs.\"},{\"answer\": \"Optimize performance and cost by determining which resources your workload needs\",\"is_correct\": true,\"explanation\": \"Right-sizing enables your workload to perform as well as possible while only using the required resources. This includes selecting the right instance type, size, and compute options for the workload.\"},{\"answer\": \"Choose memory-optimized instances for systems that require more memory than CPU\",\"is_correct\": true,\"explanation\": \"Selecting memory-optimized instances for systems that require more memory than CPU is an effective way to optimize performance and cost.\"}]}", "docs": ["('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')", "('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('Compute - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-compute.html')"]}, {"scenario": "An education organization wants to use AWS to deliver online learning courses to students globally.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Skills in", "item": "Selecting appropriate compute options for data processing (for example, Amazon EMR)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nAn education organization wants to use AWS to deliver online learning courses to students globally.`\"\n\nContext:\n\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Selecting compute resources that meet your requirements, performance needs, and provide great efficiency of cost and effort will enable you to accomplish more with the same number of resources. When evaluating compute options, be aware of your requirements for workload performance and cost requirements and use this to make informed decisions.In AWS, compute is available in three forms: instances, containers, and functions:Instances are virtualized servers, allowing you to change their capabilities with a button or an API call. Because resource decisions in the cloud aren\u2019t fixed, you can experiment with different server types. At AWS, these virtual server instances come in different families and sizes, and they offer a wide variety of capabilities, including solid-state drives (SSDs) and graphics processing units (GPUs).Containers are a method of operating system virtualization that allow you to run an application and its dependencies in resource-isolated processes. AWS Fargate is serverless compute for containers or Amazon EC2 can be used if you need control over the installation, configuration, and management of your compute environment. You can also choose from multiple container orchestration platforms: Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS).Functions abstract the execution environment from the code you want to execute. For example, AWS Lambda allows you to execute code without running an instance.The following question focuses on these considerations for performance efficiency.When architecting your use of compute you should take advantage of the elasticity mechanisms available to ensure you have sufficient capacity to sustain performance as demand changes.PERF 2: How do you select your compute solution?The optimal compute solution for a workload varies based on application design, usage patterns, and configuration settings. Architectures can use different compute solutions for various components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency.\n\nKnowledge Area: Selecting appropriate compute options for data processing (for example, Amazon EMR)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\n\"question\": \"When architecting your use of compute you should take advantage of the elasticity mechanisms available to ensure you have sufficient capacity to sustain performance as demand changes. How do you select your compute solution?\", \n\"answer_choices\": [\n{\"answer\": \"Select resource size or type based on data about the workload and resource characteristics\",\"is_correct\": \"False\",\"explanation\": \"The selection of a compute solution should be based on application design, usage patterns, and configuration settings, not data about the workload and resource characteristics.\"},\n{\"answer\": \"Choose memory-optimized instances for systems that require more memory than CPU\",\"is_correct\": \"False\",\"explanation\": \"Memory-optimized instances are one type of compute solution and should be chosen based on the application design, usage patterns, and configuration settings.\"},\n{\"answer\": \"Evaluate different compute options based on workload performance and cost requirements\",\"is_correct\": \"True\",\"explanation\": \"Evaluating compute options based on workload performance and cost requirements can help you select the optimal compute solution for your application.\"},\n{\"answer\": \"Standardize all instance types to one type for ease of management\",\"is_correct\": \"False\",\"explanation\": \"Standardizing all instances types to one type may not be the optimal solution for all workloads, and can lead to lower performance efficiency.\"}\n]}", "docs": ["('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')", "('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('Compute - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-compute.html')"]}, {"scenario": "A social media platform wants to use AWS to store and process user-generated content.", "domain": "Domain 3: Design High-Performing Architectures", "task": "Task Statement 5: Determine high-performing data ingestion and transformation solutions.", "focus": "Skills in", "item": "Transforming data between formats (for example, .csv to .parquet)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA social media platform wants to use AWS to store and process user-generated content.`\"\n\nContext:\n\n* Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.Level of risk exposed if this best practice is not established: LowAnalyze your data access and storage patterns.Store data files in efficient file formats such as Parquet to prevent unnecessary processing (for example, when running analytics) and to reduce the total storage provisioned.Use technologies that work natively with compressed data.Use the database engine that best supports your dominant query pattern.Manage your database indexes to ensure index designs support efficient query execution.Select network protocols that reduce the amount of network capacity consumed.Related documents:Athena Compression Support file formatsCOPY from columnar data formats with Amazon RedshiftConverting Your Input Record Format in Kinesis Data FirehoseFormat Options for ETL Inputs and Outputs in AWS GlueImprove query performance on Amazon Athena by Converting to Columnar FormatsLoading compressed data files from Amazon S3 with Amazon RedshiftMonitoring DB load with Performance Insights on Amazon AuroraMonitoring DB load with Performance Insights on Amazon RDSAWS IoT FleetWiseRelated videos:Building Sustainably on AWS\n* Evaluate the impact of tradeoffs between competing interests or alternative approaches, to help make informed decisions when determining where to focus efforts or choosing a course of action. For example, accelerating speed to market for new features may be emphasized over cost optimization, or you may choose a relational database for non-relational data to simplify the effort to migrate a system, rather than migrating to a database optimized for your data type and updating your application.AWS can help you educate your teams about AWS and its services to increase their understanding of how their choices can have an impact on your workload. You should use the resources provided by AWS Support (AWS Knowledge Center, AWS Discussion Forums, and AWS Support Center) and AWS Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS questions.AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog and The Official AWS Podcast.Common anti-patterns:You are using a relational database to manage time series and non-relational data. There are database options that are optimized to support the data types you are using but you are unaware of the benefits because you have not evaluated the tradeoffs between solutions.Your investors request that you demonstrate compliance with Payment Card Industry Data Security Standards (PCI DSS). You do not consider the tradeoffs between satisfying their request and continuing with your current development efforts. Instead you proceed with your development efforts without demonstrating compliance. Your investors stop their support of your company over concerns about the security of your platform and their investments.Benefits of establishing this best practice: Understanding the implications and consequences of your choices enables you to prioritize your options.Level of risk exposed if this best practice is not established: MediumEvaluate tradeoffs: Evaluate the impact of tradeoffs between competing interests, to help make informed decisions when determining where to focus efforts. For example, accelerating speed to market for new features might be emphasized over cost optimization.AWS can help you educate your teams about AWS and its services to increase their understanding of how their choices can have an impact on your workload. You should use the resources provided by AWS Support (AWS Knowledge Center, AWS Discussion Forums, and AWS Support Center) and AWS Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS questions.AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog and The Official AWS Podcast.Related documents:AWS BlogAWS Cloud ComplianceAWS Discussion ForumsAWS DocumentationAWS Knowledge CenterAWS SupportAWS Support CenterThe Amazon Builders' LibraryThe Official AWS Podcast\n* Use shared storage and access data from regional data stores to minimize the total networking resources required to support data movement for your workload.Level of risk exposed if this best practice is not established: LowStore data as close to the consumer as possible.Partition regionally consumed services so that their Region-specific data is stored within the Region where it is consumed.Use block-level duplication instead of file or object-level duplication when copying changes across the network.Compress data before moving it over the network.Related documents:Optimizing your AWS Infrastructure for Sustainability, Part III: NetworkingAWS Global InfrastructureAmazon CloudFront Key Features including the CloudFront Global Edge NetworkCompressing HTTP requests in Amazon OpenSearch ServiceIntermediate data compression with Amazon EMRLoading compressed data files from Amazon S3 into Amazon RedshiftServing compressed files with Amazon CloudFront\n\nKnowledge Area: Transforming data between formats (for example, .csv to .parquet)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following would be the best way for the social media platform to store and process user-generated content using AWS?\",\n\"answer_choices\": [\n  {\"answer\": \"Store data files in uncompressed formats such as .csv\",\"is_correct\": \"false\",\"explanation\": \"Storing data files in uncompressed formats, such as .csv, would be more expensive in terms of storage and processing since the files would need to be processed and stored more often than with more efficient formats such as Parquet.\"},\n  {\"answer\": \"Use technologies that do not work natively with compressed data\",\"is_correct\": \"false\",\"explanation\": \"Using technologies that do not work natively with compressed data would be more expensive in terms of storage and processing since the files would need to be processed and stored more often than with technologies that do work natively with compressed data.\"},\n  {\"answer\": \"Store data files in efficient file formats such as Parquet\",\"is_correct\": \"true\",\"explanation\": \"Storing data files in efficient file formats such as Parquet would be the best way for the social media platform to store and process user-generated content using AWS. This will prevent unnecessary processing and reduce the total storage provisioned.\"},\n  {\"answer\": \"Use the database engine that does not best supports your dominant query pattern\",\"is_correct\": \"false\",\"explanation\": \"Using the database engine that does not best support your dominant query pattern would not be the best way for the social media platform to store and process user-generated content using AWS. This would slow down query execution, and would not be efficient.\"}\n]}", "docs": ["('SUS03-BP05 Use software patterns and architectures that best support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a6.html')", "('OPS01-BP06 Evaluate tradeoffs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_priorities_eval_tradeoffs.html')", "('SUS04-BP07 Minimize data movement across networks - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a8.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Knowledge of", "item": "Access options (for example, an S3 bucket with Requester Pays object storage)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Enforce access control with least privileges and mechanisms, including backups, isolation, and versioning, to help protect your data at rest. Prevent operators from granting public access to your data.Different controls including access (using least privilege), backups (see Reliability whitepaper), isolation, and versioning can all help protect your data at rest. Access to your data should be audited using detective mechanisms covered earlier in this paper including CloudTrail, and service level log, such as Amazon Simple Storage Service (Amazon S3) access logs. You should inventory what data is publicly accessible, and plan for how you can reduce the amount of data available over time. Amazon S3 Glacier Vault Lock and Amazon S3 Object Lock are capabilities providing mandatory access control\u2014once a vault policy is locked with the compliance option, not even the root user can change it until the lock expires. The mechanism meets the Books and Records Management requirements of the SEC, CFTC, and FINRA. For more details, see this whitepaper.Level of risk exposed if this best practice is not established: LowEnforce access control: Enforce access control with least privileges, including access to encryption keys.Introduction to Managing Access Permissions to Your Amazon S3 ResourcesSeparate data based on different classification levels: Use different AWS accounts for data classification levels managed by AWS Organizations.AWS OrganizationsReview AWS KMS policies: Review the level of access granted in AWS KMS policies.Overview of managing access to your AWS KMS resourcesReview Amazon S3 bucket and object permissions: Regularly review the level of access granted in Amazon S3 bucket policies. Best practice is to not have publicly readable or writeable buckets. Consider using AWS Config to detect buckets that are publicly available, and Amazon CloudFront to serve content from Amazon S3.AWS Config RulesAmazon S3 + Amazon CloudFront: A Match Made in the CloudEnable Amazon S3 versioning and object lock.Using versioningLocking Objects Using Amazon S3 Object LockUse Amazon S3 Inventory: Amazon S3 inventory is one of the tools you can use to audit and report on the replication and encryption status of your objects.Amazon S3 InventoryReview Amazon EBS and AMI sharing permissions: Sharing permissions can allow images and volumes to be shared to AWS accounts external to your workload.Sharing an Amazon EBS SnapshotShared AMIsRelated documents:AWS KMS Cryptographic Details WhitepaperRelated videos:Securing Your Block Storage on AWS\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n\nKnowledge Area: Access options (for example, an S3 bucket with Requester Pays object storage)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which access option can be used to securely control and limit access to sensitive data stored in an S3 bucket?\", \"answer_choices\": [{\"answer\": \"Enable Amazon S3 versioning and object lock.\",\"is_correct\": \"True\",\"explanation\": \"Amazon S3 Object Lock provides an additional layer of security that prevents accidental and malicious deletion of data stored in an S3 bucket. It is important to enable versioning and object lock to protect data stored in an S3 bucket.\"},{\"answer\": \"Grant public access to the S3 bucket.\",\"is_correct\": \"False\",\"explanation\": \"Granting public access to the S3 bucket is not recommended as it can lead to potential data leakage. It is important to use an access control mechanism like least privilege or versioning and object lock to secure the data.\"},{\"answer\": \"Use AWS Config to detect buckets that are publicly available.\",\"is_correct\": \"True\",\"explanation\": \"AWS config can be used to detect buckets that are publicly available and it is a best practice to use it to monitor access to S3 buckets.\"},{\"answer\": \"Use Amazon CloudFront to serve content from Amazon S3.\",\"is_correct\": \"True\",\"explanation\": \"Amazon CloudFront can be used to serve content from Amazon S3. It is a best practice to use CloudFront to serve content from S3 to ensure that access to the S3 bucket is secure.\"}]}", "docs": ["('SEC08-BP04 Enforce access control - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_access_control.html')", "('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Knowledge of", "item": "AWS cost management service features (for example, cost allocation tags, multi-account billing)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* The increased flexibility and agility that the cloud enables encourages innovation and fast-paced development and deployment. It eliminates the manual processes and time associated with provisioning on-premises infrastructure, including identifying hardware specifications, negotiating price quotations, managing purchase orders, scheduling shipments, and then deploying the resources. However, the ease of use and virtually unlimited on-demand capacity requires a new way of thinking about expenditures.Many businesses are composed of multiple systems run by various teams. The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget.In AWS, you create an account structure with AWS Organizations or AWS Control Tower, which provides separation and assists in allocation of your costs and usage. You can also use resource tagging to apply business and organization information to your usage and cost. Use AWS Cost Explorer for visibility into your cost and usage, or create customized dashboards and analytics with Amazon Athena and Amazon QuickSight. Controlling your cost and usage is done by notifications through AWS Budgets, and controls using AWS Identity and Access Management (IAM), and Service Quotas.The following questions focus on these considerations for cost optimization.You can use cost allocation tags to categorize and track your AWS usage and costs. When you apply tags to your AWS resources (such as EC2 instances or S3 buckets), AWS generates a cost and usage report with your usage and your tags. You can apply tags that represent organization categories (such as cost centers, workload names, or owners) to organize your costs across multiple services.Ensure you use the right level of detail and granularity in cost and usage reporting and monitoring. For high level insights and trends, use daily granularity with AWS Cost Explorer. For deeper analysis and inspection use hourly granularity in AWS Cost Explorer, or Amazon Athena and Amazon QuickSight with the Cost and Usage Report (CUR) at an hourly granularity.Combining tagged resources with entity lifecycle tracking (employees, projects) makes it possible to identify orphaned resources or projects that are no longer generating value to the organization and should be decommissioned. You can set up billing alerts to notify you of predicted overspending.COST 2: How do you govern usage?Establish policies and mechanisms to ensure that appropriate costs are incurred while objectives are achieved. By employing a checks-and-balances approach, you can innovate without overspending.COST 3: How do you monitor usage and cost?Establish policies and procedures to monitor and appropriately allocate your costs. This allows you to measure and improve the cost efficiency of this workload.COST 4: How do you decommission resources?Implement change control and resource management from project inception to end-of-life. This ensures you shut down or terminate unused resources to reduce waste.\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Configure AWS Cost Explorer and AWS Budgets inline with your organization policies.Level of risk exposed if this best practice is not established: HighTo modify usage and adjust costs, each person in your organization must have access to their cost and usage information. It is recommended that all workloads and teams have the following tooling configured when they use the cloud:Reports: Summarize of all cost and usage informationNotifications: Provide notifications when cost or usage is outside of defined limits.Current State: Configure a dashboard showing current levels of cost and usage. The dashboard should be available in a highly visible place within the work environment (similar to an operations dashboard).Trending: Provide the capability to show the variability in cost and usage over the required period of time, with the required granularity.Forecasts: Provide the capability to show estimated future costs.Tracking: Show the current cost and usage against configured goals or targets.Analysis: Provide the capability for team members to perform custom and deep analysis down to the hourly granularity, with all possible dimensions.You can use AWS native tooling, such as AWS Cost Explorer, AWS Budgets, and Amazon Athena with Amazon QuickSight to provide this capability. You can also use third-party tooling \u2014 however, you must ensure that the costs of this tooling provide value to your organization.Implementation stepsCreate a Cost Optimization group: Configure your account and create a group that has access to the required Cost and Usage reports. This group must include representatives from all teams that own or manage an application. This certifies that every team has access to their cost and usage information.Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set a budget for the overall account spend, and a budget for the workload by using tags.Configure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts. Create a dashboard for the workload that tracks overall spend, and key usage metrics for the workload.Configure advanced tooling: Optionally, you can create custom tooling for your organization that provides additional detail and granularity. You can implement advanced analysis capability using Amazon Athena, and dashboards using Amazon QuickSight.Related documents:Tagging AWS resourcesAnalyzing your costs with AWS BudgetsAnalyzing your costs with Cost ExplorerManaging AWS Cost and Usage ReportsRelated examples:Well-Architected Labs - AWS Account SetupWell-Architected Labs: Billing VisualizationWell-Architected Labs: Cost and Governance UsageWell-Architected Labs: Cost and Usage AnalysisWell-Architected Labs: Cost and Usage Visualization\n\nKnowledge Area: AWS cost management service features (for example, cost allocation tags, multi-account billing)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS cost management service feature should the gaming company use to accurately attribute resource costs to individual organization or product owners?\", \n\"answer_choices\": [\n    {\"answer\": \"Amazon QuickSight\",\"is_correct\": \"False\",\"explanation\": \"Amazon QuickSight is a BI service used to create custom analytics and dashboards. It cannot be used to accurately attribute resource costs.\"}, \n    {\"answer\": \"AWS Cost Explorer\",\"is_correct\": \"False\",\"explanation\": \"AWS Cost Explorer is a service used to explore and analyze cost and usage. It cannot be used to accurately attribute resource costs.\"}, \n    {\"answer\": \"Cost Allocation Tags\",\"is_correct\": \"True\",\"explanation\": \"Cost Allocation Tags are used to categorize and track AWS usage and costs. When applied to AWS resources, including EC2 instances and S3 buckets, these tags generate a cost and usage report with usage and tags. They can be used to accurately attribute resource costs to individual organization or product owners.\"},\n    {\"answer\": \"AWS Identity and Access Management (IAM)\",\"is_correct\": \"False\",\"explanation\": \"AWS Identity and Access Management (IAM) is used to control access to AWS resources. It cannot be used to accurately attribute resource costs.\"}\n]}", "docs": ["('Expenditure and usage awareness - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-aware.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('COST03-BP04 Configure billing and cost management tools - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_monitor_usage_config_tools.html')"]}, {"scenario": "A startup wants to host their new web application on AWS.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Knowledge of", "item": "AWS cost management tools with appropriate use cases (for example, AWS Cost Explorer, AWS Budgets, AWS Cost and Usage Report)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA startup wants to host their new web application on AWS.`\"\n\nContext:\n\n* Configure AWS Cost Explorer and AWS Budgets inline with your organization policies.Level of risk exposed if this best practice is not established: HighTo modify usage and adjust costs, each person in your organization must have access to their cost and usage information. It is recommended that all workloads and teams have the following tooling configured when they use the cloud:Reports: Summarize of all cost and usage informationNotifications: Provide notifications when cost or usage is outside of defined limits.Current State: Configure a dashboard showing current levels of cost and usage. The dashboard should be available in a highly visible place within the work environment (similar to an operations dashboard).Trending: Provide the capability to show the variability in cost and usage over the required period of time, with the required granularity.Forecasts: Provide the capability to show estimated future costs.Tracking: Show the current cost and usage against configured goals or targets.Analysis: Provide the capability for team members to perform custom and deep analysis down to the hourly granularity, with all possible dimensions.You can use AWS native tooling, such as AWS Cost Explorer, AWS Budgets, and Amazon Athena with Amazon QuickSight to provide this capability. You can also use third-party tooling \u2014 however, you must ensure that the costs of this tooling provide value to your organization.Implementation stepsCreate a Cost Optimization group: Configure your account and create a group that has access to the required Cost and Usage reports. This group must include representatives from all teams that own or manage an application. This certifies that every team has access to their cost and usage information.Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set a budget for the overall account spend, and a budget for the workload by using tags.Configure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts. Create a dashboard for the workload that tracks overall spend, and key usage metrics for the workload.Configure advanced tooling: Optionally, you can create custom tooling for your organization that provides additional detail and granularity. You can implement advanced analysis capability using Amazon Athena, and dashboards using Amazon QuickSight.Related documents:Tagging AWS resourcesAnalyzing your costs with AWS BudgetsAnalyzing your costs with Cost ExplorerManaging AWS Cost and Usage ReportsRelated examples:Well-Architected Labs - AWS Account SetupWell-Architected Labs: Billing VisualizationWell-Architected Labs: Cost and Governance UsageWell-Architected Labs: Cost and Usage AnalysisWell-Architected Labs: Cost and Usage Visualization\n* Implement tooling and dashboards to monitor cost proactively for the workload. Regularly review the costs with configured tools or out of the box tools, do not just look at costs and categories when you receive notifications. Monitoring and analyzing costs proactively helps to identify positive trends and allows you to promote them throughout your organization.Level of risk exposed if this best practice is not established: LowIt is recommended to monitor cost and usage proactively within your organization, not just when there are exceptions or anomalies. Highly visible dashboards throughout your office or work environment ensure that key people have access to the information they need, and indicate the organization\u2019s focus on cost optimization. Visible dashboards allow you to actively promote successful outcomes and implement them throughout your organization.Create a daily or frequent routine to use AWS Cost Explorer or any other dashboard such as Amazon QuickSight to see the costs and analyze proactively. Analyze AWS service usage and costs at the AWS account-level, workload-level, or specific AWS service-level with grouping and filtering, and validate whether they are expected or not. Use the hourly- and resource-level granularity and tags to filter and identify incurring costs for the top resources. You can also build your own reports with the Cost Intelligence Dashboard, an Amazon QuickSight solution built by AWS Solutions Architects, and compare your budgets with the actual cost and usage.Implementation stepsReport on cost optimization: Set up a regular cycle to discuss and analyze the efficiency of the workload. Using the metrics established, report on the metrics achieved and the cost of achieving them. Identify and fix any negative trends, and identify positive trends to promote across your organization. Reporting should involve representatives from the application teams and owners, finance, and management.Create and enable daily granularity AWS Budgets for the cost and usage to take timely actions to prevent any potential cost overruns: AWS Budgets allow you to configure alert notifications, so you stay informed if any of your budget types fall out of your pre-configured thresholds. The best way to leverage AWS Budgets is to set your expected cost and usage as your limits, so that anything above your budgets can be considered overspend.Create AWS Cost Anomaly Detection for cost monitor: AWS Cost Anomaly Detection uses advanced Machine Learning technology to identify anomalous spend and root causes, so you can quickly take action. It allows you to configure cost monitors that define spend segments you want to evaluate (for example, individual AWS services, member accounts, cost allocation tags, and cost categories), and lets you set when, where, and how you receive your alert notifications. For each monitor, attach multiple alert subscriptions for business owners and technology teams, including a name, a cost impact threshold, and alerting frequency (individual alerts, daily summary, weekly summary) for each subscription.Use AWS Cost Explorer or integrate your AWS Cost and Usage Report (CUR) data with Amazon QuickSight dashboards to visualize your organization\u2019s costs: AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. The Cost Intelligence Dashboard is a customizable and accessible dashboard to help create the foundation of your own cost management and optimization tool.Related documents:AWS BudgetsAWS Cost ExplorerDaily Cost and Usage BudgetsAWS Cost Anomaly DetectionRelated examples:Well-Architected Labs: VisualizationWell-Architected Labs: Advanced VisualizationWell-Architected Labs: Cloud Intelligence DashboardsWell-Architected Labs: Cost VisualizationAWS Cost Anomaly Detection Alert with Slack\n\nKnowledge Area: AWS cost management tools with appropriate use cases (for example, AWS Cost Explorer, AWS Budgets, AWS Cost and Usage Report)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS cost management tool would be appropriate for a startup to use to monitor the cost of their new web application?\",\n\"answer_choices\": [{\"answer\": \"AWS Cost Explorer\", \"is_correct\": \"true\", \"explanation\": \"AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time.\"},{\"answer\": \"Amazon QuickSight\", \"is_correct\": \"false\", \"explanation\": \"Amazon QuickSight is a visualization tool but does not provide cost management capabilities.\"},{\"answer\": \"AWS Budgets\", \"is_correct\": \"true\", \"explanation\": \"AWS Budgets allow you to configure alert notifications, so you stay informed if any of your budget types fall out of your pre-configured thresholds.\"},{\"answer\": \"AWS Cost and Usage Report\", \"is_correct\": \"false\", \"explanation\": \"The AWS Cost and Usage Report is a data source for cost and usage analysis, but it does not provide cost management capabilities.\"}]}", "docs": ["('COST03-BP04 Configure billing and cost management tools - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_monitor_usage_config_tools.html')", "('COST01-BP06 Monitor cost proactively - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_cloud_financial_management_proactive_process.html')"]}, {"scenario": "An education organization wants to use AWS to deliver online learning courses to students globally.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Knowledge of", "item": "AWS storage services with appropriate use cases (for example, Amazon FSx, Amazon EFS, Amazon S3, Amazon EBS)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nAn education organization wants to use AWS to deliver online learning courses to students globally.`\"\n\nContext:\n\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n* Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file storage services as well as cloud data migration options for your workload.In AWS, storage is available in three forms: object, block, and file:Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems.The following question focuses on these considerations for performance efficiency.When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want.PERF 3: How do you select your storage solution?The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\n\nKnowledge Area: AWS storage services with appropriate use cases (for example, Amazon FSx, Amazon EFS, Amazon S3, Amazon EBS)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When selecting a storage solution for an education organization that wants to use AWS to deliver online learning courses to students globally, which solution should one use in order to optimize storage space and performance for the workload?\", \n\"answer_choices\": [{\"answer\": \"Amazon S3 with CloudFront\",\"is_correct\": \"False\",\"explanation\": \"Amazon S3 with CloudFront is used for intensive GET requests, not for optimizing storage space and performance.\"},{\"answer\": \"Amazon EFS\",\"is_correct\": \"False\",\"explanation\": \"Amazon EFS is used for scalability and fully managed elastic NFS file system, not for optimizing storage space and performance.\"},{\"answer\": \"Amazon EBS\",\"is_correct\": \"True\",\"explanation\": \"Amazon EBS provides two major categories of storage options: SSD-backed storage for transactional workloads, and HDD-backed storage for throughput-intensive workloads. This makes it the optimal solution for optimizing storage space and performance.\"},{\"answer\": \"Amazon FSx\",\"is_correct\": \"False\",\"explanation\": \"Amazon FSx is used for enterprise workloads and high-performance workloads, not for optimizing storage space and performance.\"}]}", "docs": ["('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')", "('Storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-storage.html')"]}, {"scenario": "A large corporation wants to move their on-premises compute to the cloud for increased scalability and security.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Knowledge of", "item": "Backup strategies", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA large corporation wants to move their on-premises compute to the cloud for increased scalability and security.`\"\n\nContext:\n\n* Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO).Best practices\n* All AWS data stores offer backup capabilities. Services such as Amazon RDS and Amazon DynamoDB additionally support automated backup that enables point-in-time recovery (PITR), which allows you to restore a backup to any time up to five minutes or less before the current time. Many AWS services offer the ability to copy backups to another AWS Region. AWS Backup is a tool that gives you the ability to centralize and automate data protection across AWS services.Amazon S3 can be used as a backup destination for self-managed and AWS-managed data sources. AWS services such as Amazon EBS, Amazon RDS, and Amazon DynamoDB have built in capabilities to create backups. Third-party backup software can also be used.On-premises data can be backed up to the AWS Cloud using AWS Storage Gateway or AWS DataSync. Amazon S3 buckets can be used to store this data on AWS. Amazon S3 offers multiple storage tiers such as Amazon S3 Glacier or S3 Glacier Deep Archive to reduce cost of data storage.You might be able to meet data recovery needs by reproducing the data from other sources. For example, Amazon Elasticache replica nodes or RDS read replicas could be used to reproduce data if the primary is lost. In cases where sources like this can be used to meet your Recovery Point Objective (RPO) and Recovery Time Objective (RTO), you might not require a backup. Another example, if working with Amazon EMR, it might not be necessary to backup your HDFS data store, as long as you can reproduce the data into EMR from S3.When selecting a backup strategy, consider the time it takes to recover data. The time needed to recover data depends on the type of backup (in the case of a backup strategy), or the complexity of the data reproduction mechanism. This time should fall within the RTO for the workload.Desired Outcome:Data sources have been identified and classified based on criticality. Then, establish a strategy for data recovery based on the RPO. This strategy involves either backing up these data sources, or having the ability to reproduce data from other sources. In the case of data loss, the strategy implemented enables recovery or reproduction of data within the defined RPO and RTO.Cloud Maturity Phase: FoundationalCommon anti-patterns:Not aware of all data sources for the workload and their criticality.Not taking backups of critical data sources.Taking backups of only some data sources without using criticality as a criterion.No defined RPO, or backup frequency cannot meet RPO.Not evaluating if a backup is necessary or if data can be reproduced from other sources.Benefits of establishing this best practice: Identifying the places where backups are necessary and implementing a mechanism to create backups, or being able to reproduce the data from an external source improves the ability to restore and recover data during an outage.Level of risk exposed if this best practice is not established: HighUnderstand and use the backup capabilities of the AWS services and resources used by the workload. Most AWS services provides capabilities to back up workload data.Implementation Steps:Identify all data sources for the workload. Data can be stored on a number of resources such as databases, volumes, filesystems, logging systems, and object storage. Refer to the Resources section to find Related documents on different AWS services where data is stored, and the backup capability these services provide.Classify data sources based on criticality. Different data sets will have different levels of criticality for a workload, and therefore different requirements for resiliency. For example, some data might be critical and require a RPO near zero, while other data might be less critical and can tolerate a higher RPO and some data loss. Similarly, different data sets might have different RTO requirements as well.Use AWS or third-party services to create backups of the data. AWS Backup is a managed service that enables creating backups of various data sources on AWS. Most of these services also have native capabilities to create backups. The AWS Marketplace has many solutions that provide these capabilites as well. Refer to the Resources listed below for information on how to create backups of data from various AWS services.For data that is not backed up, establish a data reproduction mechanism. You might choose not to backup data that can be reproduced from other sources for various reasons. There might be a situation where it is cheaper to reproduce data from sources when needed rather than creating a backup as there may be a cost associated with storing backups. Another example is where restoring from a backup takes longer than reproducing the data from sources, resulting in a breach in RTO. In such situations, consider tradeoffs and establish a well-defined process for how data can be reproduced from these sources when data recovery is necessary. For example, if you have loaded data from Amazon S3 to a data warehouse (like Amazon Redshift), or MapReduce cluster (like Amazon EMR) to do analysis on that data, this may be an example of data that can be reproduced from other sources. As long as the results of these analyses are either stored somewhere or reproducible, you would not suffer a data loss from a failure in the data warehouse or MapReduce cluster. Other examples that can be reproduced from sources include caches (like Amazon ElastiCache) or RDS read replicas.Establish a cadence for backing up data. Creating backups of data sources is a periodic process and the frequency should depend on the RPO.Level of effort for the Implementation Plan: ModerateRelated Best Practices:REL13-BP01 Define recovery objectives for downtime and data lossREL13-BP02 Use defined recovery strategies to meet the recovery objectivesRelated documents:What Is AWS Backup?What is AWS DataSync?What is Volume Gateway?APN Partner: partners that can help with backupAWS Marketplace: products that can be used for backupAmazon EBS SnapshotsBacking Up Amazon EFSBacking up Amazon FSx for Windows File ServerBackup and Restore for ElastiCache for RedisCreating a DB Cluster Snapshot in NeptuneCreating a DB SnapshotCreating an EventBridge Rule That Triggers on a ScheduleCross-Region Replication with Amazon S3EFS-to-EFS AWS BackupExporting Log Data to Amazon S3Object lifecycle managementOn-Demand Backup and Restore for DynamoDBPoint-in-time recovery for DynamoDBWorking with Amazon OpenSearch Service Index SnapshotsRelated videos:AWS re:Invent 2021 - Backup, disaster recovery, and ransomware protection with AWSAWS Backup Demo: Cross-Account and Cross-Region BackupAWS re:Invent 2019: Deep dive on AWS Backup, ft. Rackspace (STG341)Related examples:Well-Architected lab: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon S3Well-Architected lab: Testing Backup and Restore of DataWell-Architected lab: Backup and Restore with Failback for Analytics WorkloadWell-Architected lab: Disaster Recovery - Backup and Restore\n\nKnowledge Area: Backup strategies\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which strategy should be used to meet the data recovery requirements for a large corporation who wants to move their on-premises compute to the cloud for increased scalability and security?\", \"answer_choices\": [{\"answer\": \"Create backups of all data sources using AWS Backup\",\"is_correct\": \"True\",\"explanation\": \"AWS Backup is a tool that gives you the ability to centralize and automate data protection across AWS services and create backups of various data sources on AWS.\"}, {\"answer\": \"Use Amazon S3 to store data backups\",\"is_correct\": \"False\",\"explanation\": \"Amazon S3 can be used as a backup destination for self-managed and AWS-managed data sources, but it cannot be used to create backups.\"},{\"answer\": \"Reproduce the data from other sources\",\"is_correct\": \"False\",\"explanation\": \"In cases where sources like this can be used to meet your Recovery Point Objective (RPO) and Recovery Time Objective (RTO), you might not require a backup.\"},{\"answer\": \"Reduce the cost of data storage by using multiple storage tiers\",\"is_correct\": \"False\",\"explanation\": \"Amazon S3 offers multiple storage tiers such as Amazon S3 Glacier or S3 Glacier Deep Archive to reduce cost of data storage, but it does not meet the data recovery needs for the scenario.\"}]}", "docs": ["('REL 9\\xa0 How do you back up data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-09.html')", "('REL09-BP01 Identify and back up all data that needs to be backed up, or reproduce the data from sources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_backing_up_data_identified_backups_data.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Knowledge of", "item": "Block storage options (for example, hard disk drive [HDD] volume types, solid state drive [SSD] volume types)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n\nKnowledge Area: Block storage options (for example, hard disk drive [HDD] volume types, solid state drive [SSD] volume types)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which storage option for the financial services company's workload should be used to optimize storage space and performance?\", \n\"answer_choices\": [\n    {\"answer\": \"Provisioned IOPS SSD\", \"is_correct\": \"True\", \"explanation\": \"Provisioned IOPS SSD is designed for latency-sensitive transactional workloads and provides the highest performance and storage space optimization.\"}, \n    {\"answer\": \"General purpose SSD\", \"is_correct\": \"False\", \"explanation\": \"General purpose SSD is designed to balance price and performance and is not ideal for latency-sensitive workloads.\"}, \n    {\"answer\": \"HDD-backed storage\", \"is_correct\": \"False\", \"explanation\": \"HDD-backed storage is designed for throughput-intensive workloads, not latency-sensitive workloads.\"}, \n    {\"answer\": \"Multi-part uploads with Amazon S3\", \"is_correct\": \"False\", \"explanation\": \"Multi-part uploads with Amazon S3 are designed for large files that require fast transfer, not for latency-sensitive workloads.\"}\n]\n}", "docs": ["('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Knowledge of", "item": "Hybrid storage options (for example, DataSync, Transfer Family, Storage Gateway)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file storage services as well as cloud data migration options for your workload.In AWS, storage is available in three forms: object, block, and file:Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems.The following question focuses on these considerations for performance efficiency.When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want.PERF 3: How do you select your storage solution?The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n\nKnowledge Area: Hybrid storage options (for example, DataSync, Transfer Family, Storage Gateway)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"What is the best practice for selecting the optimal storage solution for a system that requires real-time data processing?\",\n\"answer_choices\": [\n{\"answer\": \"Assign only one storage type, such as Amazon EBS, to all workloads\", \"is_correct\": false, \"explanation\": \"Using only one storage type for all workloads may not be the optimal solution, as different workloads may require different storage tiers to optimize performance and cost efficiency.\"},\n{\"answer\": \"Evaluate all storage service options to determine the best fit for the workload and its required performance\", \"is_correct\": true, \"explanation\": \"Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads and can potentially accelerate your time to market for deploying new services and features.\"},\n{\"answer\": \"Use Provisioned IOPS for all workloads without testing\", \"is_correct\": false, \"explanation\": \"Provisioned IOPS should not be used for all workloads without real-world testing against all storage tiers, as different workloads may require different storage tiers to optimize performance and cost efficiency.\"},\n{\"answer\": \"Assume that all workloads have similar storage access performance requirements\",\"is_correct\": false, \"explanation\": \"Not all workloads have similar storage access performance requirements and should be tested prior to being used.\"}\n]\n}", "docs": ["('Storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-storage.html')", "('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')", "('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Knowledge of", "item": "Storage access patterns", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n\nKnowledge Area: Storage access patterns\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice for selecting the optimal storage solution for a system based on access patterns?\", \n\"answer_choices\": [\n    {\"answer\": \"Select one database vendor to simplify operations management\",\"is_correct\": \"false\",\"explanation\": \"This is not the best practice as it does not take into account the access patterns of the workload.\"},\n    {\"answer\": \"Identify and document the anticipated growth of the data and traffic\",\"is_correct\": \"false\",\"explanation\": \"This is necessary in order to select the optimal storage solution but it is not the best practice.\"},\n    {\"answer\": \"Use the collected metrics and logs and experiment with options to find the optimal configuration\",\"is_correct\": \"false\",\"explanation\": \"This is an important step in selecting the optimal storage solution but it is not the best practice.\"},\n    {\"answer\": \"Use storage that best supports how your data is accessed and stored\",\"is_correct\": \"true\",\"explanation\": \"This is the best practice as it takes into account the access patterns of the workload.\"}\n]\n}", "docs": ["('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')", "('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')", "('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')"]}, {"scenario": "A research institution wants to perform data analysis on a large dataset using AWS compute resources.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Knowledge of", "item": "Storage types with associated characteristics (for example, object, file, block)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA research institution wants to perform data analysis on a large dataset using AWS compute resources.`\"\n\nContext:\n\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n* Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file storage services as well as cloud data migration options for your workload.In AWS, storage is available in three forms: object, block, and file:Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems.The following question focuses on these considerations for performance efficiency.When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want.PERF 3: How do you select your storage solution?The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\n* Evaluate the various characteristics and configuration options and how they relate to storage. Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral storage to optimize storage space and performance for your workload.Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: SSD-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS), and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive transactional workloads and general-purpose SSD that balance price and performance for a wide variety of transactional data.Amazon S3 transfer acceleration enables fast transfer of files over long distances between your client and your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with multiple parts uploading at the same time to help maximize network throughput.Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes: general purpose performance mode, and max I/O performance mode. There are also two throughput modes to choose from for your file system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your workload, see the Amazon EFS User Guide.Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low latency data access. You can choose the throughput level you need to match your workload\u2019s needs.Common anti-patterns:You only use one storage type, such as Amazon EBS, for all workloads.You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.You assume that all workloads have similar storage access performance requirements.Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time to market for deploying new services and features.Level of risk exposed if this best practice is not established: MediumDetermine storage characteristics: When you evaluate a storage solution, determine which storage characteristics you require, such as ability to share, file size, cache size, latency, throughput, and persistence of data. Then match your requirements to the AWS service that best fits your needs.Related documents:Cloud Storage with AWSAmazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSCloud Storage with AWSEBS I/O CharacteristicsRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n\nKnowledge Area: Storage types with associated characteristics (for example, object, file, block)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When selecting a storage solution for a research institution that wants to perform data analysis on a large dataset using AWS compute resources, which storage type and characteristics should be considered to optimize performance?\",\n\"answer_choices\": [{\"answer\":\"The type of access method (block, file, or object) and the patterns of access (random or sequential)\", \"is_correct\":\"true\",\"explanation\":\"The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\"},{\"answer\":\"The type of storage (SSD, HDD, or object storage) and the latency of the storage\", \"is_correct\":\"false\",\"explanation\":\"The type of storage (SSD, HDD, or object storage) and the latency of the storage are important considerations, but they should not be the only considerations when selecting a storage solution. The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\"},{\"answer\":\"The ability to share, file size, cache size, throughput, and persistence of data,\", \"is_correct\":\"false\",\"explanation\":\"The ability to share, file size, cache size, throughput, and persistence of data are important characteristics to consider when selecting a storage solution, but they should not be the only considerations. The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\"},{\"answer\":\"The cost of the storage and the speed of access,\", \"is_correct\":\"false\",\"explanation\":\"The cost of the storage and the speed of access are important considerations, but they should not be the only considerations when selecting a storage solution. The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\"}]}", "docs": ["('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')", "('Storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-storage.html')", "('PERF03-BP02 Evaluate available configuration options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_evaluated_options.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Designing appropriate storage strategies (for example, batch uploads to Amazon S3 compared with individual uploads)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* Choose storage systems based on your workload's access patterns and configure them by determining how the workload accesses data. Increase storage efficiency by choosing object storage over block storage. Configure the storage options you choose to match your data access patterns.How you access data impacts how the storage solution performs. Select the storage solution that aligns best to your access patterns, or consider changing your access patterns to align with the storage solution to maximize performance.Creating a RAID 0 array allows you to achieve a higher level of performance for a file system than what you can provision on a single volume. Consider using RAID 0 when I/O performance is more important than fault tolerance. For example, you could use it with a heavily used database where data replication is already set up separately.Select appropriate storage metrics for your workload across all of the storage options consumed for the workload. When using filesystems that use burst credits, create alarms to let you know when you are approaching those credit limits. You must create storage dashboards to show the overall workload storage health.For storage systems that are a fixed size, such as Amazon EBS or Amazon FSx, ensure that you are monitoring the amount of storage used versus the overall storage size and create automation if possible to increase the storage size when reaching a thresholdCommon anti-patterns:You assume that storage performance is adequate if customers are not complaining.You only use one tier of storage, assuming all workloads fit within that tier.Benefits of establishing this best practice: You need a unified operational view, real-time granular data, and historical reference to optimize performance and resource utilization. You can create automatic dashboards and data with one-second granularity to perform metric math on your data and derive operational and utilization insights for your storage needs.Level of risk exposed if this best practice is not established: LowOptimize your storage usage and access patterns: Choose storage systems based on your workload's access patterns and the characteristics of the available storage options. Determine the best place to store data that will enable you to meet your requirements while reducing overhead. Use performance optimizations and access patterns when configuring and interacting with data based on the characteristics of your storage (for example, striping volumes or partitioning data).Select appropriate metrics for storage options: Ensure that you select the appropriate storage metrics for the workload. Each storage option offers various metrics to track how your workload performs over time. Ensure that you are measuring against any storage burst metrics (for example, monitoring burst credits for Amazon EFS). For storage systems that are fixed sized, such as Amazon Elastic Block Store or Amazon FSx, ensure that you are monitoring the amount of storage used versus the overall storage size. Create automation when possible to increase the storage size when reaching a threshold.Monitor metrics: Amazon CloudWatch can collect metrics across the resources in your architecture. You can also collect and publish custom metrics to surface business or derived metrics. Use CloudWatch or third-party solutions to set alarms that indicate when thresholds are breached.Related documents:Amazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSEBS I/O CharacteristicsMonitoring and understanding Amazon EBS performance using Amazon CloudWatchRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n* Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file storage services as well as cloud data migration options for your workload.In AWS, storage is available in three forms: object, block, and file:Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems.The following question focuses on these considerations for performance efficiency.When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want.PERF 3: How do you select your storage solution?The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\n\nKnowledge Area: Designing appropriate storage strategies (for example, batch uploads to Amazon S3 compared with individual uploads)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want. What is the best way to select a storage solution?\",\n\"answer_choices\": [\n    {\"answer\": \"Create a RAID 0 array to increase performance.\",\"is_correct\": \"false\", \"explanation\": \"RAID 0 is a method of combining multiple physical disk drives into a single logical drive. It does not provide a storage solution and does not necessarily improve performance.\"}, \n    {\"answer\": \"Choose storage systems based on your workload's access patterns and configure them by determining how the workload accesses data.\",\"is_correct\": \"true\", \"explanation\": \"Choosing storage systems based on your workload's access patterns and configuring them by determining how the workload accesses data is the best way to select a storage solution. This ensures that the storage solution aligns with the access patterns and maximizes performance.\"},\n    {\"answer\": \"Monitor performance using Amazon CloudWatch and create alarms when thresholds are breached.\",\"is_correct\": \"false\", \"explanation\": \"Amazon CloudWatch can be used to monitor performance, but this is not the best way to select a storage solution.\"},\n    {\"answer\": \"Migrate data to the appropriate technology based on access pattern.\",\"is_correct\": \"false\", \"explanation\": \"Migrating data to the appropriate technology based on access patterns can help optimize performance, but this is not the best way to select a storage solution.\"}\n]}", "docs": ["('PERF03-BP03 Make decisions based on access patterns and metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_optimize_patterns.html')", "('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')", "('Storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-storage.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Determining the correct storage size for a workload", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Use metrics from the currently running workload to select the right size and type to optimize for cost. Appropriately provision throughput, sizing, and storage for services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Elastic Block Store (Amazon EBS) (PIOPS), Amazon Relational Database Service (Amazon RDS), Amazon EMR, and networking. This can be done with a feedback loop such as automatic scaling or by custom code in the workload.Level of risk exposed if this best practice is not established: LowCreate a feedback loop within the workload that uses active metrics from the running workload to make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you configure to perform the right sizing operations for you. AWS also provides APIs, SDKs, and features that allow resources to be modified with minimal effort. You can program a workload to stop-and-start an Amazon Elastic Compute Cloud(Amazon EC2) instance to allow a change of instance size or instance type. This provides the benefits of right-sizing while removing almost all the operational cost required to make the change.Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage Service(Amazon S3) Intelligent-Tiering. Amazon S3 Intelligent-Tiering automatically moves your data between two access tiers: frequent access and infrequent access, based on your usage patterns.Implementation stepsConfigure workload metrics: Ensure you capture the key metrics for the workload. These metrics provide an indication of the customer experience, such as the workload output, and align to the differences between resource types and sizes, such as CPU and memory usage.View rightsizing recommendations: Use the rightsizing recommendations in AWS Compute Optimizer to make adjustments to your workload.Select resource type and size automatically based on metrics: Using the workload metrics, manually or automatically select your workload resources. Configuring AWS Auto Scaling or implementing code within your application can reduce the effort required if frequent changes are needed, and it can potentially implement changes sooner than a manual process.Related documents:AWS Auto ScalingAWS Compute OptimizerAmazon CloudWatch featuresCloudWatch Getting Set UpCloudWatch Publishing Custom MetricsCost Optimization: Amazon EC2 Right SizingGetting Started with Amazon EC2 Auto ScalingAmazon S3 Intelligent-TieringLaunch an EC2 Instance Using the SDK\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n\nKnowledge Area: Determining the correct storage size for a workload\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice for determining the correct storage size for a workload?\",\n\"answer_choices\": [{\"answer\": \"Choose the largest instance available for all workloads\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern, as it does not take into account the performance characteristics of the workload.\"},{\"answer\": \"Standardize all instance types for ease of management\",\"is_correct\": \"False\",\"explanation\": \"This is an anti-pattern, as it does not take into account the performance characteristics of the workload.\"},{\"answer\": \"Analyze the performance characteristics of the workload and select the appropriate resource type and size\",\"is_correct\": \"True\",\"explanation\": \"This is the best practice, as it allows you to select the right size and type of resource that best matches your workload's profile.\"},{\"answer\": \"Configure an automatic feedback loop within the workload to make changes to the resource size\",\"is_correct\": \"False\",\"explanation\": \"This is a related best practice, but it is not the best practice for determining the correct storage size for a workload.\"}]}", "docs": ["('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('COST06-BP03 Select resource type, size, and number automatically based on metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_metrics.html')", "('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Determining the lowest cost method of transferring data for a workload to AWS storage", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* Gather organization requirements and perform data transfer modeling of the workload and each of its components. This identifies the lowest cost point for its current data transfer requirements.Level of risk exposed if this best practice is not established: HighUnderstand where the data transfer occurs in your workload, the cost of the transfer, and its associated benefit. This allows you to make an informed decision to modify or accept the architectural decision. For example, you may have a Multi-Availability Zone configuration where you replicate data between the Availability Zones. You model the cost of structure and decide that this is an acceptable cost (similar to paying for compute and storage in both Availability Zone) to achieve the required reliability and resilience.Model the costs over different usage levels. Workload usage can change over time, and different services may be more cost effective at different levels.Use AWS Cost Explorer or the AWS Cost and Usage Report (CUR) to understand and model your data transfer costs. Configure a proof of concept (PoC) or test your workload, and run a test with a realistic simulated load. You can model your costs at different workload demands.Implementation stepsCalculate data transfer costs: Use the AWS pricing pages and calculate the data transfer costs for the workload. Calculate the data transfer costs at different usage levels, for both increases and reductions in workload usage. Where there are multiple options for the workload architecture, calculate the cost for each option for comparison.Link costs to outcomes: For each data transfer cost incurred, specify the outcome that it achieves for the workload. If it is transfer between components, it may be for decoupling, if it is between Availability Zones it may be for redundancy.Related documents:AWS caching solutionsAWS PricingAmazon EC2 PricingAmazon VPC pricingDeliver content faster with Amazon CloudFront\n* All components are selected, and architecture is designed to reduce data transfer costs. This includes using components such as wide-area-network (WAN) optimization and Multi-Availability Zone (AZ) configurationsLevel of risk exposed if this best practice is not established: LowArchitecting for data transfer ensures that you minimize data transfer costs. This may involve using content delivery networks to locate data closer to users, or using dedicated network links from your premises to AWS. You can also use WAN optimization and application optimization to reduce the amount of data that is transferred between components.Implementation stepsSelect components for data transfer: Using the data transfer modeling, focus on where the largest data transfer costs are or where they would be if the workload usage changes. Look for alternative architectures, or additional components that remove or reduce the need for data transfer, or lower its cost.Related documents:AWS caching solutionsDeliver content faster with Amazon CloudFront\n* Use shared storage and access data from regional data stores to minimize the total networking resources required to support data movement for your workload.Level of risk exposed if this best practice is not established: LowStore data as close to the consumer as possible.Partition regionally consumed services so that their Region-specific data is stored within the Region where it is consumed.Use block-level duplication instead of file or object-level duplication when copying changes across the network.Compress data before moving it over the network.Related documents:Optimizing your AWS Infrastructure for Sustainability, Part III: NetworkingAWS Global InfrastructureAmazon CloudFront Key Features including the CloudFront Global Edge NetworkCompressing HTTP requests in Amazon OpenSearch ServiceIntermediate data compression with Amazon EMRLoading compressed data files from Amazon S3 into Amazon RedshiftServing compressed files with Amazon CloudFront\n* Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.Level of risk exposed if this best practice is not established: LowAnalyze your data access and storage patterns.Store data files in efficient file formats such as Parquet to prevent unnecessary processing (for example, when running analytics) and to reduce the total storage provisioned.Use technologies that work natively with compressed data.Use the database engine that best supports your dominant query pattern.Manage your database indexes to ensure index designs support efficient query execution.Select network protocols that reduce the amount of network capacity consumed.Related documents:Athena Compression Support file formatsCOPY from columnar data formats with Amazon RedshiftConverting Your Input Record Format in Kinesis Data FirehoseFormat Options for ETL Inputs and Outputs in AWS GlueImprove query performance on Amazon Athena by Converting to Columnar FormatsLoading compressed data files from Amazon S3 with Amazon RedshiftMonitoring DB load with Performance Insights on Amazon AuroraMonitoring DB load with Performance Insights on Amazon RDSAWS IoT FleetWiseRelated videos:Building Sustainably on AWS\n* Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload. Fetch data from shared storage only as needed. Detach unused volumes to make more resources available.Level of risk exposed if this best practice is not established: LowMigrate data to shared storage when the data has multiple consumers.Fetch data from shared storage only as needed.Delete data as appropriate for your usage patterns, and implement time-to-live (TTL) functionality to manage cached data.Detach volumes from clients that are not actively using them.Related documents:Amazon FSxCaching strategiesWhat is Amazon Elastic File System?What is Amazon S3?\n\nKnowledge Area: Determining the lowest cost method of transferring data for a workload to AWS storage\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best practice for determining the lowest cost method of transferring data for a workload to AWS storage?\", \n\"answer_choices\": [{\"answer\": \"Calculate data transfer costs and create a model of the workload and its components.\",\"is_correct\": \"True\",\"explanation\": \"Calculating data transfer costs and creating a model of the workload and its components is the best practice for determining the lowest cost method of transferring data for a workload to AWS storage.\"},{\"answer\": \"Link costs to outcomes and configure a proof of concept.\",\"is_correct\": \"True\",\"explanation\": \"Linking costs to outcomes and configuring a proof of concept is part of the best practice for determining the lowest cost method of transferring data for a workload to AWS storage.\"},{\"answer\": \"Use shared storage and access data from regional data stores.\",\"is_correct\": \"True\",\"explanation\": \"Using shared storage and accessing data from regional data stores is part of the best practice for determining the lowest cost method of transferring data for a workload to AWS storage.\"},{\"answer\": \"Adopt shared storage and single sources of truth to avoid data duplication.\",\"is_correct\": \"False\",\"explanation\": \"Adopting shared storage and single sources of truth to avoid data duplication is part of the best practice, but not the best practice for determining the lowest cost method of transferring data for a workload to AWS storage.\"}]}", "docs": ["('COST08-BP01 Perform data transfer modeling - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_modeling.html')", "('COST08-BP02 Select components to optimize data transfer cost - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_optimized_components.html')", "('SUS04-BP07 Minimize data movement across networks - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a8.html')", "('SUS03-BP05 Use software patterns and architectures that best support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a6.html')", "('SUS04-BP06 Use shared file systems or object storage to access common data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a7.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Determining when storage auto scaling is required", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* Scale resources reactively when necessary if availability is impacted, to restore workload availability.You first must configure health checks and the criteria on these checks to indicate when availability is impacted by lack of resources. Then either notify the appropriate personnel to manually scale the resource, or trigger automation to automatically scale it.Scale can be manually adjusted for your workload, for example, changing the number of EC2 instances in an Auto Scaling group or modifying throughput of a DynamoDB table can be done through the AWS Management Console or AWS CLI. However automation should be used whenever possible (refer to Use automation when obtaining or scaling resources).Level of risk exposed if this best practice is not established: MediumObtain resources upon detection of impairment to a workload. Scale resources reactively when necessary if availability is impacted, to restore workload availability.Use scaling plans, which are the core component of AWS Auto Scaling, to configure a set of instructions for scaling your resources. If you work with AWS CloudFormation or add tags to AWS resources, you can set up scaling plans for different sets of resources, per application. AWS Auto Scaling provides recommendations for scaling strategies customized to each resource. After you create your scaling plan, AWS Auto Scaling combines dynamic scaling and predictive scaling methods together to support your scaling strategy.AWS Auto Scaling: How Scaling Plans WorkAmazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. You can specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes above this size.What Is Amazon EC2 Auto Scaling?Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling.Managing Throughput Capacity Automatically with DynamoDB Auto ScalingRelated documents:APN Partner: partners that can help you create automated compute solutionsAWS Auto Scaling: How Scaling Plans WorkAWS Marketplace: products that can be used with auto scalingManaging Throughput Capacity Automatically with DynamoDB Auto ScalingWhat Is Amazon EC2 Auto Scaling?\n* Scale resources proactively to meet demand and avoid availability impact.Many AWS services automatically scale to meet demand. If using Amazon EC2 instances or Amazon ECS clusters, you can configure automatic scaling of these to occur based on usage metrics that correspond to demand for your workload. For Amazon EC2, average CPU utilization, load balancer request count, or network bandwidth can be used to scale out (or scale in) EC2 instances. For Amazon ECS, average CPU utilization, load balancer request count, and memory utilization can be used to scale out (or scale in) ECS tasks. Using Target Auto Scaling on AWS, the autoscaler acts like a household thermostat, adding or removing resources to maintain the target value (for example, 70% CPU utilization) that you specify.AWS Auto Scaling can also do Predictive Auto Scaling, which uses machine learning to analyze each resource's historical workload and regularly forecasts the future load for the next two days.Little\u2019s Law helps calculate how many instances of compute (EC2 instances, concurrent Lambda functions, etc.) that you need.L = \u03bbWL = number of instances (or mean concurrency in the system)\u03bb = mean rate at which requests arrive (req/sec)W = mean time that each request spends in the system (sec)For example, at 100 rps, if each request takes 0.5 seconds to process, you will need 50 instances to keep up with demand.Level of risk exposed if this best practice is not established: MediumObtain resources upon detection that more resources are needed for a workload. Scale resources proactively to meet demand and avoid availability impact.Calculate how many compute resources you will need (compute concurrency) to handle a given request rate.Telling Stories About Little's LawWhen you have a historical pattern for usage, set up scheduled scaling for Amazon EC2 auto scaling.Scheduled Scaling for Amazon EC2 Auto ScalingUse AWS predictive scaling.Predictive Scaling for EC2, Powered by Machine LearningRelated documents:AWS Auto Scaling: How Scaling Plans WorkAWS Marketplace: products that can be used with auto scalingManaging Throughput Capacity Automatically with DynamoDB Auto ScalingPredictive Scaling for EC2, Powered by Machine LearningScheduled Scaling for Amazon EC2 Auto ScalingTelling Stories About Little's LawWhat Is Amazon EC2 Auto Scaling?\n\nKnowledge Area: Determining when storage auto scaling is required\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is necessary to determine when storage auto scaling is required?\",\n\"answer_choices\": [{\"answer\": \"Perform load balancing to distribute load across different resources.\",\"is_correct\": \"False\",\"explanation\": \"Load balancing can help to distribute the load across different resources but it is not necessary to determine when storage auto scaling is required.\"},{\"answer\": \"Analyze historical workload to forecast future load for the next two days.\",\"is_correct\": \"True\",\"explanation\": \"Predictive auto scaling can use machine learning to analyze historical workloads and forecast future load for the next two days in order to determine when storage auto scaling is required.\"},{\"answer\": \"Determine the number of EC2 instances in an Auto Scaling group.\",\"is_correct\": \"False\",\"explanation\": \"Determining the number of EC2 instances in an Auto Scaling group does not necessarily determine when storage auto scaling is required.\"},{\"answer\": \"Calculate the number of instances that are required to meet the request rate.\",\"is_correct\": \"True\",\"explanation\": \"Little's Law can be used to calculate the number of instances that are required to meet the request rate in order to determine when storage auto scaling is required.\"}]}", "docs": ["('REL07-BP02 Obtain resources upon detection of impairment to a workload - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_adapt_to_changes_reactive_adapt_auto.html')", "('REL07-BP03 Obtain resources upon detection that more resources are needed for a workload - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_adapt_to_changes_proactive_adapt_auto.html')"]}, {"scenario": "A research institution wants to perform data analysis on a large dataset using AWS compute resources.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Managing S3 object lifecycles", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA research institution wants to perform data analysis on a large dataset using AWS compute resources.`\"\n\nContext:\n\n* Manage the lifecycle of all your data and automatically enforce deletion timelines to minimize the total storage requirements of your workload.Level of risk exposed if this best practice is not established: LowDefine lifecycle policies for all your data classification types.Set automated lifecycle policies to enforce lifecycle rules.Delete unused volumes and snapshots.Aggregate data where applicable based on lifecycle rules.Related documents:Amazon ECR Lifecycle policiesAmazon EFS lifecycle managementAmazon S3 Intelligent-TieringEvaluating Resources with AWS Config RulesManaging your storage lifecycle on Amazon S3Object lifecycle policies in AWS Elemental MediaStoreRelated videos:Amazon S3 Lifecycle\n* Enforce access control with least privileges and mechanisms, including backups, isolation, and versioning, to help protect your data at rest. Prevent operators from granting public access to your data.Different controls including access (using least privilege), backups (see Reliability whitepaper), isolation, and versioning can all help protect your data at rest. Access to your data should be audited using detective mechanisms covered earlier in this paper including CloudTrail, and service level log, such as Amazon Simple Storage Service (Amazon S3) access logs. You should inventory what data is publicly accessible, and plan for how you can reduce the amount of data available over time. Amazon S3 Glacier Vault Lock and Amazon S3 Object Lock are capabilities providing mandatory access control\u2014once a vault policy is locked with the compliance option, not even the root user can change it until the lock expires. The mechanism meets the Books and Records Management requirements of the SEC, CFTC, and FINRA. For more details, see this whitepaper.Level of risk exposed if this best practice is not established: LowEnforce access control: Enforce access control with least privileges, including access to encryption keys.Introduction to Managing Access Permissions to Your Amazon S3 ResourcesSeparate data based on different classification levels: Use different AWS accounts for data classification levels managed by AWS Organizations.AWS OrganizationsReview AWS KMS policies: Review the level of access granted in AWS KMS policies.Overview of managing access to your AWS KMS resourcesReview Amazon S3 bucket and object permissions: Regularly review the level of access granted in Amazon S3 bucket policies. Best practice is to not have publicly readable or writeable buckets. Consider using AWS Config to detect buckets that are publicly available, and Amazon CloudFront to serve content from Amazon S3.AWS Config RulesAmazon S3 + Amazon CloudFront: A Match Made in the CloudEnable Amazon S3 versioning and object lock.Using versioningLocking Objects Using Amazon S3 Object LockUse Amazon S3 Inventory: Amazon S3 inventory is one of the tools you can use to audit and report on the replication and encryption status of your objects.Amazon S3 InventoryReview Amazon EBS and AMI sharing permissions: Sharing permissions can allow images and volumes to be shared to AWS accounts external to your workload.Sharing an Amazon EBS SnapshotShared AMIsRelated documents:AWS KMS Cryptographic Details WhitepaperRelated videos:Securing Your Block Storage on AWS\n* Track, measure, and audit the lifecycle of projects, teams, and environments to avoid using and paying for unnecessary resources.Level of risk exposed if this best practice is not established: LowEnsure that you track the entire lifecycle of the workload. This ensures that when workloads or workload components are no longer required, they can be decommissioned or modified. This is especially useful when you release new services or features. The existing workloads and components may appear to be in use, but should be decommissioned to redirect customers to the new service. Notice previous stages of workloads \u2014 after a workload is in production, previous environments can be decommissioned or greatly reduced in capacity until they are required again.AWS provides a number of management and governance services you can use for entity lifecycle tracking. You can use AWS Config or AWS Systems Manager to provide a detailed inventory of your AWS resources and configuration. It is recommended that you integrate with your existing project or asset management systems to keep track of active projects and products within your organization. Combining your current system with the rich set of events and metrics provided by AWS allows you to build a view of significant lifecycle events and proactively manage resources to reduce unnecessary costs.Refer to the Well-Architected Operational Excellence Pillar whitepaper for more details on implementing entity lifecycle tracking.Implementation stepsPerform workload reviews: As defined by your organizational policies, audit your existing projects. The amount of effort spent in the audit should be proportional to the approximate risk, value, or cost to the organization. Key areas to include in the audit would be risk to the organization of an incident or outage, value, or contribution to the organization (measured in revenue or brand reputation), cost of the workload (measured as total cost of resources and operational costs), and usage of the workload (measured in number of organization outcomes per unit of time). If these areas change over the lifecycle, adjustments to the workload are required, such as full or partial decommissioning.Related documents:AWS ConfigAWS Systems ManagerAWS managed policies for job functionsAWS multiple account billing strategyControl access to AWS Regions using IAM policies\n* Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload. Fetch data from shared storage only as needed. Detach unused volumes to make more resources available.Level of risk exposed if this best practice is not established: LowMigrate data to shared storage when the data has multiple consumers.Fetch data from shared storage only as needed.Delete data as appropriate for your usage patterns, and implement time-to-live (TTL) functionality to manage cached data.Detach volumes from clients that are not actively using them.Related documents:Amazon FSxCaching strategiesWhat is Amazon Elastic File System?What is Amazon S3?\n\nKnowledge Area: Managing S3 object lifecycles\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is a best practice for managing S3 object lifecycles?\", \"answer_choices\": [{\"answer\": \"Enforce access control with least privileges\",\"is_correct\": \"False\",\"explanation\": \"Enforcing access control is important, however it is not a best practice for managing S3 object lifecycles.\"},{\"answer\": \"Track, measure, and audit the lifecycle of projects, teams, and environments\",\"is_correct\": \"False\",\"explanation\": \"Tracking, measuring, and auditing the lifecycle of projects, teams and environments is important, but it is not a best practice for managing S3 object lifecycles.\"},{\"answer\": \"Define lifecycle policies for data classification types\",\"is_correct\": \"True\",\"explanation\": \"Defining lifecycle policies for data classification types is a best practice for managing S3 object lifecycles.\"},{\"answer\": \"Adopt shared storage and single sources of truth\",\"is_correct\": \"False\",\"explanation\": \"Adopting shared storage and single sources of truth is important, but it is not a best practice for managing S3 object lifecycles.\"}]}", "docs": ["('SUS04-BP03 Use lifecycle policies to delete unnecessary data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a4.html')", "('SEC08-BP04 Enforce access control - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_protect_data_rest_access_control.html')", "('COST02-BP06 Track project lifecycle - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_govern_usage_track_lifecycle.html')", "('SUS04-BP06 Use shared file systems or object storage to access common data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a7.html')"]}, {"scenario": "A telecommunications company wants to use AWS to host and manage their network infrastructure.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Selecting the appropriate backup and/or archival solution", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA telecommunications company wants to use AWS to host and manage their network infrastructure.`\"\n\nContext:\n\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n* To minimize storage consumption, only back up data that has business value or is needed to satisfy compliance requirements. Examine backup policies and exclude ephemeral storage that doesn\u2019t provide value in a recovery scenario.Level of risk exposed if this best practice is not established: LowUse your data classification to establish what data needs to be backed up.Exclude data that you can easily recreate.Exclude ephemeral data from your backups.Exclude local copies of data, unless the time required to restore that data from a common location exceeds your service level agreements (SLAs).Related documents:Using AWS Backup to back up and restore Amazon EFS file systemsAmazon EBS snapshotsWorking with backups on Amazon Relational Database Service\n* Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO).Best practices\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n* Having backups and redundant workload components in place is the start of your DR strategy. RTO and RPO are your objectives for restoration of your workload. Set these based on business needs. Implement a strategy to meet these objectives, considering locations and function of workload resources and data. The probability of disruption and cost of recovery are also key factors that help to inform the business value of providing disaster recovery for a workload.Best practices\n\nKnowledge Area: Selecting the appropriate backup and/or archival solution\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is the best practice for selecting an appropriate backup and/or archival solution for a telecommunications company using AWS?\", \n\"answer_choices\": [\n    {\"answer\": \"Monitor data access patterns and migrate archival data to storage designed for that purpose.\", \n    \"is_correct\": \"true\",\n    \"explanation\": \"This is the best practice for selecting an appropriate backup and/or archival solution for a telecommunications company using AWS. By monitoring data access patterns and migrating archival data to storage designed for that purpose, the company can ensure that the optimal storage solution is being used to minimize storage consumption and improve performance.\"},\n    {\"answer\": \"Back up all data, applications, and configuration to meet recovery time objectives and recovery point objectives.\", \n    \"is_correct\": \"false\",\n    \"explanation\": \"This is not the best practice for selecting an appropriate backup and/or archival solution for a telecommunications company using AWS. Backing up all data, applications, and configuration can lead to inefficient use of resources. It is best to back up only data that has business value or is needed to satisfy compliance requirements and to exclude ephemeral storage that doesn\u2019t provide value in a recovery scenario.\"},\n    {\"answer\": \"Use energy-efficient, archival-class storage for active data use cases.\", \n    \"is_correct\": \"false\",\n    \"explanation\": \"This is not the best practice for selecting an appropriate backup and/or archival solution for a telecommunications company using AWS. It is best to use energy-efficient, archival-class storage for infrequently accessed data, and to use Solid State Devices (SSDs) for active data use cases.\"},\n    {\"answer\": \"Exclude local copies of data, unless the time required to restore that data from a common location exceeds service level agreements.\", \n    \"is_correct\": \"false\",\n    \"explanation\": \"This is not the best practice for selecting an appropriate backup and/or archival solution for a telecommunications company using AWS. It is best to exclude ephemeral data from backups and to use data classification to establish what data needs to be backed up.\"}\n]\n}", "docs": ["('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')", "('SUS04-BP08 Back up data only when difficult to recreate - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a9.html')", "('REL 9\\xa0 How do you back up data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-09.html')", "('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')", "('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')", "('REL 13\\xa0 How do you plan for disaster recovery (DR)? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-13.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Selecting the appropriate service for data migration to storage services", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n* Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.Level of risk exposed if this best practice is not established: LowAnalyze your data access and storage patterns.Store data files in efficient file formats such as Parquet to prevent unnecessary processing (for example, when running analytics) and to reduce the total storage provisioned.Use technologies that work natively with compressed data.Use the database engine that best supports your dominant query pattern.Manage your database indexes to ensure index designs support efficient query execution.Select network protocols that reduce the amount of network capacity consumed.Related documents:Athena Compression Support file formatsCOPY from columnar data formats with Amazon RedshiftConverting Your Input Record Format in Kinesis Data FirehoseFormat Options for ETL Inputs and Outputs in AWS GlueImprove query performance on Amazon Athena by Converting to Columnar FormatsLoading compressed data files from Amazon S3 with Amazon RedshiftMonitoring DB load with Performance Insights on Amazon AuroraMonitoring DB load with Performance Insights on Amazon RDSAWS IoT FleetWiseRelated videos:Building Sustainably on AWS\n* Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file storage services as well as cloud data migration options for your workload.In AWS, storage is available in three forms: object, block, and file:Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless computing, Big Data storage or backup and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost.File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems.The following question focuses on these considerations for performance efficiency.When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want.PERF 3: How do you select your storage solution?The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.Best practices:\n\nKnowledge Area: Selecting the appropriate service for data migration to storage services\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When considering data migration to storage services, what is the best practice for minimizing the resources provisioned while supporting your workload?\", \n\"answer_choices\": [\n    {\"answer\": \"Store data files in efficient file formats such as Parquet to prevent unnecessary processing\",\"is_correct\": \"True\",\"explanation\": \"Storing data files in efficient file formats such as Parquet enables the workload to minimize data processing and storage requirements.\"},\n    {\"answer\": \"Use energy-intensive, archival-class storage for infrequently accessed data\",\"is_correct\": \"False\",\"explanation\": \"Archival-class storage should be used for infrequently accessed data and energy-efficient storage should be used.\"},\n    {\"answer\": \"Use technologies that do not work natively with compressed data\",\"is_correct\": \"False\",\"explanation\": \"Compressed data should be used and the technologies should be able to work natively with it.\"},\n    {\"answer\": \"Analyze your data access and storage patterns\",\"is_correct\": \"True\",\"explanation\": \"Analyzing data access and storage patterns is a key best practice for selecting storage that best supports how the data is accessed and stored.\"}\n]}", "docs": ["('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')", "('SUS03-BP05 Use software patterns and architectures that best support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a6.html')", "('Storage - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-storage.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('SUS 4 How do you take advantage of data access and usage patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-04.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Selecting the appropriate storage tier", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* Choose storage systems based on your workload's access patterns and configure them by determining how the workload accesses data. Increase storage efficiency by choosing object storage over block storage. Configure the storage options you choose to match your data access patterns.How you access data impacts how the storage solution performs. Select the storage solution that aligns best to your access patterns, or consider changing your access patterns to align with the storage solution to maximize performance.Creating a RAID 0 array allows you to achieve a higher level of performance for a file system than what you can provision on a single volume. Consider using RAID 0 when I/O performance is more important than fault tolerance. For example, you could use it with a heavily used database where data replication is already set up separately.Select appropriate storage metrics for your workload across all of the storage options consumed for the workload. When using filesystems that use burst credits, create alarms to let you know when you are approaching those credit limits. You must create storage dashboards to show the overall workload storage health.For storage systems that are a fixed size, such as Amazon EBS or Amazon FSx, ensure that you are monitoring the amount of storage used versus the overall storage size and create automation if possible to increase the storage size when reaching a thresholdCommon anti-patterns:You assume that storage performance is adequate if customers are not complaining.You only use one tier of storage, assuming all workloads fit within that tier.Benefits of establishing this best practice: You need a unified operational view, real-time granular data, and historical reference to optimize performance and resource utilization. You can create automatic dashboards and data with one-second granularity to perform metric math on your data and derive operational and utilization insights for your storage needs.Level of risk exposed if this best practice is not established: LowOptimize your storage usage and access patterns: Choose storage systems based on your workload's access patterns and the characteristics of the available storage options. Determine the best place to store data that will enable you to meet your requirements while reducing overhead. Use performance optimizations and access patterns when configuring and interacting with data based on the characteristics of your storage (for example, striping volumes or partitioning data).Select appropriate metrics for storage options: Ensure that you select the appropriate storage metrics for the workload. Each storage option offers various metrics to track how your workload performs over time. Ensure that you are measuring against any storage burst metrics (for example, monitoring burst credits for Amazon EFS). For storage systems that are fixed sized, such as Amazon Elastic Block Store or Amazon FSx, ensure that you are monitoring the amount of storage used versus the overall storage size. Create automation when possible to increase the storage size when reaching a threshold.Monitor metrics: Amazon CloudWatch can collect metrics across the resources in your architecture. You can also collect and publish custom metrics to surface business or derived metrics. Use CloudWatch or third-party solutions to set alarms that indicate when thresholds are breached.Related documents:Amazon EBS Volume TypesAmazon EC2 StorageAmazon EFS: Amazon EFS PerformanceAmazon FSx for Lustre PerformanceAmazon FSx for Windows File Server PerformanceAmazon Glacier: Amazon Glacier DocumentationAmazon S3: Request Rate and Performance ConsiderationsCloud Storage with AWSEBS I/O CharacteristicsMonitoring and understanding Amazon EBS performance using Amazon CloudWatchRelated videos:Deep dive on Amazon EBS (STG303-R1)Optimize your storage performance with Amazon S3 (STG343)Related examples:Amazon EFS CSI DriverAmazon EBS CSI DriverAmazon EFS UtilitiesAmazon EBS AutoscaleAmazon S3 Examples\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n\nKnowledge Area: Selecting the appropriate storage tier\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which storage option should the gaming company choose to host and scale their multiplayer game servers on AWS?\",\n\"answer_choices\": [{\"answer\": \"Amazon EBS\",\"is_correct\": \"False\",\"explanation\": \"Amazon EBS is a block storage system that is more suitable for applications that require low latency access to a fixed set of data, such as a database. It is not well suited for the high-scale and dynamic workload that is required for a multiplayer game.\"},{\"answer\": \"Amazon S3\",\"is_correct\": \"False\",\"explanation\": \"Amazon S3 is an object storage system that can be used for static data but is not suitable for this workload because it does not offer low latency access for dynamic data.\"},{\"answer\": \"Amazon EFS\",\"is_correct\": \"False\",\"explanation\": \"Amazon EFS is a file storage system that is better suited for applications that require low latency access to data but is not well suited for the high-scale and dynamic workload that is required for a multiplayer game.\"},{\"answer\": \"Amazon FSx\",\"is_correct\": \"True\",\"explanation\": \"Amazon FSx provides a high-performance, low latency storage solution that is well suited for the high-scale and dynamic workload that is required for a multiplayer game.\"}]}", "docs": ["('PERF03-BP03 Make decisions based on access patterns and metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_optimize_patterns.html')", "('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')", "('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Selecting the correct data lifecycle for storage", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.Best practices:\n* Manage the lifecycle of all your data and automatically enforce deletion timelines to minimize the total storage requirements of your workload.Level of risk exposed if this best practice is not established: LowDefine lifecycle policies for all your data classification types.Set automated lifecycle policies to enforce lifecycle rules.Delete unused volumes and snapshots.Aggregate data where applicable based on lifecycle rules.Related documents:Amazon ECR Lifecycle policiesAmazon EFS lifecycle managementAmazon S3 Intelligent-TieringEvaluating Resources with AWS Config RulesManaging your storage lifecycle on Amazon S3Object lifecycle policies in AWS Elemental MediaStoreRelated videos:Amazon S3 Lifecycle\n* Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, Solid State Devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Level of risk exposed if this best practice is not established: MediumMonitor your data access patterns.Migrate data to the appropriate technology based on access pattern.Migrate archival data to storage designed for that purpose.Related documents:Amazon EBS volume typesAmazon EC2 instance storeAmazon S3 Intelligent-TieringUsing Amazon S3 storage classesWhat is Amazon CloudWatch?What is Amazon S3 Glacier?Related videos:Architectural Patterns for Data Lakes on AWS\n* Your defined lifecycle strategy should be based on sensitivity level as well as legal and organization requirements. Aspects including the duration for which you retain data, data destruction processes, data access management, data transformation, and data sharing should be considered. When choosing a data classification methodology, balance usability versus access. You should also accommodate the multiple levels of access and nuances for implementing a secure, but still usable, approach for each level. Always use a defense in depth approach and reduce human access to data and mechanisms for transforming, deleting, or copying data. For example, require users to strongly authenticate to an application, and give the application, rather than the users, the requisite access permission to perform action at a distance. In addition, ensure that users come from a trusted network path and require access to the decryption keys. Use tools, such as dashboards and automated reporting, to give users information from the data rather than giving them direct access to the data.Level of risk exposed if this best practice is not established: LowIdentify data types: Identify the types of data that you are storing or processing in your workload. That data could be text, images, binary databases, and so forth.Related documents:Data Classification WhitepaperGetting started with Amazon MacieRelated videos:Introducing the New Amazon Macie\n* Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Components might become idle from lack of use because of changes in user behavior over time. Revise patterns and architecture to consolidate under-utilized components to increase overall utilization. Retire components that are no longer required. Understand the performance of your workload components, and optimize the components that consume the most resources. Be aware of the devices your customers use to access your services, and implement patterns to minimize the need for device upgrades.The following question focuses on these considerations for sustainability:Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.Implement a data classification policy: Classify data to understand its significance to business outcomes. Use this information to determine when you can move data to more energy-efficient storage or safely delete it.Use technologies that support data access and storage patterns: Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, solid state devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Use lifecycle policies to delete unnecessary data: Manage the lifecycle of all your data and automatically enforce deletion timelines to minimize the total storage requirements of your workload.Minimize over-provisioning in block storage: To minimize total provisioned storage, create block storage with size allocations that are appropriate for the workload. Use elastic volumes to expand storage as data grows without having to resize storage attached to compute resources. Regularly review elastic volumes and shrink over-provisioned volumes to fit the current data size.Remove unneeded or redundant data: Duplicate data only when necessary to minimize total storage consumed. Use backup technologies that deduplicate data at the file and block level. Limit the use of Redundant Array of Independent Drives (RAID) configurations except where required to meet SLAs.Use shared file systems or object storage to access common data: Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload. Fetch data from shared storage only as needed. Detach unused volumes to free resources. Minimize data movement across networks: Use shared storage and access data from regional data stores to minimize the total networking resources required to support data movement for your workload.Back up data only when difficult to recreate: To minimize storage consumption, only back up data that has business value or is required to satisfy compliance requirements. Examine backup policies and exclude ephemeral storage that doesn\u2019t provide value in a recovery scenario.SUS 4: How do you take advantage of data access and usage patterns to support your sustainability goals?Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.\n\nKnowledge Area: Selecting the correct data lifecycle for storage\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"How do you take advantage of data access and usage patterns to support your sustainability goals?\", \n\"answer_choices\": [\n    {\"answer\": \"Create block storage with size allocations that are appropriate for the workload and use elastic volumes to expand storage as data grows\",\"is_correct\": \"False\",\"explanation\": \"Creating block storage and using elastic volumes can help minimize storage requirements, but is not the only way to take advantage of data access and usage patterns\"},\n    {\"answer\": \"Use energy-efficient, archival-class storage for infrequently accessed data\",\"is_correct\": \"True\",\"explanation\": \"Using energy-efficient, archival-class storage for infrequently accessed data is a good way to take advantage of data access and usage patterns and reduce the total storage requirements of a workload\"},\n    {\"answer\": \"Monitor your data access patterns and migrate data to the appropriate technology based on access pattern\",\"is_correct\": \"True\",\"explanation\": \"Monitoring data access patterns and migrating data to the appropriate technology based on access pattern is a good way to take advantage of data access and usage patterns and reduce storage requirements\"},\n    {\"answer\": \"Fetch data from shared storage only as needed and detach unused volumes to free resources\",\"is_correct\": \"True\",\"explanation\": \"Fetching data from shared storage only as needed and detaching unused volumes to free resources is a good way to take advantage of data access and usage patterns and reduce storage requirements\"}\n]}", "docs": ["('SUS 4 How do you take advantage of data access and usage patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-04.html')", "('SUS04-BP03 Use lifecycle policies to delete unnecessary data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a4.html')", "('SUS04-BP02 Use technologies that support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a3.html')", "('SEC07-BP04 Define data lifecycle management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_data_classification_lifecycle_management.html')", "('Data patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-data-patterns.html')"]}, {"scenario": "A large corporation wants to move their on-premises compute to the cloud for increased scalability and security.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 1: Design cost-optimized storage solutions", "focus": "Skills in", "item": "Selecting the most cost-effective storage service for a workload", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA large corporation wants to move their on-premises compute to the cloud for increased scalability and security.`\"\n\nContext:\n\n* Using the appropriate instances and resources for your workload is key to cost savings. For example, a reporting process might take five hours to run on a smaller server but one hour to run on a larger server that is twice as expensive. Both servers give you the same outcome, but the smaller server incurs more cost over time.A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. For example, rather than maintaining servers to deliver email, you can use a service that charges on a per-message basis.AWS offers a variety of flexible and cost-effective pricing options to acquire instances from Amazon EC2 and other services in a way that best fits your needs. On-DemandInstances allow you to pay for compute capacity by the hour, with no minimum commitments required. Savings Plans and Reserved Instances offer savings of up to 75% o\ufb00 On-Demand pricing. With Spot Instances, you can leverage unused Amazon EC2 capacity and offer savings of up to 90% o\ufb00 On-Demand pricing. Spot Instances are appropriate where the system can tolerate using a \ufb02eet of servers where individual servers can come and go dynamically, such as stateless web servers, batch processing, or when using HPC and big data.Appropriate service selection can also reduce usage and costs; such as CloudFront to minimize data transfer, or completely eliminate costs, such as utilizing Amazon Aurora on RDS to remove expensive database licensing costs.The following questions focus on these considerations for cost optimization.By factoring in cost during service selection, and using tools such as Cost Explorer and AWS Trusted Advisor to regularly review your AWS usage, you can actively monitor your utilization and adjust your deployments accordingly.COST 5: How do you evaluate cost when you select services?Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.COST 6: How do you meet cost targets when you select resource type, size and number?Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number.COST 7: How do you use pricing models to reduce cost?Use the pricing model that is most appropriate for your resources to minimize expense.COST 8: How do you plan for data transfer charges?Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time.\n* Workloads often have cost requirements for operation. Use internal cost controls to select resource types and sizes based on predicted resource need.Determine which workload components could be replaced with fully managed services, such as managed databases, in-memory caches, and ETL services. Reducing your operational workload allows you to focus resources on business outcomes.For cost requirement best practices, refer to the Cost-Effective Resources section of the Cost Optimization Pillar whitepaper.Common anti-patterns:You only use one family of instances.You do not evaluate licensed solutions versus open-source solutionsYou only use block storage.You deploy common software on EC2 instances and Amazon EBS or ephemeral volumes that are available as a managed service.Benefits of establishing this best practice: Considering cost when making your selections will allow you to enable other investments.Level of risk exposed if this best practice is not established: MediumOptimize workload components to reduce cost: Right size workload components and enable elasticity to reduce cost and maximize component efficiency. Determine which workload components can be replaced with managed services when appropriate, such as managed databases, in-memory caches, and reverse proxies.Related documents:AWS Architecture CenterAWS Partner NetworkAWS Solutions LibraryAWS Knowledge CenterAWS Compute OptimizerRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)This is my ArchitectureOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:AWS SamplesAWS SDK ExamplesRightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n\nKnowledge Area: Selecting the most cost-effective storage service for a workload\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following services should be considered when selecting the most cost-effective storage service for a workload?\", \n\"answer_choices\": [{\"answer\": \"Amazon S3\",\"is_correct\": \"True\",\"explanation\": \"Amazon S3 is a cost-effective storage solution for storing large quantities of data at a lower cost than traditional storage solutions.\"},{\"answer\": \"Amazon EC2 Instances\",\"is_correct\": \"False\",\"explanation\": \"Amazon EC2 Instances are used for computing tasks, not for storage.\"},{\"answer\": \"Amazon EBS\",\"is_correct\": \"True\",\"explanation\": \"Amazon EBS is a cost-effective storage solution for storing smaller quantities of data and providing block storage for applications needing higher performance and throughput.\"},{\"answer\": \"Amazon Aurora\",\"is_correct\": \"True\",\"explanation\": \"Amazon Aurora is a cost-effective managed database service that provides higher performance and scalability than traditional database solutions.\"}]}", "docs": ["('Cost-effective resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-cereso.html')", "('PERF01-BP03 Factor cost requirements into decisions - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_performing_architecture_cost.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Knowledge of", "item": "AWS cost management service features (for example, cost allocation tags, multi-account billing)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* The increased flexibility and agility that the cloud enables encourages innovation and fast-paced development and deployment. It eliminates the manual processes and time associated with provisioning on-premises infrastructure, including identifying hardware specifications, negotiating price quotations, managing purchase orders, scheduling shipments, and then deploying the resources. However, the ease of use and virtually unlimited on-demand capacity requires a new way of thinking about expenditures.Many businesses are composed of multiple systems run by various teams. The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget.In AWS, you create an account structure with AWS Organizations or AWS Control Tower, which provides separation and assists in allocation of your costs and usage. You can also use resource tagging to apply business and organization information to your usage and cost. Use AWS Cost Explorer for visibility into your cost and usage, or create customized dashboards and analytics with Amazon Athena and Amazon QuickSight. Controlling your cost and usage is done by notifications through AWS Budgets, and controls using AWS Identity and Access Management (IAM), and Service Quotas.The following questions focus on these considerations for cost optimization.You can use cost allocation tags to categorize and track your AWS usage and costs. When you apply tags to your AWS resources (such as EC2 instances or S3 buckets), AWS generates a cost and usage report with your usage and your tags. You can apply tags that represent organization categories (such as cost centers, workload names, or owners) to organize your costs across multiple services.Ensure you use the right level of detail and granularity in cost and usage reporting and monitoring. For high level insights and trends, use daily granularity with AWS Cost Explorer. For deeper analysis and inspection use hourly granularity in AWS Cost Explorer, or Amazon Athena and Amazon QuickSight with the Cost and Usage Report (CUR) at an hourly granularity.Combining tagged resources with entity lifecycle tracking (employees, projects) makes it possible to identify orphaned resources or projects that are no longer generating value to the organization and should be decommissioned. You can set up billing alerts to notify you of predicted overspending.COST 2: How do you govern usage?Establish policies and mechanisms to ensure that appropriate costs are incurred while objectives are achieved. By employing a checks-and-balances approach, you can innovate without overspending.COST 3: How do you monitor usage and cost?Establish policies and procedures to monitor and appropriately allocate your costs. This allows you to measure and improve the cost efficiency of this workload.COST 4: How do you decommission resources?Implement change control and resource management from project inception to end-of-life. This ensures you shut down or terminate unused resources to reduce waste.\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Configure AWS Cost Explorer and AWS Budgets inline with your organization policies.Level of risk exposed if this best practice is not established: HighTo modify usage and adjust costs, each person in your organization must have access to their cost and usage information. It is recommended that all workloads and teams have the following tooling configured when they use the cloud:Reports: Summarize of all cost and usage informationNotifications: Provide notifications when cost or usage is outside of defined limits.Current State: Configure a dashboard showing current levels of cost and usage. The dashboard should be available in a highly visible place within the work environment (similar to an operations dashboard).Trending: Provide the capability to show the variability in cost and usage over the required period of time, with the required granularity.Forecasts: Provide the capability to show estimated future costs.Tracking: Show the current cost and usage against configured goals or targets.Analysis: Provide the capability for team members to perform custom and deep analysis down to the hourly granularity, with all possible dimensions.You can use AWS native tooling, such as AWS Cost Explorer, AWS Budgets, and Amazon Athena with Amazon QuickSight to provide this capability. You can also use third-party tooling \u2014 however, you must ensure that the costs of this tooling provide value to your organization.Implementation stepsCreate a Cost Optimization group: Configure your account and create a group that has access to the required Cost and Usage reports. This group must include representatives from all teams that own or manage an application. This certifies that every team has access to their cost and usage information.Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set a budget for the overall account spend, and a budget for the workload by using tags.Configure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts. Create a dashboard for the workload that tracks overall spend, and key usage metrics for the workload.Configure advanced tooling: Optionally, you can create custom tooling for your organization that provides additional detail and granularity. You can implement advanced analysis capability using Amazon Athena, and dashboards using Amazon QuickSight.Related documents:Tagging AWS resourcesAnalyzing your costs with AWS BudgetsAnalyzing your costs with Cost ExplorerManaging AWS Cost and Usage ReportsRelated examples:Well-Architected Labs - AWS Account SetupWell-Architected Labs: Billing VisualizationWell-Architected Labs: Cost and Governance UsageWell-Architected Labs: Cost and Usage AnalysisWell-Architected Labs: Cost and Usage Visualization\n\nKnowledge Area: AWS cost management service features (for example, cost allocation tags, multi-account billing)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is an example of a best practice for cost and usage monitoring in AWS?\", \n\"answer_choices\": [\n{\"answer\": \"Configure Amazon RDS for cost and usage monitoring\", \"is_correct\": \"false\", \"explanation\": \"Amazon RDS is a managed service and does not provide cost and usage monitoring.\"}, \n{\"answer\": \"Configure AWS Cost Explorer and AWS Budgets inline with your organization policies\", \"is_correct\": \"true\", \"explanation\": \"AWS Cost Explorer and AWS Budgets can be configured inline with your organization policies to help monitor usage and cost.\"}, \n{\"answer\": \"Configure Amazon DynamoDB for cost and usage monitoring\", \"is_correct\": \"false\", \"explanation\": \"Amazon DynamoDB is a managed service and does not provide cost and usage monitoring.\"}, \n{\"answer\": \"Configure Amazon EC2 for cost and usage monitoring\", \"is_correct\": \"false\", \"explanation\": \"Amazon EC2 is a building-block service and does not provide cost and usage monitoring.\"} \n]}", "docs": ["('Expenditure and usage awareness - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-aware.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('COST03-BP04 Configure billing and cost management tools - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_monitor_usage_config_tools.html')"]}, {"scenario": "An education organization wants to use AWS to deliver online learning courses to students globally.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Knowledge of", "item": "AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nAn education organization wants to use AWS to deliver online learning courses to students globally.`\"\n\nContext:\n\n* Configure AWS Cost Explorer and AWS Budgets inline with your organization policies.Level of risk exposed if this best practice is not established: HighTo modify usage and adjust costs, each person in your organization must have access to their cost and usage information. It is recommended that all workloads and teams have the following tooling configured when they use the cloud:Reports: Summarize of all cost and usage informationNotifications: Provide notifications when cost or usage is outside of defined limits.Current State: Configure a dashboard showing current levels of cost and usage. The dashboard should be available in a highly visible place within the work environment (similar to an operations dashboard).Trending: Provide the capability to show the variability in cost and usage over the required period of time, with the required granularity.Forecasts: Provide the capability to show estimated future costs.Tracking: Show the current cost and usage against configured goals or targets.Analysis: Provide the capability for team members to perform custom and deep analysis down to the hourly granularity, with all possible dimensions.You can use AWS native tooling, such as AWS Cost Explorer, AWS Budgets, and Amazon Athena with Amazon QuickSight to provide this capability. You can also use third-party tooling \u2014 however, you must ensure that the costs of this tooling provide value to your organization.Implementation stepsCreate a Cost Optimization group: Configure your account and create a group that has access to the required Cost and Usage reports. This group must include representatives from all teams that own or manage an application. This certifies that every team has access to their cost and usage information.Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set a budget for the overall account spend, and a budget for the workload by using tags.Configure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts. Create a dashboard for the workload that tracks overall spend, and key usage metrics for the workload.Configure advanced tooling: Optionally, you can create custom tooling for your organization that provides additional detail and granularity. You can implement advanced analysis capability using Amazon Athena, and dashboards using Amazon QuickSight.Related documents:Tagging AWS resourcesAnalyzing your costs with AWS BudgetsAnalyzing your costs with Cost ExplorerManaging AWS Cost and Usage ReportsRelated examples:Well-Architected Labs - AWS Account SetupWell-Architected Labs: Billing VisualizationWell-Architected Labs: Cost and Governance UsageWell-Architected Labs: Cost and Usage AnalysisWell-Architected Labs: Cost and Usage Visualization\n* Implement tooling and dashboards to monitor cost proactively for the workload. Regularly review the costs with configured tools or out of the box tools, do not just look at costs and categories when you receive notifications. Monitoring and analyzing costs proactively helps to identify positive trends and allows you to promote them throughout your organization.Level of risk exposed if this best practice is not established: LowIt is recommended to monitor cost and usage proactively within your organization, not just when there are exceptions or anomalies. Highly visible dashboards throughout your office or work environment ensure that key people have access to the information they need, and indicate the organization\u2019s focus on cost optimization. Visible dashboards allow you to actively promote successful outcomes and implement them throughout your organization.Create a daily or frequent routine to use AWS Cost Explorer or any other dashboard such as Amazon QuickSight to see the costs and analyze proactively. Analyze AWS service usage and costs at the AWS account-level, workload-level, or specific AWS service-level with grouping and filtering, and validate whether they are expected or not. Use the hourly- and resource-level granularity and tags to filter and identify incurring costs for the top resources. You can also build your own reports with the Cost Intelligence Dashboard, an Amazon QuickSight solution built by AWS Solutions Architects, and compare your budgets with the actual cost and usage.Implementation stepsReport on cost optimization: Set up a regular cycle to discuss and analyze the efficiency of the workload. Using the metrics established, report on the metrics achieved and the cost of achieving them. Identify and fix any negative trends, and identify positive trends to promote across your organization. Reporting should involve representatives from the application teams and owners, finance, and management.Create and enable daily granularity AWS Budgets for the cost and usage to take timely actions to prevent any potential cost overruns: AWS Budgets allow you to configure alert notifications, so you stay informed if any of your budget types fall out of your pre-configured thresholds. The best way to leverage AWS Budgets is to set your expected cost and usage as your limits, so that anything above your budgets can be considered overspend.Create AWS Cost Anomaly Detection for cost monitor: AWS Cost Anomaly Detection uses advanced Machine Learning technology to identify anomalous spend and root causes, so you can quickly take action. It allows you to configure cost monitors that define spend segments you want to evaluate (for example, individual AWS services, member accounts, cost allocation tags, and cost categories), and lets you set when, where, and how you receive your alert notifications. For each monitor, attach multiple alert subscriptions for business owners and technology teams, including a name, a cost impact threshold, and alerting frequency (individual alerts, daily summary, weekly summary) for each subscription.Use AWS Cost Explorer or integrate your AWS Cost and Usage Report (CUR) data with Amazon QuickSight dashboards to visualize your organization\u2019s costs: AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. The Cost Intelligence Dashboard is a customizable and accessible dashboard to help create the foundation of your own cost management and optimization tool.Related documents:AWS BudgetsAWS Cost ExplorerDaily Cost and Usage BudgetsAWS Cost Anomaly DetectionRelated examples:Well-Architected Labs: VisualizationWell-Architected Labs: Advanced VisualizationWell-Architected Labs: Cloud Intelligence DashboardsWell-Architected Labs: Cost VisualizationAWS Cost Anomaly Detection Alert with Slack\n\nKnowledge Area: AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS cost management tool should an education organization use to deliver online learning courses to students globally?\", \n\"answer_choices\": [\n{\"answer\": \"AWS Cost Explorer\",\"is_correct\": \"true\", \"explanation\": \"AWS Cost Explorer is an easy-to-use interface that provides visibility into usage and costs to optimize budget and identify trends.\" },\n{\"answer\": \"Amazon QuickSight\",\"is_correct\": \"false\", \"explanation\": \"Amazon QuickSight is a business intelligence service that provides visualizations, but it is not a cost management tool.\" },\n{\"answer\": \"AWS Budgets\",\"is_correct\": \"false\", \"explanation\": \"AWS Budgets allow you to set a budget for the overall account spend and a budget for the workload, but it is not a cost management tool.\" },\n{\"answer\": \"AWS Cost and Usage Report\",\"is_correct\": \"false\", \"explanation\": \"AWS Cost and Usage Report provides comprehensive cost and usage information, but it is not a cost management tool.\" }\n]}", "docs": ["('COST03-BP04 Configure billing and cost management tools - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_monitor_usage_config_tools.html')", "('COST01-BP06 Monitor cost proactively - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_cloud_financial_management_proactive_process.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Knowledge of", "item": "AWS global infrastructure (for example, Availability Zones, AWS Regions)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* Use the cloud location options available to reduce network latency or improve throughput. Use AWS Regions, Availability Zones, placement groups, and edge locations such as AWS Outposts, AWS Local Zones, and AWS Wavelength, to reduce network latency or improve throughput.The AWS Cloud infrastructure is built around Regions and Availability Zones. A Region is a physical location in the world having multiple Availability Zones.Availability Zones consist of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. These Availability Zones offer you the ability to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data centerChoose the appropriate Region or Regions for your deployment based on the following key elements:Where your users are located: Choosing a Region close to your workload\u2019s users ensures lower latency when they use the workload.Where your data is located: For data-heavy applications, the major bottleneck in latency is data transfer. Application code should execute as close to the data as possible.Other constraints: Consider constraints such as security and compliance.Amazon EC2 provides placement groups for networking. A placement group is a logical grouping of instances to decrease latency or increase reliability. Using placement groups with supported instance types and an Elastic Network Adapter (ENA) enables workloads to participate in a low-latency, 25 Gbps network. Placement groups are recommended for workloads that benefit from low network latency, high network throughput, or both. Using placement groups has the benefit of lowering jitter in network communications.Latency-sensitive services are delivered at the edge using a global network of edge locations. These edge locations commonly provide services such as content delivery network (CDN) and domain name system (DNS). By having these services at the edge, workloads can respond with low latency to requests for content or DNS resolution. These services also provide geographic services such as geo targeting of content (providing different content based on the end users\u2019 location), or latency-based routing to direct end users to the nearest Region (minimum latency).Amazon CloudFront is a global CDN that can be used to accelerate both static content such as images, scripts, and videos, as well as dynamic content such as APIs or web applications. It relies on a global network of edge locations that will cache the content and provide high-performance network connectivity to your users. CloudFront also accelerates many other features such as content uploading and dynamic applications, making it a performance addition to all applications serving traffic over the internet. Lambda@Edge is a feature of Amazon CloudFront that will let you run code closer to users of your workload, which improves performance and reduces latency.Amazon Route 53 is a highly available and scalable cloud DNS web service. It\u2019s designed to give developers and businesses an extremely reliable and cost-effective way to route end users to internet applications by translating names, like www.example.com, into numeric IP addresses, like 192.168.2.1, that computers use to connect to each other. Route 53 is fully compliant with IPv6.AWS Outposts is designed for workloads that need to remain on-premises due to latency requirements, where you want that workload to run seamlessly with the rest of your other workloads in AWS. AWS Outposts are fully managed and configurable compute and storage racks built with AWS-designed hardware that allow you to run compute and storage on-premises, while seamlessly connecting to the broad array of AWS services in in the cloud.AWS Local Zones is designed to run workloads that require single-digit millisecond latency, like video rendering and graphics intensive, virtual desktop applications. Local Zones allow you to gain all the benefits of having compute and storage resources closer to end-users.AWS Wavelength is designed to deliver ultra-low latency applications to 5G devices by extending AWS infrastructure, services, APIs, and tools to 5G networks. Wavelength embeds storage and compute inside telco providers 5G networks to help your 5G workload if it requires single-digit millisecond latency, such as IoT devices, game streaming, autonomous vehicles, and live media production.Use edge services to reduce latency and to enable content caching. Ensure that you have configured cache control correctly for both DNS and HTTP/HTTPS to gain the most benefit from these approaches.Common anti-patterns:You consolidate all workload resources into one geographic location.You chose the closest region to your location but not to the workload end user.Benefits of establishing this best practice: You must ensure that your network is available wherever you want to reach customers. Using the AWS private global network ensures that your customers get the lowest latency experience by deploying workloads into the locations nearest them.Level of risk exposed if this best practice is not established: MediumReduce latency by selecting the correct locations: Identify where your users and data are located. Take advantage of AWS Regions, Availability Zones, placement groups, and edge locations to reduce latency.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n\nKnowledge Area: AWS global infrastructure (for example, Availability Zones, AWS Regions)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following strategies should a financial services company use to minimize latency and improve throughput when running a highly available and secure infrastructure to process financial transactions?\", \n\"answer_choices\": [{\"answer\": \"Consolidate all workloads into one geographic location\",\"is_correct\": \"false\",\"explanation\": \"This is an anti-pattern and will increase latency\"}, \n{\"answer\": \"Choose the closest region to the workload end user\",\"is_correct\": \"true\",\"explanation\": \"Choosing a region close to the workload's users will ensure lower latency when they use the workload.\"}, \n{\"answer\": \"Deploy resources into the nearest Region with the lowest latency\",\"is_correct\": \"false\",\"explanation\": \"The workload end user should be considered first when selecting a region to minimize latency\"}, \n{\"answer\": \"Deploy resources into multiple AWS Regions\",\"is_correct\": \"false\",\"explanation\": \"Deploying resources into multiple AWS Regions is a good strategy, but not the only one. Other strategies such as using Availability Zones, placement groups, and edge locations should also be considered.\"}]}", "docs": ["('PERF05-BP06 Choose your workload\u2019s location based on network requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_location.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Knowledge of", "item": "AWS purchasing options (for example, Spot Instances, Reserved Instances, Savings Plans)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* Using the appropriate instances and resources for your workload is key to cost savings. For example, a reporting process might take five hours to run on a smaller server but one hour to run on a larger server that is twice as expensive. Both servers give you the same outcome, but the smaller server incurs more cost over time.A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. For example, rather than maintaining servers to deliver email, you can use a service that charges on a per-message basis.AWS offers a variety of flexible and cost-effective pricing options to acquire instances from Amazon EC2 and other services in a way that best fits your needs. On-DemandInstances allow you to pay for compute capacity by the hour, with no minimum commitments required. Savings Plans and Reserved Instances offer savings of up to 75% o\ufb00 On-Demand pricing. With Spot Instances, you can leverage unused Amazon EC2 capacity and offer savings of up to 90% o\ufb00 On-Demand pricing. Spot Instances are appropriate where the system can tolerate using a \ufb02eet of servers where individual servers can come and go dynamically, such as stateless web servers, batch processing, or when using HPC and big data.Appropriate service selection can also reduce usage and costs; such as CloudFront to minimize data transfer, or completely eliminate costs, such as utilizing Amazon Aurora on RDS to remove expensive database licensing costs.The following questions focus on these considerations for cost optimization.By factoring in cost during service selection, and using tools such as Cost Explorer and AWS Trusted Advisor to regularly review your AWS usage, you can actively monitor your utilization and adjust your deployments accordingly.COST 5: How do you evaluate cost when you select services?Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.COST 6: How do you meet cost targets when you select resource type, size and number?Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number.COST 7: How do you use pricing models to reduce cost?Use the pricing model that is most appropriate for your resources to minimize expense.COST 8: How do you plan for data transfer charges?Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time.\n* Permanently running resources should utilize reserved capacity such as Savings Plans or Reserved Instances. Short-term capacity is configured to use Spot Instances, or Spot Fleet. On-Demand Instances are only used for short-term workloads that cannot be interrupted and do not run long enough for reserved capacity, between 25% to 75% of the period, depending on the resource type.Level of risk exposed if this best practice is not established: LowConsider the requirements of the workload components and understand the potential pricing models. Define the availability requirement of the component. Determine if there are multiple independent resources that perform the function in the workload, and what the workload requirements are over time. Compare the cost of the resources using the default On-Demand pricing model and other applicable models. Factor in any potential changes in resources or workload components.Implementation stepsImplement pricing models: Using your analysis results, purchase Savings Plans (SPs), Reserved Instances (RIs) or implement Spot Instances. If it is your first RI purchase then choose the top 5 or 10 recommendations in the list, then monitor and analyze the results over the next month or two. Purchase small numbers of commitment discounts regular cycles, for example every two weeks or monthly. Implement Spot Instances for workloads that can be interrupted or are stateless.Workload review cycle: Implement a review cycle for the workload that specifically analyzes pricing model coverage. Once the workload has the required coverage, purchase additional commitment discounts every two to four weeks, or as your organization usage changes.Related documents:Accessing Reserved Instance recommendationsEC2 FleetHow to Purchase Reserved InstancesInstance purchasing optionsSpot InstancesRelated videos:Save up to 90% and run production workloads on Spot\n* This study guide will help you pass the newer AWS Certified Solutions Architect - Associate exam. Ideally, you should reference this guide while working through the following material:\n\nKnowledge Area: AWS purchasing options (for example, Spot Instances, Reserved Instances, Savings Plans)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"Which pricing model would be most appropriate for a transportation company wanting to process real-time data from connected vehicles to optimize their operations?\",\n\"answer_choices\": [{\"answer\": \"On-Demand Instances\",\"is_correct\": \"False\",\"explanation\": \"On-Demand Instances are suitable for short-term workloads that cannot be interrupted and do not run long enough for reserved capacity.\"},{\"answer\": \"Savings Plans\",\"is_correct\": \"True\",\"explanation\": \"Savings Plans are appropriate for permanently running resources and offer savings of up to 75% off On-Demand pricing.\"},{\"answer\": \"Spot Instances\",\"is_correct\": \"True\",\"explanation\": \"Spot Instances offer savings of up to 90% off On-Demand pricing and are appropriate for workloads that can tolerate using a fleet of servers where individual servers can come and go dynamically.\"},{\"answer\": \"Reserved Instances\",\"is_correct\": \"True\",\"explanation\": \"Reserved Instances are appropriate for permanently running resources and offer savings of up to 75% off On-Demand pricing.\"}]}", "docs": ["('Cost-effective resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-cereso.html')", "('COST07-BP04 Implement pricing models for all components of this workload - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_pricing_model_implement_models.html')", "('GitHub - keenanromain/AWS-SAA-C02-Study-Guide: How to become a certified AWS Solutions Architect', 'https://github.com/keenanromain/AWS-SAA-C02-Study-Guide')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Knowledge of", "item": "Distributed compute strategies (for example, edge processing)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Distributed systems rely on communications networks to interconnect components (such as servers or services). Your workload must operate reliably despite data loss or latency over these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices enable workloads to withstand stresses or failures, more quickly recover from them, and mitigate the impact of such impairments. The result is improved mean time to recovery (MTTR).Best practices\n* Distributed systems rely on communications networks to interconnect components, such as servers or services. Your workload must operate reliably despite data loss or latency in these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices prevent failures and improve mean time between failures (MTBF).Best practices\n* The optimal compute solution for a workload varies based on application design, usage patterns, and configuration settings. Architectures can use different compute solutions for various components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency.Best practices\n* Compute resources in your workload require multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more.Best practices\n* The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.Best practices\n* Optimize resources provided to team members to minimize the sustainability impact while supporting their needs. For example, perform complex operations, such as rendering and compilation, on highly utilized shared cloud desktops instead of on underutilized high-powered single-user systems.Level of risk exposed if this best practice is not established: LowProvision workstations and other devices to align with how they\u2019re used.Use virtual desktops and application streaming to limit upgrade and device requirements.Move processor or memory-intensive tasks to the cloud.Evaluate the impact of processes and systems on your device lifecycle, and select solutions that minimize the requirement for device replacement while satisfying business requirements.Implement remote management for devices to reduce required business travel.Related documents:What is Amazon WorkSpaces?Amazon AppStream 2.0 DocumentationNICE DCVAWS Systems Manager Fleet ManagerRelated videos:Building Sustainably on AWS\n* Graphics Processing Units (GPUs) can be a source of high-power consumption, and many GPU workloads are highly variable, such as rendering, transcoding, and machine learning training and modeling. Only run GPU instances for the time needed, and decommission them with automation when not required to minimize resources consumed.Level of risk exposed if this best practice is not established: LowUse GPUs only for tasks where they\u2019re more efficient than CPU-based alternatives.Use automation to release GPU instances when not in use.Use flexible graphics acceleration rather than dedicated GPU instances.Take advantage of custom-purpose hardware that is specific to your workload.Related documents:Accelerated ComputingAWS InferentiaAWS TrainiumAccelerated Computing for EC2 InstancesAmazon EC2 VT1 InstancesAmazon Elastic Graphics\n* A scalable workload provides elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time.Best practices\n* There are five design principles for performance efficiency in the cloud:Democratize advanced technologies: Make advanced technology implementation easier for your team by delegating complex tasks to your cloud vendor. Rather than asking your IT team to learn about hosting and running a new technology, consider consuming the technology as a service. For example, NoSQL databases, media transcoding, and machine learning are all technologies that require specialized expertise. In the cloud, these technologies become services that your team can consume, allowing your team to focus on product development rather than resource provisioning and management.Go global in minutes: Deploying your workload in multiple AWS Regions around the world allows you to provide lower latency and a better experience for your customers at minimal cost.Use serverless architectures: Serverless architectures remove the need for you to run and maintain physical servers for traditional compute activities. For example, serverless storage services can act as static websites (removing the need for web servers) and event services can host code. This removes the operational burden of managing physical servers, and can lower transactional costs because managed services operate at cloud scale.Experiment more often: With virtual and automatable resources, you can quickly carry out comparative testing using different types of instances, storage, or configurations.Consider mechanical sympathy: Understand how cloud services are consumed and always use the technology approach that aligns best with your workload goals. For example, consider data access patterns when you select database or storage approaches.\n\nKnowledge Area: Distributed compute strategies (for example, edge processing)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What would be the best practice to ensure the healthcare company's workload operates reliably despite data loss or latency over communications networks?\", \n\"answer_choices\": [\n    {\"answer\": \"Implement remote management for devices to reduce required business travel\",\"is_correct\": \"False\", \"explanation\": \"This best practice does not address data loss or latency over communications networks.\"},\n    {\"answer\": \"Optimize resources provided to team members to minimize the sustainability impact while supporting their needs\",\"is_correct\": \"False\", \"explanation\": \"This best practice does not address data loss or latency over communications networks.\"},\n    {\"answer\": \"Components of the distributed system must operate in a way that does not negatively impact other components or the workload\",\"is_correct\": \"True\", \"explanation\": \"This best practice will enable the workload to withstand stresses or failures, more quickly recover from them, and mitigate the impact of such impairments.\"},\n    {\"answer\": \"Use GPUs only for tasks where they're more efficient than CPU-based alternatives\",\"is_correct\": \"False\", \"explanation\": \"This best practice does not address data loss or latency over communications networks.\"}\n]}", "docs": ["('REL 5\\xa0 How do you design interactions in a distributed system to mitigate or withstand failures? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-05.html')", "('REL 4\\xa0 How do you design interactions in a distributed system to prevent failures? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-04.html')", "('PERF 2\\xa0 How do you select your compute solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-02.html')", "('SEC 6\\xa0 How do you protect your compute resources? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-06.html')", "('PERF 5\\xa0 How do you configure your networking solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-05.html')", "('SUS02-BP05 Optimize team member resources for activities performed - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_user_a6.html')", "('SUS05-BP04 Optimize your use of GPUs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a5.html')", "('REL 7\\xa0 How do you design your workload to adapt to changes in demand? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-07.html')", "('Design principles - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-dp.html')"]}, {"scenario": "A company wants to build a mobile application that needs to access and store data in the cloud.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Knowledge of", "item": "Hybrid compute options (for example, AWS Outposts, AWS Snowball Edge)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to build a mobile application that needs to access and store data in the cloud.`\"\n\nContext:\n\n* Understand how your workload can benefit from the use of different compute options, such as instances, containers and functions.Desired outcome: By understanding all of the compute options available, you will be aware of the opportunities to increase performance, reduce unnecessary infrastructure costs, and lower the operational effort required to maintain your workload. You can also accelerate your time to market when you deploy new services and features.Common anti-patterns:In a post-migration workload, using the same compute solution that was being used on premises.Lacking awareness of the cloud compute solutions and how those solutions might improve your compute performance.Oversizing an existing compute solution to meet scaling or performance requirements, when an alternative compute solution would align to your workload characteristics more precisely.Benefits of establishing this best practice: By identifying the compute requirements and evaluating the available compute solutions, business stakeholders and engineering teams will understand the benefits and limitations of using the selected compute solution. The selected compute solution should fit the workload performance criteria. Key criteria include processing needs, traffic patterns, data access patterns, scaling needs, and latency requirements.Level of risk exposed if this best practice is not established: HighUnderstand the virtualization, containerization, and management solutions that can benefit your workload and meet your performance requirements. A workload can contain multiple types of compute solutions. Each compute solution has differing characteristics. Based on your workload scale and compute requirements, a compute solution can be selected and configured to meet your needs. The cloud architect should learn the advantages and disadvantages of instances, containers, and functions. The following steps will help you through how to select your compute solution to match your workload characteristics and performance requirements.Implementation steps:Select the location of where the compute solution must reside by evaluating PERF05-BP06 Choose your workload\u2019s location based on network requirements. This location will limit the types of compute solution available to you.Identify the type of compute solution that works with the location requirement and application requirementsAmazon Elastic Compute Cloud (Amazon EC2) virtual server instances come in a wide variety of different families and sizes. They offer a wide variety of capabilities, including solid state drives (SSDs) and graphics processing units (GPUs). EC2 instances offer the greatest flexibility on instance choice. When you launch an EC2 instance, the instance type that you specify determines the hardware of your instance. Each instance type offers different compute, memory, and storage capabilities. Instance types are grouped in instance families based on these capabilities. Typical use cases include: running enterprise applications, high performance computing (HPC), training and deploying machine learning applications and running cloud native applications.Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that allows you to automatically run and manage containers on a cluster of EC2 instances or serverless instances using AWS Fargate. You can use Amazon ECS with other services such as Amazon Route 53, Secrets Manager, AWS Identity and Access Management (IAM), and Amazon CloudWatch. Amazon ECS is recommended if your application is containerized and your engineering team prefers Docker containers.Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. You can choose to run your EKS clusters using AWS Fargate, removing the need to provision and manage servers. Managing Amazon EKS is simplified due to integrations with AWS Services such as Amazon CloudWatch, Auto Scaling Groups, AWS Identity and Access Management (IAM), and Amazon Virtual Private Cloud (VPC). When using containers, you must use compute metrics to select the optimal type for your workload, similar to how you use compute metrics to select your EC2 or AWS Fargate instance types. Amazon EKS is recommended if your application is containerized and your engineering team prefers Kubernetes over Docker containers.You can use AWS Lambda to run code that supports the allowed runtime, memory, and CPU options. Simply upload your code, and AWS Lambda will manage everything required to run and scale that code. You can set up your code to automatically trigger from other AWS services or call it directly. Lambda is recommended for short running, microservice architectures developed for the cloud.After you have experimented with your new compute solution, plan your migration and validate your performance metrics. This is a continual process, see PERF02-BP04 Determine the required configuration by right-sizing.Level of effort for the implementation plan: If a workload is moving from one compute solution to another, there could be a moderate level of effort involved in refactoring the application.Related documents:Cloud Compute with AWSEC2 Instance TypesProcessor State Control for Your EC2 InstanceEKS Containers: EKS Worker NodesAmazon ECS Containers: Amazon ECS Container InstancesFunctions: Lambda Function ConfigurationPrescriptive Guidance for ContainersPrescriptive Guidance for ServerlessRelated videos:How to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Amazon EC2 foundations (CMP211-R2)Powering next-gen Amazon EC2: Deep dive into the Nitro systemDeliver high-performance ML inference with AWS Inferentia (CMP324-R1)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Related examples:Migrating the web application to containersRun a Serverless Hello WorldTypeServerContainersFunctionAWS serviceAmazon Elastic Compute Cloud (Amazon EC2)Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS)AWS LambdaKey CharacteristicsHas dedicated option for hardware license requirements, Placement Options, and a large selection of different instance families based on compute metricsEasy deployment, consistent environments, runs on top of EC2 instances, ScalableShort runtime (15 minutes or less), maximum memory and CPU are not as high as other services, Managed hardware layer, Scales to millions of concurrent requestsCommon use-casesLift and shift migrations, monolithic application, hybrid environments, enterprise applicationsMicroservices, hybrid environments,Microservices, event-driven applications\n\nKnowledge Area: Hybrid compute options (for example, AWS Outposts, AWS Snowball Edge)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which compute option should the company use to build a mobile application that needs to access and store data in the cloud?\", \"answer_choices\": [{\"answer\": \"Amazon Elastic Compute Cloud (Amazon EC2)\", \"is_correct\": \"false\", \"explanation\": \"EC2 is a virtual server instance which offers flexibility on instance choice but is not well suited for mobile applications that need to access data in the cloud.\"},{\"answer\": \"Amazon Elastic Container Service (Amazon ECS)\", \"is_correct\": \"true\", \"explanation\": \"ECS is a fully managed container orchestration service that allows containers to be automatically run and managed on a cluster of EC2 instances or serverless instances, making it ideal for a mobile application that needs to access data in the cloud.\"},{\"answer\": \"Amazon Elastic Kubernetes Service (Amazon EKS)\", \"is_correct\": \"false\", \"explanation\": \"EKS is a fully managed Kubernetes service, and while it is suitable for containerized applications, it is not the best choice for applications that need to access data in the cloud.\"},{\"answer\": \"AWS Lambda\", \"is_correct\": \"false\", \"explanation\": \"Lambda is recommended for short running, microservice architectures developed for the cloud, but it is not the best choice for a mobile application that needs to access data in the cloud.\"}]}", "docs": ["('PERF02-BP01 Evaluate the available compute options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_evaluate_options.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Knowledge of", "item": "Instance types, families, and sizes (for example, memory optimized, compute optimized, virtualization)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Continually monitor the release of new instance types and take advantage of energy efficiency improvements, including those instance types designed to support specific workloads such as machine learning training, inference, and video transcoding.Common anti-patterns:You are only using one family of instances.You are only using x86 instances.You specify one instance type in your Amazon EC2 Auto Scaling configuration.You use AWS instances in a manner that they were not designed for (for example, you use compute-optimized instances for a memory-intensive workload).You do not evaluate new instance types regularly.You do not check recommendations from AWS rightsizing tools such as AWS Compute Optimizer.Benefits of establishing this best practice: By using energy-efficient and right-sized instances, you are able to greatly reduce the environmental impact and cost of your workload.Level of risk exposed if this best practice is not established: LowLearn and explore instance types which can lower your workload environmental impact.Subscribe to What's New with AWS to be up-to-date with the latest AWS technologies and instances.Learn about different AWS instance types.Learn about AWS Graviton-based instances which offer the best performance per watt of energy use in Amazon EC2 by watching re:Invent 2020 - Deep dive on AWS Graviton2 processor-powered Amazon EC2 instances and Deep dive into AWS Graviton3 and Amazon EC2 C7g instances.Plan and transition your workload to instance types with the least impact.Define a process to evaluate new features or instances for your workload. Take advantage of agility in the cloud to quickly test how new instance types can improve your workload environmental sustainability. Use proxy metrics to measure how many resources it takes you to complete a unit of work.If possible, modify your workload to work with different numbers of vCPUs and different amounts of memory to maximize your choice of instance type.Consider transitioning your workload to Graviton-based instances to improve the performance efficiency of your workload (see AWS Graviton Fast Start and AWS Graviton2 for ISVs). Keep in mind the considerations when transitioning workloads to AWS Graviton-based Amazon Elastic Compute Cloud instances.Consider selecting the AWS Graviton option in your usage of AWS managed services.Migrate your workload to Regions that offer instances with the least sustainability impact and still meet your business requirements.For machine learning workloads, use Amazon EC2 instances which are based on custom Amazon Machine Learning chips such as AWS Trainium, AWS Inferentia, and Amazon EC2 DL1.Use Amazon SageMaker Inference Recommender to right size ML inference endpoint.For workloads with real time video transcoding, use Amazon EC2 VT1 Instances.For spikey workloads (workloads with infrequent requirements for additional capacity), use burstable performance instances.For stateless and fault-tolerant workloads, use Amazon EC2 Spot Instances to increase overall utilization of the cloud, and reduce the sustainability impact of unused resources.Operate and optimize your workload instance.For ephemeral workloads, evaluate instance Amazon CloudWatch metrics such as CPUUtilization to identify if the instance is idle or under-utilized.For stable workloads, check AWS rightsizing tools such as AWS Compute Optimizer at regular intervals to identify opportunities to optimize and right-size the instances.Related documents:Optimizing your AWS Infrastructure for Sustainability, Part I: ComputeAWS Graviton ProcessorAWS InferentiaAWS TrainiumAmazon EC2 DL1Amazon EC2 Burstable performance instancesAmazon EC2 Capacity Reservation FleetsAmazon EC2 Spot FleetAmazon EC2 Spot InstancesAmazon EC2 VT1 InstancesAmazon EC2 instance typesAWS Compute OptimizerFunctions: Lambda Function ConfigurationRelated videos:Deep dive on AWS Graviton2 processer-powered Amazon EC2 instancesDeep dive into AWS Graviton3 and Amazon EC2 C7g instancesRelated examples:Lab: Rightsizing RecommendationsLab: Rightsizing with Compute OptimizerLab: Optimize Hardware Patterns and Observe Sustainability KPIs\n\nKnowledge Area: Instance types, families, and sizes (for example, memory optimized, compute optimized, virtualization)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"For a manufacturing company using AWS to optimize their supply chain and production processes, which instance type would be best to choose for a memory-intensive workload such as a database?\", \n\"answer_choices\": [\n{\"answer\": \"EKS containers with EKS worker nodes\",\"is_correct\": \"false\",\"explanation\": \"EKS containers are used for running containerized applications, whereas memory-intensive workloads such as databases require an R-family of instances.\"},\n{\"answer\": \"Compute-optimized instances\",\"is_correct\": \"false\",\"explanation\": \"Compute-optimized instances are best suited for components that do data processing that is not memory-intensive.\"},\n{\"answer\": \"Elastic container system\",\"is_correct\": \"false\",\"explanation\": \"Elastic containers are better-suited for bursting workloads, not memory-intensive workloads such as databases.\"},\n{\"answer\": \"R-family of instances\",\"is_correct\": \"true\",\"explanation\": \"R-family of instances are best suited for memory-intensive workloads such as databases.\"}\n]}", "docs": ["('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')", "('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('SUS05-BP02 Use instance types with the least impact - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a3.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Knowledge of", "item": "Optimization of compute utilization (for example, containers, serverless computing, microservices)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Use system-level metrics to identify the behavior and requirements of your workload over time. Evaluate your workload's needs by comparing the available resources with these requirements and make changes to your compute environment to best match your workload's profile. For example, over time a system might be observed to be more memory-intensive than initially thought, so moving to a different instance family or size could improve both performance and efficiency.Common anti-patterns:You only monitor system-level metrics to gain insight into your workload.You architect your compute needs for peak workload requirements.You oversize the compute solution to meet scaling or performance requirements when moving to a new compute solution would match your workload characteristicsBenefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and a historical reference. You can create automatic dashboards to visualize this data and perform metric math to derive operational and utilization insights.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to optimize resources: To achieve maximum performance and efficiency, use the data gathered over time from your workload to tune and optimize your resources. Look at the trends in your workload's usage of current resources and determine where you can make changes to better match your workload's needs. When resources are over-committed, system performance degrades, whereas underutilization results in a less efficient use of resources and higher cost.Related documents:Cloud Compute with AWSAWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemRelated examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n\nKnowledge Area: Optimization of compute utilization (for example, containers, serverless computing, microservices)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following techniques is the best way to optimize compute utilization for a company's disaster recovery plan on AWS?\",\n\"answer_choices\": [{\"answer\": \"Monitor system-level metrics to gain insight into the workload\",\"is_correct\": \"false\",\"explanation\": \"Monitoring system-level metrics is important, but is not the best way to optimize compute utilization for a company's disaster recovery plan on AWS.\"},{\"answer\": \"Architect your compute needs for peak workload requirements\",\"is_correct\": \"false\",\"explanation\": \"Architecting your compute needs for peak workload requirements is not the best way to optimize compute utilization for a company's disaster recovery plan on AWS.\"}, {\"answer\": \"Use the data gathered over time from the workload to tune and optimize resources\",\"is_correct\": \"true\",\"explanation\": \"Using the data gathered over time from the workload to tune and optimize resources is the best way to optimize compute utilization for a company's disaster recovery plan on AWS.\"},{\"answer\": \"Oversize the compute solution to meet scaling or performance requirements\",\"is_correct\": \"false\",\"explanation\": \"Oversizing the compute solution to meet scaling or performance requirements is not the best way to optimize compute utilization for a company's disaster recovery plan on AWS.\"}]}", "docs": ["('PERF02-BP06 Re-evaluate compute needs based on metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_use_metrics.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Knowledge of", "item": "Scaling strategies (for example, auto scaling, hibernation)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Identify periods of low or no utilization and scale down resources to eliminate excess capacity and improve efficiency.Common anti-patterns:You do not scale your infrastructure with user load.You manually scale your infrastructure all the time.You leave increased capacity after a scaling event instead of scaling back down.Benefits of establishing this best practice: Configuring and testing workload elasticity will help reduce workload environmental impact, save money, and maintain performance benchmarks. You can take advantage of elasticity in the cloud to automatically scale capacity during and after user load spikes to make sure you are only using the exact number of resources needed to meet the needs of your customers.Level of risk exposed if this best practice is not established: MediumElasticity matches the supply of resources you have against the demand for those resources. Instances, containers, and functions provide mechanisms for elasticity, either in combination with automatic scaling or as a feature of the service. Use elasticity in your architecture to ensure that workload can scale down quickly and easily during the period of low user load:Amazon EC2 Auto ScalingUse to verify you have the correct number of Amazon EC2 instances available to handle the user load for your application.Application Auto ScalingUse to automatically scale the resources for individual AWS services beyond Amazon EC2, such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.Kubernetes Cluster AutoscalerUse to automatically scale Kubernetes clusters on AWS.Verify that the metrics for scaling up or down are validated against the type of workload being deployed. If you are deploying a video transcoding application, 100% CPU utilization is expected and should not be your primary metric. You can use a customized metric (such as memory utilization) for your scaling policy if required. To choose the right metrics, consider the following guidance for Amazon EC2:The metric should be a valid utilization metric and describe how busy an instance is.The metric value must increase or decrease proportionally to the number of instances in the Auto Scaling group.Use dynamic scaling instead of manual scaling for your Auto Scaling group. We also recommend that you use target tracking scaling policies in your dynamic scaling.Verify that workload deployments can handle both scale-up and scale-down events. Create test scenarios for scale-down events to ensure that the workload behaves as expected. You can use Activity history to test and verify a scaling activity for an Auto Scaling group.Evaluate your workload for predictable patterns and proactively scale as you anticipate predicted and planned changes in demand. Use Predictive Scaling with Amazon EC2 Auto Scaling to eliminate the need to overprove capacity.Related documents:Getting Started with Amazon EC2 Auto ScalingPredictive Scaling for EC2, Powered by Machine LearningAnalyze user behavior using Amazon OpenSearch Service, Amazon Kinesis Data Firehose and KibanaWhat is Amazon CloudWatch?What is AWS X-Ray?VPC Flow LogsMonitoring DB load with Performance Insights on Amazon RDSIntroducing Native Support for Predictive Scaling with Amazon EC2 Auto ScalingHow to create an Amazon EC2 Auto Scaling policy based on a memory utilization metric (Linux)Introducing Karpenter - An Open-Source, High-Performance Kubernetes Cluster AutoscalerRelated videos:Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Related examples:Lab: Amazon EC2 Auto Scaling Group ExamplesLab: Implement Autoscaling with KarpenterAuto-scaling mechanismWhere to useAmazon EC2 Auto ScalingUse to verify you have the correct number of Amazon EC2 instances available to handle the user load for your application.Application Auto ScalingUse to automatically scale the resources for individual AWS services beyond Amazon EC2, such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.Kubernetes Cluster AutoscalerUse to automatically scale Kubernetes clusters on AWS.\n* Using the capabilities of the cloud, you can make frequent changes to your workload implementations. Update deployed components as your needs change.Level of risk exposed if this best practice is not established: MediumEnable horizontal scaling, and use automation to scale out as loads increase and to scale in as loads decrease.Scale using small increments for variable workloads.Align scaling with cyclical utilization patterns (for example, a payroll system with intense bi-weekly processing activities) as load varies over days, weeks, months, or years.Negotiate service level Agreements (SLAs) that allow for a temporary reduction in capacity while automation deploys replacement resources.Related documents:AWS Compute Optimizer DocumentationOperating Lambda: Performance optimizationAuto Scaling Documentation\n* Scale resources proactively to meet demand and avoid availability impact.Many AWS services automatically scale to meet demand. If using Amazon EC2 instances or Amazon ECS clusters, you can configure automatic scaling of these to occur based on usage metrics that correspond to demand for your workload. For Amazon EC2, average CPU utilization, load balancer request count, or network bandwidth can be used to scale out (or scale in) EC2 instances. For Amazon ECS, average CPU utilization, load balancer request count, and memory utilization can be used to scale out (or scale in) ECS tasks. Using Target Auto Scaling on AWS, the autoscaler acts like a household thermostat, adding or removing resources to maintain the target value (for example, 70% CPU utilization) that you specify.AWS Auto Scaling can also do Predictive Auto Scaling, which uses machine learning to analyze each resource's historical workload and regularly forecasts the future load for the next two days.Little\u2019s Law helps calculate how many instances of compute (EC2 instances, concurrent Lambda functions, etc.) that you need.L = \u03bbWL = number of instances (or mean concurrency in the system)\u03bb = mean rate at which requests arrive (req/sec)W = mean time that each request spends in the system (sec)For example, at 100 rps, if each request takes 0.5 seconds to process, you will need 50 instances to keep up with demand.Level of risk exposed if this best practice is not established: MediumObtain resources upon detection that more resources are needed for a workload. Scale resources proactively to meet demand and avoid availability impact.Calculate how many compute resources you will need (compute concurrency) to handle a given request rate.Telling Stories About Little's LawWhen you have a historical pattern for usage, set up scheduled scaling for Amazon EC2 auto scaling.Scheduled Scaling for Amazon EC2 Auto ScalingUse AWS predictive scaling.Predictive Scaling for EC2, Powered by Machine LearningRelated documents:AWS Auto Scaling: How Scaling Plans WorkAWS Marketplace: products that can be used with auto scalingManaging Throughput Capacity Automatically with DynamoDB Auto ScalingPredictive Scaling for EC2, Powered by Machine LearningScheduled Scaling for Amazon EC2 Auto ScalingTelling Stories About Little's LawWhat Is Amazon EC2 Auto Scaling?\n\nKnowledge Area: Scaling strategies (for example, auto scaling, hibernation)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"The healthcare company wants to store and process sensitive patient information in the cloud. What is the most efficient way to ensure that the infrastructure is able to scale with user load while reducing environmental impact, saving money, and maintaining performance benchmarks?\", \n\"answer_choices\": [\n{\"answer\": \"Enable manual scaling of the infrastructure.\", \"is_correct\": \"False\", \"explanation\": \"Manual scaling does not always guarantee the most efficient use of resources, and it does not reduce environmental impact or save money.\"}, \n{\"answer\": \"Use dynamic scaling with Amazon EC2 Auto Scaling.\", \"is_correct\": \"True\", \"explanation\": \"Using dynamic scaling with Amazon EC2 Auto Scaling ensures that the correct number of EC2 instances are available to handle user load for an application, reduces environmental impact, saves money, and maintains performance benchmarks.\"}, \n{\"answer\": \"Use Kubernetes Cluster Autoscaler to automatically scale the infrastructure.\", \"is_correct\": \"False\", \"explanation\": \"Kubernetes Cluster Autoscaler is used to automatically scale Kubernetes clusters on AWS, and will not ensure that the correct number of EC2 instances are available to handle user load for an application.\"}, \n{\"answer\": \"Use Application Auto Scaling to automatically scale the resources.\", \"is_correct\": \"False\", \"explanation\": \"Application Auto Scaling is used to automatically scale the resources for individual AWS services beyond Amazon EC2, such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services, and will not ensure that the correct number of EC2 instances are available to handle user load for an application.\"}\n]}", "docs": ["('SUS02-BP01 Scale infrastructure with user load - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_user_a2.html')", "('SUS05-BP01 Use the minimum amount of hardware to meet your needs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_hardware_a2.html')", "('REL07-BP03 Obtain resources upon detection that more resources are needed for a workload - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_adapt_to_changes_proactive_adapt_auto.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Skills in", "item": "Determining an appropriate load balancing strategy (for example, Application Load Balancer [Layer 7] compared with Network Load Balancer [Layer 4] compared with Gateway Load Balancer)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Distribute traffic across multiple resources or services to allow your workload to take advantage of the elasticity that the cloud provides. You can also use load balancing for offloading encryption termination to improve performance and to manage and route traffic effectively.When implementing a scale-out architecture where you want to use multiple instances for service content, you can use load balancers inside your Amazon VPC. AWS provides multiple models for your applications in the ELB service. Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers.Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. It is capable of handling millions of requests per second while maintaining ultra-low latencies, and it is optimized to handle sudden and volatile traffic patterns.Elastic Load Balancing provides integrated certificate management and SSL/TLS decryption, allowing you the flexibility to centrally manage the SSL settings of the load balancer and offload CPU intensive work from your workload.Common anti-patterns:You route all internet traffic through existing load balancers.You use generic TCP load balancing and making each compute node handle SSL encryption.Benefits of establishing this best practice: A load balancer handles the varying load of your application traffic in a single Availability Zone, or across multiple Availability Zones. Load balancers feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant.Level of risk exposed if this best practice is not established: HighUse the appropriate load balancer for your workload: Select the appropriate load balancer for your workload. If you must load balance HTTP requests, we recommend Application Load Balancer. For network and transport protocols (layer 4 \u2013 TCP, UDP) load balancing, and for extreme performance and low latency applications, we recommend Network Load Balancer. Application Load Balancers support HTTPS and Network Load Balancers support TLS encryption offloading.Enable offload of HTTPS or TLS encryption: Elastic Load Balancing includes integrated certificate management, user-authentication, and SSL/TLS decryption. It provides the flexibility to centrally manage TLS settings and offload CPU intensive workloads from your applications. Encrypt all HTTPS traffic as part of your load balancer deployment.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies.Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public endpoints. You might also choose to evaluate AWS Marketplace software appliances for load balancing and proxying.Consumers of the service your workload provides, whether they are end-users or other services, make requests on these service endpoints. Several AWS resources are available to enable you to provide highly available endpoints.Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.Distributed Denial of Service (DDoS) attacks risk shutting out legitimate traffic and lowering availability for your users. AWS Shield provides automatic protection against these attacks at no extra cost for AWS service endpoints on your workload. You can augment these features with virtual appliances from APN Partners and the AWS Marketplace to meet your needs.Common anti-patterns:Using public internet addresses on instances or containers and managing the connectivity to them via DNS.Using Internet Protocol addresses instead of domain names for locating services.Providing content (web pages, static assets, media files) to a large geographic area and not using a content delivery network.Benefits of establishing this best practice: By implementing highly available services in your workload, you know that your workload will be available to your users.Level of risk exposed if this best practice is not established: HighEnsure that you have highly available connectivity for users of the workload Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public facing endpoints. You may also choose to evaluate AWS Marketplace software appliances for load-balancing and proxying.Ensure that you have a highly available connection to your users.Ensure that you are using a highly available DNS to manage the domain names of your application endpoints.If your users access your application via the internet, use service API operations to confirm the correct usage of Internet Gateways. Also confirm that the route tables entries for the subnets hosting your application endpoints are correct.DescribeInternetGatewaysDescribeRouteTablesEnsure that you are using a highly available reverse proxy or load balancer in front of your application.If your users access your application via your on-premises environment, ensure that your connectivity between AWS and your on-premises environment is highly available.Use Route 53 to manage your domain names.What is Amazon Route 53?Use a third-party DNS provider that meets your requirements.Use Elastic Load Balancing.What is Elastic Load Balancing?Use an AWS Marketplace appliance that meets your requirements.Related documents:APN Partner: partners that can help plan your networkingAWS Direct Connect Resiliency RecommendationsAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivityUsing the Direct Connect Resiliency Toolkit to get startedVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is AWS Global Accelerator?What Is Amazon VPC?What Is a Transit Gateway?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?Working with Direct Connect GatewaysRelated videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n\nKnowledge Area: Determining an appropriate load balancing strategy (for example, Application Load Balancer [Layer 7] compared with Network Load Balancer [Layer 4] compared with Gateway Load Balancer)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"For a company that wants to run a disaster recovery plan for their critical systems on AWS, which of the following load balancers should they use to best distribute traffic, allow for elasticity, and manage and route traffic effectively?\", \n\"answer_choices\": [\n    {\"answer\": \"Network Load Balancer\",\"is_correct\": false,\"explanation\": \"Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. It is not the best choice for this scenario.\"},\n    {\"answer\": \"Amazon Route 53\",\"is_correct\": false,\"explanation\": \"Amazon Route 53 is a Domain Name System (DNS) service that connects user requests to infrastructure running in AWS and can also be used to route users to infrastructure outside of AWS. It is not the best choice for this scenario.\"},\n    {\"answer\": \"Application Load Balancer\",\"is_correct\": true,\"explanation\": \"Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. This is the best choice for this scenario.\"}, \n    {\"answer\": \"AWS Global Accelerator\",\"is_correct\": false,\"explanation\": \"AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network. It is not the best choice for this scenario.\"}\n]}", "docs": ["('PERF05-BP04 Leverage load-balancing and encryption offloading - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_encryption_offload.html')", "('REL02-BP01 Use highly available network connectivity for your workload public endpoints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ha_conn_users.html')"]}, {"scenario": "A company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Skills in", "item": "Determining appropriate scaling methods and strategies for elastic workloads (for example, horizontal compared with vertical, EC2 hibernation)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.`\"\n\nContext:\n\n* Use metrics from the currently running workload to select the right size and type to optimize for cost. Appropriately provision throughput, sizing, and storage for services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Elastic Block Store (Amazon EBS) (PIOPS), Amazon Relational Database Service (Amazon RDS), Amazon EMR, and networking. This can be done with a feedback loop such as automatic scaling or by custom code in the workload.Level of risk exposed if this best practice is not established: LowCreate a feedback loop within the workload that uses active metrics from the running workload to make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you configure to perform the right sizing operations for you. AWS also provides APIs, SDKs, and features that allow resources to be modified with minimal effort. You can program a workload to stop-and-start an Amazon Elastic Compute Cloud(Amazon EC2) instance to allow a change of instance size or instance type. This provides the benefits of right-sizing while removing almost all the operational cost required to make the change.Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage Service(Amazon S3) Intelligent-Tiering. Amazon S3 Intelligent-Tiering automatically moves your data between two access tiers: frequent access and infrequent access, based on your usage patterns.Implementation stepsConfigure workload metrics: Ensure you capture the key metrics for the workload. These metrics provide an indication of the customer experience, such as the workload output, and align to the differences between resource types and sizes, such as CPU and memory usage.View rightsizing recommendations: Use the rightsizing recommendations in AWS Compute Optimizer to make adjustments to your workload.Select resource type and size automatically based on metrics: Using the workload metrics, manually or automatically select your workload resources. Configuring AWS Auto Scaling or implementing code within your application can reduce the effort required if frequent changes are needed, and it can potentially implement changes sooner than a manual process.Related documents:AWS Auto ScalingAWS Compute OptimizerAmazon CloudWatch featuresCloudWatch Getting Set UpCloudWatch Publishing Custom MetricsCost Optimization: Amazon EC2 Right SizingGetting Started with Amazon EC2 Auto ScalingAmazon S3 Intelligent-TieringLaunch an EC2 Instance Using the SDK\n* Identify periods of low or no utilization and scale down resources to eliminate excess capacity and improve efficiency.Common anti-patterns:You do not scale your infrastructure with user load.You manually scale your infrastructure all the time.You leave increased capacity after a scaling event instead of scaling back down.Benefits of establishing this best practice: Configuring and testing workload elasticity will help reduce workload environmental impact, save money, and maintain performance benchmarks. You can take advantage of elasticity in the cloud to automatically scale capacity during and after user load spikes to make sure you are only using the exact number of resources needed to meet the needs of your customers.Level of risk exposed if this best practice is not established: MediumElasticity matches the supply of resources you have against the demand for those resources. Instances, containers, and functions provide mechanisms for elasticity, either in combination with automatic scaling or as a feature of the service. Use elasticity in your architecture to ensure that workload can scale down quickly and easily during the period of low user load:Amazon EC2 Auto ScalingUse to verify you have the correct number of Amazon EC2 instances available to handle the user load for your application.Application Auto ScalingUse to automatically scale the resources for individual AWS services beyond Amazon EC2, such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.Kubernetes Cluster AutoscalerUse to automatically scale Kubernetes clusters on AWS.Verify that the metrics for scaling up or down are validated against the type of workload being deployed. If you are deploying a video transcoding application, 100% CPU utilization is expected and should not be your primary metric. You can use a customized metric (such as memory utilization) for your scaling policy if required. To choose the right metrics, consider the following guidance for Amazon EC2:The metric should be a valid utilization metric and describe how busy an instance is.The metric value must increase or decrease proportionally to the number of instances in the Auto Scaling group.Use dynamic scaling instead of manual scaling for your Auto Scaling group. We also recommend that you use target tracking scaling policies in your dynamic scaling.Verify that workload deployments can handle both scale-up and scale-down events. Create test scenarios for scale-down events to ensure that the workload behaves as expected. You can use Activity history to test and verify a scaling activity for an Auto Scaling group.Evaluate your workload for predictable patterns and proactively scale as you anticipate predicted and planned changes in demand. Use Predictive Scaling with Amazon EC2 Auto Scaling to eliminate the need to overprove capacity.Related documents:Getting Started with Amazon EC2 Auto ScalingPredictive Scaling for EC2, Powered by Machine LearningAnalyze user behavior using Amazon OpenSearch Service, Amazon Kinesis Data Firehose and KibanaWhat is Amazon CloudWatch?What is AWS X-Ray?VPC Flow LogsMonitoring DB load with Performance Insights on Amazon RDSIntroducing Native Support for Predictive Scaling with Amazon EC2 Auto ScalingHow to create an Amazon EC2 Auto Scaling policy based on a memory utilization metric (Linux)Introducing Karpenter - An Open-Source, High-Performance Kubernetes Cluster AutoscalerRelated videos:Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Related examples:Lab: Amazon EC2 Auto Scaling Group ExamplesLab: Implement Autoscaling with KarpenterAuto-scaling mechanismWhere to useAmazon EC2 Auto ScalingUse to verify you have the correct number of Amazon EC2 instances available to handle the user load for your application.Application Auto ScalingUse to automatically scale the resources for individual AWS services beyond Amazon EC2, such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.Kubernetes Cluster AutoscalerUse to automatically scale Kubernetes clusters on AWS.\n\nKnowledge Area: Determining appropriate scaling methods and strategies for elastic workloads (for example, horizontal compared with vertical, EC2 hibernation)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\n\"question\": \"Which scaling method should be used to minimize costs when running batch processing jobs on AWS?\",\n\"answer_choices\": [\n    {\"answer\": \"Manual scaling\",\"is_correct\": \"false\",\"explanation\": \"Manual scaling requires a significant amount of time and effort and can result in higher costs.\"},\n    {\"answer\": \"Dynamic scaling\",\"is_correct\": \"true\",\"explanation\": \"Dynamic scaling uses metrics from the currently running workload to select the right size and type to optimize for cost.\"},\n    {\"answer\": \"AWS Auto Scaling\",\"is_correct\": \"false\",\"explanation\": \"AWS Auto Scaling can be used to automatically adjust the resources needed but it is best used in combination with dynamic scaling.\"},\n    {\"answer\": \"Predictive scaling\",\"is_correct\": \"false\",\"explanation\": \"Predictive scaling is best used to anticipate predicted and planned changes in demand and should be used in combination with dynamic scaling.\"}\n]\n}", "docs": ["('COST06-BP03 Select resource type, size, and number automatically based on metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_metrics.html')", "('SUS02-BP01 Scale infrastructure with user load - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_user_a2.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Skills in", "item": "Determining cost-effective AWS compute services with appropriate use cases (for example, Lambda, Amazon EC2, Fargate)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* Selecting compute resources that meet your requirements, performance needs, and provide great efficiency of cost and effort will enable you to accomplish more with the same number of resources. When evaluating compute options, be aware of your requirements for workload performance and cost requirements and use this to make informed decisions.In AWS, compute is available in three forms: instances, containers, and functions:Instances are virtualized servers, allowing you to change their capabilities with a button or an API call. Because resource decisions in the cloud aren\u2019t fixed, you can experiment with different server types. At AWS, these virtual server instances come in different families and sizes, and they offer a wide variety of capabilities, including solid-state drives (SSDs) and graphics processing units (GPUs).Containers are a method of operating system virtualization that allow you to run an application and its dependencies in resource-isolated processes. AWS Fargate is serverless compute for containers or Amazon EC2 can be used if you need control over the installation, configuration, and management of your compute environment. You can also choose from multiple container orchestration platforms: Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS).Functions abstract the execution environment from the code you want to execute. For example, AWS Lambda allows you to execute code without running an instance.The following question focuses on these considerations for performance efficiency.When architecting your use of compute you should take advantage of the elasticity mechanisms available to ensure you have sufficient capacity to sustain performance as demand changes.PERF 2: How do you select your compute solution?The optimal compute solution for a workload varies based on application design, usage patterns, and configuration settings. Architectures can use different compute solutions for various components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency.\n* Using the appropriate instances and resources for your workload is key to cost savings. For example, a reporting process might take five hours to run on a smaller server but one hour to run on a larger server that is twice as expensive. Both servers give you the same outcome, but the smaller server incurs more cost over time.A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. For example, rather than maintaining servers to deliver email, you can use a service that charges on a per-message basis.AWS offers a variety of flexible and cost-effective pricing options to acquire instances from Amazon EC2 and other services in a way that best fits your needs. On-DemandInstances allow you to pay for compute capacity by the hour, with no minimum commitments required. Savings Plans and Reserved Instances offer savings of up to 75% o\ufb00 On-Demand pricing. With Spot Instances, you can leverage unused Amazon EC2 capacity and offer savings of up to 90% o\ufb00 On-Demand pricing. Spot Instances are appropriate where the system can tolerate using a \ufb02eet of servers where individual servers can come and go dynamically, such as stateless web servers, batch processing, or when using HPC and big data.Appropriate service selection can also reduce usage and costs; such as CloudFront to minimize data transfer, or completely eliminate costs, such as utilizing Amazon Aurora on RDS to remove expensive database licensing costs.The following questions focus on these considerations for cost optimization.By factoring in cost during service selection, and using tools such as Cost Explorer and AWS Trusted Advisor to regularly review your AWS usage, you can actively monitor your utilization and adjust your deployments accordingly.COST 5: How do you evaluate cost when you select services?Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.COST 6: How do you meet cost targets when you select resource type, size and number?Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number.COST 7: How do you use pricing models to reduce cost?Use the pricing model that is most appropriate for your resources to minimize expense.COST 8: How do you plan for data transfer charges?Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time.\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n\nKnowledge Area: Determining cost-effective AWS compute services with appropriate use cases (for example, Lambda, Amazon EC2, Fargate)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When architecting a manufacturing company's use of AWS compute resources to optimize their supply chain and production processes, how should resources be selected to achieve the highest performance efficiency?\",\n\"answer_choices\": [\n    {\"answer\": \"Use AWS Spot Instances for stateless web servers, batch processing, or High Performance Computing and Big Data workloads\",\"is_correct\": \"false\", \"explanation\": \"Spot Instances can reduce cost, but may result in reduced performance since instances can come and go dynamically.\"},\n    {\"answer\": \"Use Amazon EC2 Instances with SSDs and GPUs to meet workload performance requirements\",\"is_correct\": \"false\",\"explanation\": \"EC2 Instances can meet performance requirements, but may not provide the most cost-effective solution.\"},\n    {\"answer\": \"Use Amazon Fargate or Amazon EC2 to run applications, their dependencies, and manage compute environments\",\"is_correct\": \"true\", \"explanation\": \"Using Fargate or EC2 to run applications and manage compute environments can provide the most cost-effective solution while meeting performance requirements.\"},\n    {\"answer\": \"Use AWS Lambda to execute the code without running an instance\",\"is_correct\": \"false\",\"explanation\": \"Lambda can abstract the execution environment from the code, but may not meet performance requirements for the workload.\"}\n]}", "docs": ["('Compute - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-compute.html')", "('Cost-effective resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-cereso.html')", "('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Skills in", "item": "Determining the required availability for different classes of workloads (for example, production workloads, non-production workloads)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* Workloads can change over time. Some services or features are more cost effective at different usage levels. By performing the analysis on each component over time and at projected usage, the workload remains cost-effective over its lifetime.Level of risk exposed if this best practice is not established: LowAs AWS releases new services and features, the optimal services for your workload may change. Effort required should reflect potential benefits. Workload review frequency depends on your organization requirements. If it is a workload of significant cost, implementing new services sooner will maximize cost savings, so more frequent review can be advantageous. Another trigger for review is change in usage patterns. Significant changes in usage can indicate that alternate services would be more optimal. For example, for higher data transfer rates a direct connect service may be cheaper than a VPN, and provide the required connectivity. Predict the potential impact of service changes, so you can monitor for these usage level triggers and implement the most cost-effective services sooner.Implementation stepsDefine predicted usage patterns: Working with your organization, such as marketing and product owners, document what the expected and predicted usage patterns will be for the workload.Perform cost analysis at predicted usage: Using the usage patterns defined, perform the analysis at each of these points. The analysis effort should reflect the potential outcome. For example, if the change in usage is large, a thorough analysis should be performed to verify any costs and changes.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* The cloud provides the flexibility to expand or reduce your resources dynamically through a variety of mechanisms to meet changes in demand. Combined with compute-related metrics, a workload can automatically respond to changes and use the optimal set of resources to achieve its goal.Optimally matching supply to demand delivers the lowest cost for a workload, but you also must plan for sufficient supply to allow for provisioning time and individual resource failures. Demand can be fixed or variable, requiring metrics and automation to ensure that management does not become a burdensome and disproportionately large cost.With AWS, you can use a number of different approaches to match supply with demand. The Cost Optimization Pillar whitepaper describes how to use the following approaches to cost:Demand-based approachBuffer-based approachTime-based approachYou must ensure that workload deployments can handle both scale-up and scale-down events. Create test scenarios for scale-down events to ensure that the workload behaves as expected.Common anti-patterns:You react to alarms by manually increasing capacity.You leave increased capacity after a scaling event instead of scaling back down.Benefits of establishing this best practice: Configuring and testing workload elasticity will help save money, maintain performance benchmarks, and improves reliability as traffic changes. Most non-production instances should be stopped when they are not being used. Although it's possible to manually shut down unused instances, this is impractical at larger scales. You can also take advantage of volume-based elasticity, which allows you to optimize performance and cost by automatically increasing the number of compute instances during demand spikes and decreasing capacity when demand decreases.Level of risk exposed if this best practice is not established: MediumTake advantage of elasticity: Elasticity matches the supply of resources you have against the demand for those resources. Instances, containers, and functions provide mechanisms for elasticity either in combination with automatic scaling or as a feature of the service. Use elasticity in your architecture to ensure that you have sufficient capacity to meet performance requirements at all scales of use. Ensure that the metrics for scaling up or down elastic resources are validated against the type of workload being deployed. If you are deploying a video transcoding application, 100% CPU utilization is expected and should not be your primary metric. Alternatively, you can measure against the queue depth of transcoding jobs waiting to scale your instance types. Ensure that workload deployments can handle both scale up and scale down events. Scaling down workload components safely is as critical as scaling up resources when demand dictates. Create test scenarios for scale-down events to ensure that the workload behaves as expected.Related documents:Cloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemRelated examples:Amazon EC2 Auto Scaling Group ExamplesAmazon EFS Tutorials\n\nKnowledge Area: Determining the required availability for different classes of workloads (for example, production workloads, non-production workloads)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following best practices should be established to ensure that the financial services company's infrastructure remains cost-effective over its lifetime?\", \n\"answer_choices\": [\n    {\"answer\": \"Use the AWS TCO Calculator to predict usage patterns.\", \"is_correct\": \"false\", \"explanation\": \"The TCO Calculator is used to compare cost models between cloud and on-premises infrastructure, not to predict usage patterns.\"},\n    {\"answer\": \"Perform cost analysis at predicted usage levels.\", \"is_correct\": \"true\", \"explanation\": \"By performing the analysis on each component over time and at projected usage, the workload remains cost-effective over its lifetime.\"},\n    {\"answer\": \"Select resource size or type based on the workload's expected usage.\", \"is_correct\": \"false\", \"explanation\": \"Selecting resource sizes or types should be based on data about the workload and resource characteristics, not on the expected usage.\"},\n    {\"answer\": \"Configure and test workload elasticity.\", \"is_correct\": \"false\", \"explanation\": \"Configuring and testing workload elasticity will help save money, maintain performance benchmarks, and improve reliability, but is not required for cost-effectiveness.\"}\n]}", "docs": ["('COST05-BP06 Perform cost analysis for different usage over time - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_analyze_over_time.html')", "('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('PERF02-BP05 Use the available elasticity of resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_elasticity.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Skills in", "item": "Selecting the appropriate instance family for a workload", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Workloads often have cost requirements for operation. Use internal cost controls to select resource types and sizes based on predicted resource need.Determine which workload components could be replaced with fully managed services, such as managed databases, in-memory caches, and ETL services. Reducing your operational workload allows you to focus resources on business outcomes.For cost requirement best practices, refer to the Cost-Effective Resources section of the Cost Optimization Pillar whitepaper.Common anti-patterns:You only use one family of instances.You do not evaluate licensed solutions versus open-source solutionsYou only use block storage.You deploy common software on EC2 instances and Amazon EBS or ephemeral volumes that are available as a managed service.Benefits of establishing this best practice: Considering cost when making your selections will allow you to enable other investments.Level of risk exposed if this best practice is not established: MediumOptimize workload components to reduce cost: Right size workload components and enable elasticity to reduce cost and maximize component efficiency. Determine which workload components can be replaced with managed services when appropriate, such as managed databases, in-memory caches, and reverse proxies.Related documents:AWS Architecture CenterAWS Partner NetworkAWS Solutions LibraryAWS Knowledge CenterAWS Compute OptimizerRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)This is my ArchitectureOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:AWS SamplesAWS SDK ExamplesRightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n\nKnowledge Area: Selecting the appropriate instance family for a workload\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which instance family should be used to store log files for a company's applications for analysis and compliance purposes?\",\n\"answer_choices\": [{\"answer\": \"r-family of instances\",\"is_correct\": \"True\", \"explanation\": \"The r-family of instances is the best option for memory-intensive workloads, such as log files.\"},{\"answer\": \"Elastic Container System\",\"is_correct\": \"False\", \"explanation\": \"The Elastic Container System is better suited for a bursting workload.\"},{\"answer\": \"Compute-Optimized Instances\",\"is_correct\": \"False\", \"explanation\": \"Compute-Optimized Instances are better suited for components that do data processing that is not memory-intensive.\"},{\"answer\": \"Largest Instance Available\",\"is_correct\": \"False\", \"explanation\": \"Largest Instances Available are not the best option as they are not optimized for a memory-intensive workloads.\"}]}", "docs": ["('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')", "('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('PERF01-BP03 Factor cost requirements into decisions - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_performing_architecture_cost.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 2: Design cost-optimized compute solutions", "focus": "Skills in", "item": "Selecting the appropriate instance size for a workload", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Select resource size or type based on data about the workload and resource characteristics. For example, compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-premises) version of the workload, using documentation, or using other sources of information about the workload.Level of risk exposed if this best practice is not established: MediumSelect resource size or type based on workload and resource characteristics, for example, compute, memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers, published solutions).Implementation stepsSelect resources based on data: Using your cost modeling data, select the expected workload usage level, then select the specified resource type and size.Related documents:AWS Auto ScalingAmazon CloudWatch featuresCost Optimization: EC2 Right Sizing\n* Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system.Common anti-patterns:You choose the largest instance available for all workloads.You standardize all instances types to one type for ease of management.Benefits of establishing this best practice: Being familiar with the AWS compute offerings allows you to determine the correct solution for your various workloads. After you have selected the various compute offerings for your workload, you have the agility to quickly experiment with those compute offerings to determine which ones meet the needs of your workload.Level of risk exposed if this best practice is not established: MediumModify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory-optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory-intensive. Right sizing enables your workload to perform as well as possible while only using the required resourcesRelated documents:AWS Compute OptimizerCloud Compute with AWSEC2 Instance TypesECS Containers: Amazon ECS Container InstancesEKS Containers: EKS Worker NodesFunctions: Lambda Function ConfigurationProcessor State Control for Your EC2 InstanceRelated videos:Amazon EC2 foundations (CMP211-R2)Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)Deliver high performance ML inference with AWS Inferentia (CMP324-R1)Optimize performance and cost for your AWS compute (CMP323-R1)Powering next-gen Amazon EC2: Deep dive into the Nitro systemHow to choose compute option for startupsOptimize performance and cost for your AWS compute (CMP323-R1)Related examples:Rightsizing with Compute Optimizer and Memory utilization enabledAWS Compute Optimizer Demo code\n* Use metrics from the currently running workload to select the right size and type to optimize for cost. Appropriately provision throughput, sizing, and storage for services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Elastic Block Store (Amazon EBS) (PIOPS), Amazon Relational Database Service (Amazon RDS), Amazon EMR, and networking. This can be done with a feedback loop such as automatic scaling or by custom code in the workload.Level of risk exposed if this best practice is not established: LowCreate a feedback loop within the workload that uses active metrics from the running workload to make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you configure to perform the right sizing operations for you. AWS also provides APIs, SDKs, and features that allow resources to be modified with minimal effort. You can program a workload to stop-and-start an Amazon Elastic Compute Cloud(Amazon EC2) instance to allow a change of instance size or instance type. This provides the benefits of right-sizing while removing almost all the operational cost required to make the change.Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage Service(Amazon S3) Intelligent-Tiering. Amazon S3 Intelligent-Tiering automatically moves your data between two access tiers: frequent access and infrequent access, based on your usage patterns.Implementation stepsConfigure workload metrics: Ensure you capture the key metrics for the workload. These metrics provide an indication of the customer experience, such as the workload output, and align to the differences between resource types and sizes, such as CPU and memory usage.View rightsizing recommendations: Use the rightsizing recommendations in AWS Compute Optimizer to make adjustments to your workload.Select resource type and size automatically based on metrics: Using the workload metrics, manually or automatically select your workload resources. Configuring AWS Auto Scaling or implementing code within your application can reduce the effort required if frequent changes are needed, and it can potentially implement changes sooner than a manual process.Related documents:AWS Auto ScalingAWS Compute OptimizerAmazon CloudWatch featuresCloudWatch Getting Set UpCloudWatch Publishing Custom MetricsCost Optimization: Amazon EC2 Right SizingGetting Started with Amazon EC2 Auto ScalingAmazon S3 Intelligent-TieringLaunch an EC2 Instance Using the SDK\n\nKnowledge Area: Selecting the appropriate instance size for a workload\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which approach should be used to select the appropriate instance size for a workload on AWS?\", \n\"answer_choices\": [\n    {\"answer\": \"Run a cost analysis to select the largest instance available for the workload.\",\"is_correct\": \"False\",\"explanation\": \"This approach is an anti-pattern as it can lead to inefficient use of resources and increse costs.\"},\n    {\"answer\": \"Analyze the performance characteristics of the workload and select the resource type that best matches the characteristics.\",\"is_correct\": \"True\",\"explanation\": \"Analyzing the performance characteristics of the workload and selecting the resource type that best matches the characteristics is the correct approach for selecting the appropriate instance size for a workload on AWS.\"},\n    {\"answer\": \"Standardize all instances types to one type for ease of management.\",\"is_correct\": \"False\",\"explanation\": \"This approach is an anti-pattern as all workloads have different performance characteristics and may require different resource types.\"},\n    {\"answer\": \"Use metrics from the currently running workload to select the right size and type to optimize for cost.\",\"is_correct\": \"True\",\"explanation\": \"Using metrics from the currently running workload to select the right size and type to optimize for cost can help optimize performance and cost when selecting the appropriate instance size for a workload on AWS.\"}\n]}", "docs": ["('COST06-BP02 Select resource type, size, and number based on data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_data.html')", "('PERF02-BP04 Determine the required configuration by right-sizing - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_compute_right_sizing.html')", "('COST06-BP03 Select resource type, size, and number automatically based on metrics - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_type_size_number_resources_metrics.html')"]}, {"scenario": "A manufacturing company wants to use AWS to optimize their supply chain and production processes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Knowledge of", "item": "AWS cost management service features (for example, cost allocation tags, multi-account billing)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA manufacturing company wants to use AWS to optimize their supply chain and production processes.`\"\n\nContext:\n\n* The increased flexibility and agility that the cloud enables encourages innovation and fast-paced development and deployment. It eliminates the manual processes and time associated with provisioning on-premises infrastructure, including identifying hardware specifications, negotiating price quotations, managing purchase orders, scheduling shipments, and then deploying the resources. However, the ease of use and virtually unlimited on-demand capacity requires a new way of thinking about expenditures.Many businesses are composed of multiple systems run by various teams. The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget.In AWS, you create an account structure with AWS Organizations or AWS Control Tower, which provides separation and assists in allocation of your costs and usage. You can also use resource tagging to apply business and organization information to your usage and cost. Use AWS Cost Explorer for visibility into your cost and usage, or create customized dashboards and analytics with Amazon Athena and Amazon QuickSight. Controlling your cost and usage is done by notifications through AWS Budgets, and controls using AWS Identity and Access Management (IAM), and Service Quotas.The following questions focus on these considerations for cost optimization.You can use cost allocation tags to categorize and track your AWS usage and costs. When you apply tags to your AWS resources (such as EC2 instances or S3 buckets), AWS generates a cost and usage report with your usage and your tags. You can apply tags that represent organization categories (such as cost centers, workload names, or owners) to organize your costs across multiple services.Ensure you use the right level of detail and granularity in cost and usage reporting and monitoring. For high level insights and trends, use daily granularity with AWS Cost Explorer. For deeper analysis and inspection use hourly granularity in AWS Cost Explorer, or Amazon Athena and Amazon QuickSight with the Cost and Usage Report (CUR) at an hourly granularity.Combining tagged resources with entity lifecycle tracking (employees, projects) makes it possible to identify orphaned resources or projects that are no longer generating value to the organization and should be decommissioned. You can set up billing alerts to notify you of predicted overspending.COST 2: How do you govern usage?Establish policies and mechanisms to ensure that appropriate costs are incurred while objectives are achieved. By employing a checks-and-balances approach, you can innovate without overspending.COST 3: How do you monitor usage and cost?Establish policies and procedures to monitor and appropriately allocate your costs. This allows you to measure and improve the cost efficiency of this workload.COST 4: How do you decommission resources?Implement change control and resource management from project inception to end-of-life. This ensures you shut down or terminate unused resources to reduce waste.\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Configure AWS Cost Explorer and AWS Budgets inline with your organization policies.Level of risk exposed if this best practice is not established: HighTo modify usage and adjust costs, each person in your organization must have access to their cost and usage information. It is recommended that all workloads and teams have the following tooling configured when they use the cloud:Reports: Summarize of all cost and usage informationNotifications: Provide notifications when cost or usage is outside of defined limits.Current State: Configure a dashboard showing current levels of cost and usage. The dashboard should be available in a highly visible place within the work environment (similar to an operations dashboard).Trending: Provide the capability to show the variability in cost and usage over the required period of time, with the required granularity.Forecasts: Provide the capability to show estimated future costs.Tracking: Show the current cost and usage against configured goals or targets.Analysis: Provide the capability for team members to perform custom and deep analysis down to the hourly granularity, with all possible dimensions.You can use AWS native tooling, such as AWS Cost Explorer, AWS Budgets, and Amazon Athena with Amazon QuickSight to provide this capability. You can also use third-party tooling \u2014 however, you must ensure that the costs of this tooling provide value to your organization.Implementation stepsCreate a Cost Optimization group: Configure your account and create a group that has access to the required Cost and Usage reports. This group must include representatives from all teams that own or manage an application. This certifies that every team has access to their cost and usage information.Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set a budget for the overall account spend, and a budget for the workload by using tags.Configure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts. Create a dashboard for the workload that tracks overall spend, and key usage metrics for the workload.Configure advanced tooling: Optionally, you can create custom tooling for your organization that provides additional detail and granularity. You can implement advanced analysis capability using Amazon Athena, and dashboards using Amazon QuickSight.Related documents:Tagging AWS resourcesAnalyzing your costs with AWS BudgetsAnalyzing your costs with Cost ExplorerManaging AWS Cost and Usage ReportsRelated examples:Well-Architected Labs - AWS Account SetupWell-Architected Labs: Billing VisualizationWell-Architected Labs: Cost and Governance UsageWell-Architected Labs: Cost and Usage AnalysisWell-Architected Labs: Cost and Usage Visualization\n\nKnowledge Area: AWS cost management service features (for example, cost allocation tags, multi-account billing)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following can be used to attribute resource costs to the individual organization or product owners in order to help reduce waste?\", \"answer_choices\": [{\"answer\": \"Amazon EC2\",\"is_correct\": \"false\",\"explanation\": \"Amazon EC2 is a building-block AWS service and not used to attribute resource costs.\"},{\"answer\": \"AWS Organizations\",\"is_correct\": \"true\",\"explanation\": \"AWS Organizations is used to create an account structure with provides separation and assists in allocation of costs and usage.\"},{\"answer\": \"Amazon Athena\",\"is_correct\": \"false\",\"explanation\": \"Amazon Athena is used to create customized dashboards and analytics with the Cost and Usage Report (CUR) but not used to attribute resource costs.\"},{\"answer\": \"AWS Budgets\",\"is_correct\": \"false\",\"explanation\": \"AWS Budgets is used to control costs and usage but not used to attribute resource costs.\"}]}", "docs": ["('Expenditure and usage awareness - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-aware.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('COST03-BP04 Configure billing and cost management tools - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_monitor_usage_config_tools.html')"]}, {"scenario": "A marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Knowledge of", "item": "AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA marketing company wants to use AWS to process and analyze large volumes of customer data for targeted advertising campaigns.`\"\n\nContext:\n\n* Configure AWS Cost Explorer and AWS Budgets inline with your organization policies.Level of risk exposed if this best practice is not established: HighTo modify usage and adjust costs, each person in your organization must have access to their cost and usage information. It is recommended that all workloads and teams have the following tooling configured when they use the cloud:Reports: Summarize of all cost and usage informationNotifications: Provide notifications when cost or usage is outside of defined limits.Current State: Configure a dashboard showing current levels of cost and usage. The dashboard should be available in a highly visible place within the work environment (similar to an operations dashboard).Trending: Provide the capability to show the variability in cost and usage over the required period of time, with the required granularity.Forecasts: Provide the capability to show estimated future costs.Tracking: Show the current cost and usage against configured goals or targets.Analysis: Provide the capability for team members to perform custom and deep analysis down to the hourly granularity, with all possible dimensions.You can use AWS native tooling, such as AWS Cost Explorer, AWS Budgets, and Amazon Athena with Amazon QuickSight to provide this capability. You can also use third-party tooling \u2014 however, you must ensure that the costs of this tooling provide value to your organization.Implementation stepsCreate a Cost Optimization group: Configure your account and create a group that has access to the required Cost and Usage reports. This group must include representatives from all teams that own or manage an application. This certifies that every team has access to their cost and usage information.Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set a budget for the overall account spend, and a budget for the workload by using tags.Configure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts. Create a dashboard for the workload that tracks overall spend, and key usage metrics for the workload.Configure advanced tooling: Optionally, you can create custom tooling for your organization that provides additional detail and granularity. You can implement advanced analysis capability using Amazon Athena, and dashboards using Amazon QuickSight.Related documents:Tagging AWS resourcesAnalyzing your costs with AWS BudgetsAnalyzing your costs with Cost ExplorerManaging AWS Cost and Usage ReportsRelated examples:Well-Architected Labs - AWS Account SetupWell-Architected Labs: Billing VisualizationWell-Architected Labs: Cost and Governance UsageWell-Architected Labs: Cost and Usage AnalysisWell-Architected Labs: Cost and Usage Visualization\n* Implement tooling and dashboards to monitor cost proactively for the workload. Regularly review the costs with configured tools or out of the box tools, do not just look at costs and categories when you receive notifications. Monitoring and analyzing costs proactively helps to identify positive trends and allows you to promote them throughout your organization.Level of risk exposed if this best practice is not established: LowIt is recommended to monitor cost and usage proactively within your organization, not just when there are exceptions or anomalies. Highly visible dashboards throughout your office or work environment ensure that key people have access to the information they need, and indicate the organization\u2019s focus on cost optimization. Visible dashboards allow you to actively promote successful outcomes and implement them throughout your organization.Create a daily or frequent routine to use AWS Cost Explorer or any other dashboard such as Amazon QuickSight to see the costs and analyze proactively. Analyze AWS service usage and costs at the AWS account-level, workload-level, or specific AWS service-level with grouping and filtering, and validate whether they are expected or not. Use the hourly- and resource-level granularity and tags to filter and identify incurring costs for the top resources. You can also build your own reports with the Cost Intelligence Dashboard, an Amazon QuickSight solution built by AWS Solutions Architects, and compare your budgets with the actual cost and usage.Implementation stepsReport on cost optimization: Set up a regular cycle to discuss and analyze the efficiency of the workload. Using the metrics established, report on the metrics achieved and the cost of achieving them. Identify and fix any negative trends, and identify positive trends to promote across your organization. Reporting should involve representatives from the application teams and owners, finance, and management.Create and enable daily granularity AWS Budgets for the cost and usage to take timely actions to prevent any potential cost overruns: AWS Budgets allow you to configure alert notifications, so you stay informed if any of your budget types fall out of your pre-configured thresholds. The best way to leverage AWS Budgets is to set your expected cost and usage as your limits, so that anything above your budgets can be considered overspend.Create AWS Cost Anomaly Detection for cost monitor: AWS Cost Anomaly Detection uses advanced Machine Learning technology to identify anomalous spend and root causes, so you can quickly take action. It allows you to configure cost monitors that define spend segments you want to evaluate (for example, individual AWS services, member accounts, cost allocation tags, and cost categories), and lets you set when, where, and how you receive your alert notifications. For each monitor, attach multiple alert subscriptions for business owners and technology teams, including a name, a cost impact threshold, and alerting frequency (individual alerts, daily summary, weekly summary) for each subscription.Use AWS Cost Explorer or integrate your AWS Cost and Usage Report (CUR) data with Amazon QuickSight dashboards to visualize your organization\u2019s costs: AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. The Cost Intelligence Dashboard is a customizable and accessible dashboard to help create the foundation of your own cost management and optimization tool.Related documents:AWS BudgetsAWS Cost ExplorerDaily Cost and Usage BudgetsAWS Cost Anomaly DetectionRelated examples:Well-Architected Labs: VisualizationWell-Architected Labs: Advanced VisualizationWell-Architected Labs: Cloud Intelligence DashboardsWell-Architected Labs: Cost VisualizationAWS Cost Anomaly Detection Alert with Slack\n\nKnowledge Area: AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS cost management tool should the marketing company use to process and analyze large volumes of customer data for targeted advertising campaigns?\",\n\"answer_choices\": [{\"answer\": \"Amazon Athena with Amazon QuickSight\",\"is_correct\": \"True\",\"explanation\": \"Amazon Athena with Amazon QuickSight allows for custom and deep analysis down to the hourly granularity, with all possible dimensions, and provides dashboards using Amazon QuickSight.\"},{\"answer\": \"AWS Cost Explorer\",\"is_correct\": \"False\",\"explanation\": \"AWS Cost Explorer allows for visualization, understanding, and management of AWS costs and usage over time, but does not provide deep analysis.\"},{\"answer\": \"AWS Budgets\",\"is_correct\": \"False\",\"explanation\": \"AWS Budgets allows for alert notifications when costs or usage are outside of defined limits, but does not provide deep analysis.\"},{\"answer\": \"AWS Cost and Usage Report\",\"is_correct\": \"False\",\"explanation\": \"AWS Cost and Usage Report allows for integration with Amazon QuickSight dashboards to visualize organization costs, but does not provide deep analysis.\"}]}", "docs": ["('COST03-BP04 Configure billing and cost management tools - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_monitor_usage_config_tools.html')", "('COST01-BP06 Monitor cost proactively - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_cloud_financial_management_proactive_process.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Knowledge of", "item": "Caching strategies", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* Where applicable, use multiple strategies to improve performance. For example, using strategies like caching data to prevent excessive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Common anti-patterns:You assume that workload performance is adequate if customers are not complaining.You only collect data on performance after you have made performance-related changes.Benefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and historical reference. You can create dashboards and perform metric math on your data to derive operational and utilization insights for your workloads as they change over time.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Related documents:Amazon Builders\u2019 LibraryBest Practices for Implementing Amazon ElastiCacheAWS Database CachingAmazon CloudWatch RUMDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)AWS purpose-built databases (DAT209-L)Optimize applications through Amazon CloudWatch RUMRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n* The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently.Best practices\n* Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload. Fetch data from shared storage only as needed. Detach unused volumes to make more resources available.Level of risk exposed if this best practice is not established: LowMigrate data to shared storage when the data has multiple consumers.Fetch data from shared storage only as needed.Delete data as appropriate for your usage patterns, and implement time-to-live (TTL) functionality to manage cached data.Detach volumes from clients that are not actively using them.Related documents:Amazon FSxCaching strategiesWhat is Amazon Elastic File System?What is Amazon S3?\n* Mechanisms exist for your team members to discover the information that they are looking for in a timely manner, access it, and identify that it\u2019s current and complete. Mechanisms are present to identify needed content, content in need of refresh, and content that should be archived so that it\u2019s no longer referenced.Common anti-patterns:A single frustrated customer opens a support case for a new product feature request to address a perceived issue. It is added to the list of priority improvements.Level of risk exposed if this best practice is not established: HighKnowledge management: Ensure mechanisms exist for your team members to discover the information that they are looking for in a timely manner, access it, and identify that it\u2019s current and complete. Maintain mechanisms to identify needed content, content in need of refresh, and content that should be archived so that it\u2019s no longer referenced.\n* When evaluating performance-related improvements, determine which choices will impact your customers and workload efficiency. For example, if using a key-value data store increases system performance, it is important to evaluate how the eventually consistent nature of it will impact customers.Identify areas of poor performance in your system through metrics and monitoring. Determine how you can make improvements, what trade-offs those improvements bring, and how they impact the system and the user experience. For example, implementing caching data can help dramatically improve performance but requires a clear strategy for how and when to update or invalidate cached data to prevent incorrect system behavior.Common anti-patterns:You assume that all performance gains should be implemented, even if there are tradeoffs for implementation such as eventual consistency.You only evaluate changes to workloads when a performance issue has reached a critical point.Benefits of establishing this best practice: When you are evaluating potential performance-related improvements, you must decide if the tradeoffs for the changes are consistent with the workload requirements. In some cases, you may have to implement additional controls to compensate for the tradeoffs.Level of risk exposed if this best practice is not established: HighIdentify tradeoffs: Use metrics and monitoring to identify areas of poor performance in your system. Determine how to make improvements, and how tradeoffs will impact the system and the user experience. For example, implementing caching data can help dramatically improve performance, but it requires a clear strategy for how and when to update or invalidate cached data to prevent incorrect system behavior.Related documents:Amazon Builders\u2019 LibraryAmazon QuickSight KPIsAmazon CloudWatch RUMX-Ray DocumentationRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)Build a Monitoring PlanOptimize applications through Amazon CloudWatch RUMDemo of Amazon CloudWatch SyntheticsRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web Client\n* When architecting solutions, determining tradeoffs enables you to select an optimal approach. Often you can improve performance by trading consistency, durability, and space for time and latency.Best practices\n\nKnowledge Area: Caching strategies\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following is a best practice for caching strategies to improve performance for a healthcare company storing and processing sensitive patient information in the cloud?\", \n\"answer_choices\": [{\"answer\": \"Implement a distributed load testing on AWS\",\"is_correct\": \"False\",\"explanation\": \"Load testing is part of a process to understand how tradeoffs impact the workload, not part of the cache strategy.\"}, \n{\"answer\": \"Create a dashboard to collect and evaluate metrics\",\"is_correct\": \"True\",\"explanation\": \"Creating a dashboard to collect and evaluate metrics is a best practice for understanding the impact of tradeoffs on system performance.\"},  \n{\"answer\": \"Adopt a single source of truth to reduce storage requirements\",\"is_correct\": \"True\",\"explanation\": \"Adopting a single source of truth to reduce storage requirements is a best practice for efficient use of resources.\"}, \n{\"answer\": \"Fetch data from shared storage only when needed\",\"is_correct\": \"True\",\"explanation\": \"Fetching data from shared storage only when needed is a best practice for cache strategies to improve performance.\"}]}", "docs": ["('PERF08-BP05 Use various performance-related strategies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_implement_strategy.html')", "('PERF 3\\xa0 How do you select your storage solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/peff-03.html')", "('SUS04-BP06 Use shared file systems or object storage to access common data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a7.html')", "('OPS11-BP04 Perform knowledge management - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_evolve_ops_knowledge_management.html')", "('PERF08-BP03 Identify how tradeoffs impact customers and efficiency - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_understand_impact.html')", "('PERF 8\\xa0 How do you use tradeoffs to improve performance? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-08.html')"]}, {"scenario": "A company has a critical application that needs to be up at all times", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Knowledge of", "item": "Data retention policies", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company has a critical application that needs to be up at all times`\"\n\nContext:\n\n* Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.Best practices:\n* Manage the lifecycle of all your data and automatically enforce deletion timelines to minimize the total storage requirements of your workload.Level of risk exposed if this best practice is not established: LowDefine lifecycle policies for all your data classification types.Set automated lifecycle policies to enforce lifecycle rules.Delete unused volumes and snapshots.Aggregate data where applicable based on lifecycle rules.Related documents:Amazon ECR Lifecycle policiesAmazon EFS lifecycle managementAmazon S3 Intelligent-TieringEvaluating Resources with AWS Config RulesManaging your storage lifecycle on Amazon S3Object lifecycle policies in AWS Elemental MediaStoreRelated videos:Amazon S3 Lifecycle\n* Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO).Best practices\n* To minimize storage consumption, only back up data that has business value or is needed to satisfy compliance requirements. Examine backup policies and exclude ephemeral storage that doesn\u2019t provide value in a recovery scenario.Level of risk exposed if this best practice is not established: LowUse your data classification to establish what data needs to be backed up.Exclude data that you can easily recreate.Exclude ephemeral data from your backups.Exclude local copies of data, unless the time required to restore that data from a common location exceeds your service level agreements (SLAs).Related documents:Using AWS Backup to back up and restore Amazon EFS file systemsAmazon EBS snapshotsWorking with backups on Amazon Relational Database Service\n* Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access or mishandling.Best practices\n* Classification provides a way to categorize data, based on criticality and sensitivity in order to help you determine appropriate protection and retention controls.Best practices\n* Identify and document the workload storage needs and define the storage characteristics of each location. Examples of storage characteristics include: shareable access, file size, growth rate, throughput, IOPS, latency, access patterns, and persistence of data. Use these characteristics to evaluate if block, file, object, or instance storage services are the most efficient solution for your storage needs.\n* Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Components might become idle from lack of use because of changes in user behavior over time. Revise patterns and architecture to consolidate under-utilized components to increase overall utilization. Retire components that are no longer required. Understand the performance of your workload components, and optimize the components that consume the most resources. Be aware of the devices your customers use to access your services, and implement patterns to minimize the need for device upgrades.The following question focuses on these considerations for sustainability:Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.Implement a data classification policy: Classify data to understand its significance to business outcomes. Use this information to determine when you can move data to more energy-efficient storage or safely delete it.Use technologies that support data access and storage patterns: Use storage that best supports how your data is accessed and stored to minimize the resources provisioned while supporting your workload. For example, solid state devices (SSDs) are more energy intensive than magnetic drives and should be used only for active data use cases. Use energy-efficient, archival-class storage for infrequently accessed data.Use lifecycle policies to delete unnecessary data: Manage the lifecycle of all your data and automatically enforce deletion timelines to minimize the total storage requirements of your workload.Minimize over-provisioning in block storage: To minimize total provisioned storage, create block storage with size allocations that are appropriate for the workload. Use elastic volumes to expand storage as data grows without having to resize storage attached to compute resources. Regularly review elastic volumes and shrink over-provisioned volumes to fit the current data size.Remove unneeded or redundant data: Duplicate data only when necessary to minimize total storage consumed. Use backup technologies that deduplicate data at the file and block level. Limit the use of Redundant Array of Independent Drives (RAID) configurations except where required to meet SLAs.Use shared file systems or object storage to access common data: Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload. Fetch data from shared storage only as needed. Detach unused volumes to free resources. Minimize data movement across networks: Use shared storage and access data from regional data stores to minimize the total networking resources required to support data movement for your workload.Back up data only when difficult to recreate: To minimize storage consumption, only back up data that has business value or is required to satisfy compliance requirements. Examine backup policies and exclude ephemeral storage that doesn\u2019t provide value in a recovery scenario.SUS 4: How do you take advantage of data access and usage patterns to support your sustainability goals?Implement data management practices to reduce the provisioned storage required to support your workload, and the resources required to use it. Understand your data, and use storage technologies and configurations that best support the business value of the data and how it\u2019s used. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.\n\nKnowledge Area: Data retention policies\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"How do you take advantage of data access and usage patterns to support your sustainability goals?\",\n\"answer_choices\": [{\"answer\": \"Identify and document the workload storage needs and define the storage characteristics of each location.\",\"is_correct\": \"true\",\"explanation\": \"Identifying and documenting the workload storage needs and defining the storage characteristics of each location is an important step in taking advantage of data access and usage patterns to support sustainability goals. This will help determine which storage services are the most efficient solution for your storage needs.\"},{\"answer\": \"Delete unnecessary data.\",\"is_correct\": \"false\",\"explanation\": \"Deleting unnecessary data is part of a data retention policy. This is not the best way to take advantage of data access and usage patterns to support sustainability goals.\"},{\"answer\": \"Exclude data that you can easily recreate.\",\"is_correct\": \"false\",\"explanation\": \"Excluding data that you can easily recreate is part of a data retention policy. This is not the best way to take advantage of data access and usage patterns to support sustainability goals.\"},{\"answer\": \"Use technologies that support data access and storage patterns.\",\"is_correct\": \"true\",\"explanation\": \"Using technologies that support data access and storage patterns is an important step in taking advantage of data access and usage patterns to support sustainability goals. This will help minimize the resources provisioned while supporting your workload.\"}]}", "docs": ["('SUS 4 How do you take advantage of data access and usage patterns to support your sustainability goals? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-04.html')", "('SUS04-BP03 Use lifecycle policies to delete unnecessary data - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a4.html')", "('REL 9\\xa0 How do you back up data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-09.html')", "('SUS04-BP08 Back up data only when difficult to recreate - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_data_a9.html')", "('SEC 8\\xa0 How do you protect your data at rest? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-08.html')", "('SEC 7\\xa0 How do you classify your data? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-07.html')", "('PERF03-BP01 Understand storage characteristics and requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_storage_solution_understand_char.html')", "('Data patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus-data-patterns.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Knowledge of", "item": "Database capacity planning (for example, capacity units)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n\nKnowledge Area: Database capacity planning (for example, capacity units)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice for capacity planning in the AWS cloud when it comes to selecting the optimal database solution for a system?\", \"answer_choices\": [{\"answer\": \"Use the access patterns of the workload to decide which services and technologies to use\",\"is_correct\": \"True\",\"explanation\": \"Using access patterns to decide which services and technologies to use is the best practice for capacity planning in the AWS cloud when it comes to selecting the optimal database solution for a system.\"},{\"answer\": \"You only select one database vendor to simplify operations management\",\"is_correct\": \"False\",\"explanation\": \"You should not only select one database vendor to simplify operations management. You should use the access patterns of the workload to decide which services and technologies to use when selecting the optimal database solution for a system.\"},{\"answer\": \"You assume that data access patterns will stay consistent over time\",\"is_correct\": \"False\",\"explanation\": \"You should not assume that data access patterns will stay consistent over time. You should use the access patterns of the workload to decide which services and technologies to use when selecting the optimal database solution for a system.\"},{\"answer\": \"The database is configured to support a potential high traffic burst\",\"is_correct\": \"False\",\"explanation\": \"The database should not be configured to support a potential high traffic burst, which can result in the database resources remaining idle most of the time. You should use the access patterns of the workload to decide which services and technologies to use when selecting the optimal database solution for a system.\"}]}", "docs": ["('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')", "('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Knowledge of", "item": "Database connections and proxies", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* For high availability, always (when possible) deploy your workload components to multiple Availability Zones (AZs), as shown in Figure 10. For workloads with extreme resilience requirements, carefully evaluate the options for a multi-Region architecture.\n* Implement services to reduce data transfer. For example, using a content delivery network (CDN) such as Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using AWS Direct Connect instead of VPN for connectivity to AWS.Level of risk exposed if this best practice is not established: LowAmazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources. By using CloudFront, you can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network. It is ideal for small offices or business partners because it provides quick and easy connectivity, and it is a fully managed and elastic service.VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs. Gateway VPC endpoints have no hourly charges, and support Amazon Simple Storage Service(Amazon S3) and Amazon DynamoDB. Interface VPC endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost.Implementation stepsImplement services: Using the data transfer modeling, look at where the largest costs and highest volume flows are. Review the AWS services and assess whether there is a service that reduces or removes the transfer, specifically networking and content delivery. Also look for caching services where there is repeated access to data, or large amounts of data.Related documents:AWS Direct ConnectAWS Explore Our ProductsAWS caching solutionsAmazon CloudFrontDeliver content faster with Amazon CloudFront\n\nKnowledge Area: Database connections and proxies\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which services should be implemented to reduce data transfer costs for the company in the scenario?\", \n\"answer_choices\": [\n    {\"answer\": \"AWS VPN\", \"is_correct\": \"false\", \"explanation\": \"AWS VPN establishes a secure and private connection between a private network and the AWS global network, reducing network costs. However, it is not designed to reduce data transfer costs.\"}, \n    {\"answer\": \"Amazon CloudFront\", \"is_correct\": \"true\", \"explanation\": \"Amazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources, reducing data transfer costs.\"}, \n    {\"answer\": \"AWS Direct Connect\", \"is_correct\": \"false\", \"explanation\": \"AWS Direct Connect allows you to establish a dedicated network connection to AWS, reducing network costs. However, it is not designed to reduce data transfer costs.\"},\n    {\"answer\": \"VPC Endpoints\", \"is_correct\": \"false\", \"explanation\": \"VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs. However, they are not designed to reduce data transfer costs.\"}\n]}", "docs": ["('REL10-BP02 Select the appropriate locations for your multi-location deployment - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_fault_isolation_select_location.html')", "('COST08-BP03 Implement services to reduce data transfer costs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_implement_services.html')"]}, {"scenario": "A telecommunications company wants to use AWS to host and manage their network infrastructure.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Knowledge of", "item": "Database engines with appropriate use cases (for example, heterogeneous migrations, homogeneous migrations)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA telecommunications company wants to use AWS to host and manage their network infrastructure.`\"\n\nContext:\n\n* The cloud offers purpose-built database services that address different problems presented by your workload. You can choose from many purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. By picking the best database to solve a specific problem (or a group of problems), you can break away from restrictive one-size-fits-all monolithic databases and focus on building applications to meet the performance needs of your customers.In AWS you can choose from multiple purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. With AWS databases, you don\u2019t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running with self-healing storage and automated scaling, so that you can focus on higher value application development.The following question focuses on these considerations for performance efficiency.Your workload's database approach has a significant impact on performance efficiency. It's often an area that is chosen according to organizational defaults rather than through a data-driven approach. As with storage, it is critical to consider the access patterns of your workload, and also to consider if other non-database solutions could solve the problem more efficiently (such as using graph, time series, or in-memory storage database).PERF 4: How do you select your database solution?The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n* Use the access patterns of the workload to decide which services and technologies to use. In addition to non-functional requirements such as performance and scale, access patterns heavily influence the choice of the database and storage solutions. The first dimension is the need for transactions, ACID compliance, and consistent reads. Not every database supports these and most of the NoSQL databases provide an eventual consistency model. The second important dimension would be the distribution of write and reads over time and space. Globally distributed applications need to consider the traffic patterns, latency and access requirements in order to identify the optimal storage solution. The third crucial aspect to choose is the query pattern flexibility, random access patterns, and one-time queries. Considerations around highly specialized query functionality for text and natural language processing, time series, and graphs must also be taken into account.Desired outcome: The data storage has been selected based on identified and documented data access patterns. This might include the most common read, write and delete queries, the need for ad-hoc calculations and aggregations, complexity of the data, the data interdependency, and the required consistency needs.Common anti-patterns:You only select one database vendor to simplify operations management.You assume that data access patterns will stay consistent over time.You implement complex transactions, rollback, and consistency logic in the application.The database is configured to support a potential high traffic burst, which results in the database resources remaining idle most of the time.Using a shared database for transactional and analytical uses.Benefits of establishing this best practice: Selecting and optimizing your data storage based on access patterns will help decrease development complexity and optimize your performance opportunities. Understanding when to use read replicas, global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.Level of risk exposed if this best practice is not established: MediumIdentify and evaluate your data access pattern to select the correct storage configuration. Each database solution has options to configure and optimize your storage solution. Use the collected metrics and logs and experiment with options to find the optimal configuration. Use the following table to review storage options per database service.Implementation steps:Identify and document the anticipated growth of the data and traffic.Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for analytics or scaling horizontally via sharding.DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances can have new EBS volumes added at a later date for additional storage.Instance types can be changed based on changes in activity. For example, you can start with a smaller instance while you are testing, then scale the instance as you begin to receive production traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.Document requirements around normal and peak performance (transactions per second TPS and queries per second QPS) and consistency (ACID and eventual consistency).Document solution deployment aspects and the database access requirements (global, Mult-AZ, read replication, multiple write nodes)Level of effort for the implementation plan: If you do not have logs or metrics for your data management solution, you will need to complete that before identifying and documenting your data access patterns. Once your data access pattern is understood, selecting, and configuring your data storage is a low level of effort.Related documents:AWS Database CachingAmazon Athena top 10 performance tipsAmazon Aurora best practicesAmazon DynamoDB AcceleratorAmazon DynamoDB best practicesAmazon Redshift Spectrum best practicesAmazon Redshift performanceCloud Databases with AWSAmazon RDS Storage TypesRelated videos:AWS purpose-built databases (DAT209-L)Amazon Aurora storage demystified: How it all works (DAT309-R)Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)Related examples:Experiment and test with Distributed Load Testing on AWSAWS ServicesAmazon RDS, Amazon AuroraAmazon DynamoDBAmazon DocumentDBAmazon ElastiCacheAmazon NeptuneAmazon TimestreamAmazon KeyspacesAmazon QLDBScaling StorageStorage automatic scaling option available to automatically scale provisioned storage IOPS can also be scaled independently of provisioned storage when leveraging provisioned IOPs storage typesAutomatically scales. Tables are unconstrained in terms of size.Storage automatic scaling option available scale provisioned storageStorage is in-memory, tied to instance type or countStorage automatic scaling option available to automatically scale provisioned storageConfigure retention period for in-memory and magnetic tiers in daysScales table storage up and down automaticallyAutomatically scales. Tables are unconstrained in terms of size.\n* Often, multiple approaches are required for optimal performance across a workload. Well-architected systems use multiple solutions and features to improve performance.Best practices\n\nKnowledge Area: Database engines with appropriate use cases (for example, heterogeneous migrations, homogeneous migrations)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which database solution should be used for a telecommunications company that wants to host and manage their network infrastructure on AWS?\", \n\"answer_choices\": [{\"answer\": \"Amazon RDS\",\"is_correct\": \"false\",\"explanation\": \"Amazon RDS is a relational database engine, and may not be the best solution for this scenario.\"},\n{\"answer\": \"Amazon Neptune\",\"is_correct\": \"false\",\"explanation\": \"Amazon Neptune is a graph database engine, and may not be the best solution for this scenario.\"},\n{\"answer\": \"Amazon DynamoDB\",\"is_correct\": \"false\",\"explanation\": \"Amazon DynamoDB is a key-value and document database engine, and may not be the best solution for this scenario.\"},\n{\"answer\": \"An analysis of the workload's access patterns to identify the best database solution\",\"is_correct\": \"true\",\"explanation\": \"Identifying the workload's access patterns is the best way to determine the optimal database solution for this scenario.\"}]}", "docs": ["('Database - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-db.html')", "('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')", "('PERF04-BP04 Choose data storage based on access patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_access_patterns.html')", "('PERF 1\\xa0 How do you select the best performing architecture? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-01.html')"]}, {"scenario": "A financial services company wants to run a highly available and secure infrastructure to process financial transactions.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Knowledge of", "item": "Database replication (for example, read replicas)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA financial services company wants to run a highly available and secure infrastructure to process financial transactions.`\"\n\nContext:\n\n* Understand the available database options and how it can optimize your performance before you select your data management solution. Use load testing to identify database metrics that matter for your workload. While you explore the database options, take into consideration various aspects such as the parameter groups, storage options, memory, compute, read replica, eventual consistency, connection pooling, and caching options. Experiment with these various configuration options to improve the metrics.\n\nKnowledge Area: Database replication (for example, read replicas)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the most efficient method to configure a financial services company's highly available and secure infrastructure to process financial transactions?\", \n\"answer_choices\": [\n    {\"answer\": \"Configure load balancing to evenly distribute traffic across multiple database instances.\",\"is_correct\": \"False\",\"explanation\": \"This will not ensure high availability, as it does not provide redundancy.\"},\n    {\"answer\": \"Use database replication to create read replicas and utilize caching to improve read performance.\",\"is_correct\": \"True\", \"explanation\": \"Database replication will ensure high availability, as multiple copies of the database can be used to handle requests. Additionally, caching can improve read performance, allowing queries to be answered more quickly.\"},\n    {\"answer\": \"Set up a master-slave database architecture to define the order in which operations are replicated.\",\"is_correct\": \"False\",\"explanation\": \"This will not ensure high availability as it is a single point of failure. Additionally, it will not improve read performance.\"},\n    {\"answer\": \"Enable automated backups to store the database in a secure location.\",\"is_correct\": \"False\",\"explanation\": \"This will not ensure high availability, as it does not provide redundancy. Additionally, it will not improve read performance.\"}\n    ]\n}", "docs": ["('PERF04-BP02 Evaluate the available options - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_right_database_solution_evaluate_options.html')"]}, {"scenario": "A company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Skills in", "item": "Determining an appropriate database engine (for example, MySQL compared with PostgreSQL)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.`\"\n\nContext:\n\n* The cloud offers purpose-built database services that address different problems presented by your workload. You can choose from many purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. By picking the best database to solve a specific problem (or a group of problems), you can break away from restrictive one-size-fits-all monolithic databases and focus on building applications to meet the performance needs of your customers.In AWS you can choose from multiple purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. With AWS databases, you don\u2019t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running with self-healing storage and automated scaling, so that you can focus on higher value application development.The following question focuses on these considerations for performance efficiency.Your workload's database approach has a significant impact on performance efficiency. It's often an area that is chosen according to organizational defaults rather than through a data-driven approach. As with storage, it is critical to consider the access patterns of your workload, and also to consider if other non-database solutions could solve the problem more efficiently (such as using graph, time series, or in-memory storage database).PERF 4: How do you select your database solution?The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n* Evaluate the impact of tradeoffs between competing interests or alternative approaches, to help make informed decisions when determining where to focus efforts or choosing a course of action. For example, accelerating speed to market for new features may be emphasized over cost optimization, or you may choose a relational database for non-relational data to simplify the effort to migrate a system, rather than migrating to a database optimized for your data type and updating your application.AWS can help you educate your teams about AWS and its services to increase their understanding of how their choices can have an impact on your workload. You should use the resources provided by AWS Support (AWS Knowledge Center, AWS Discussion Forums, and AWS Support Center) and AWS Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS questions.AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog and The Official AWS Podcast.Common anti-patterns:You are using a relational database to manage time series and non-relational data. There are database options that are optimized to support the data types you are using but you are unaware of the benefits because you have not evaluated the tradeoffs between solutions.Your investors request that you demonstrate compliance with Payment Card Industry Data Security Standards (PCI DSS). You do not consider the tradeoffs between satisfying their request and continuing with your current development efforts. Instead you proceed with your development efforts without demonstrating compliance. Your investors stop their support of your company over concerns about the security of your platform and their investments.Benefits of establishing this best practice: Understanding the implications and consequences of your choices enables you to prioritize your options.Level of risk exposed if this best practice is not established: MediumEvaluate tradeoffs: Evaluate the impact of tradeoffs between competing interests, to help make informed decisions when determining where to focus efforts. For example, accelerating speed to market for new features might be emphasized over cost optimization.AWS can help you educate your teams about AWS and its services to increase their understanding of how their choices can have an impact on your workload. You should use the resources provided by AWS Support (AWS Knowledge Center, AWS Discussion Forums, and AWS Support Center) and AWS Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS questions.AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog and The Official AWS Podcast.Related documents:AWS BlogAWS Cloud ComplianceAWS Discussion ForumsAWS DocumentationAWS Knowledge CenterAWS SupportAWS Support CenterThe Amazon Builders' LibraryThe Official AWS Podcast\n\nKnowledge Area: Determining an appropriate database engine (for example, MySQL compared with PostgreSQL)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which database engine should the company use to run batch processing jobs on AWS in order to minimize costs when the jobs are not running?\", \n\"answer_choices\": [\n    {\"answer\": \"MySQL\",\"is_correct\": \"false\",\"explanation\": \"MySQL is a relational database engine and is not optimized for batch processing jobs.\"}, \n    {\"answer\": \"PostgreSQL\",\"is_correct\": \"false\",\"explanation\": \"PostgreSQL is a relational database engine and is not optimized for batch processing jobs.\"}, \n    {\"answer\": \"Amazon Aurora\",\"is_correct\": \"false\",\"explanation\": \"Amazon Aurora is a relational database engine and is not optimized for batch processing jobs.\"}, \n    {\"answer\": \"AWS Batch\",\"is_correct\": \"true\",\"explanation\": \"AWS Batch is a serverless compute service that allows you to run batch processing jobs on AWS with minimal costs when the jobs are not running.\"}\n]\n}", "docs": ["('Database - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-db.html')", "('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')", "('OPS01-BP06 Evaluate tradeoffs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/ops_priorities_eval_tradeoffs.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Skills in", "item": "Determining cost-effective AWS database services with appropriate use cases (for example, DynamoDB compared with Amazon RDS, serverless)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service (Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon Simple Storage Service (Amazon S3)for static websites, and Amazon Elastic Container Service (Amazon ECS). Minimize license costs by using open source software, or software that does not have license fees: for example, Amazon Linux for compute workloads or migrate databases to Amazon Aurora.Level of risk exposed if this best practice is not established: LowYou can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a resource, and provide the function of code execution, queuing services, and message delivery. The other benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation and attribution.For more information on Serverless, refer to the Well-Architected Serverless Application Lens whitepaper.Implementation stepsSelect each service to optimize cost: Using your prioritized list and analysis, select each option that provides the best match with your organizational priorities.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Using the appropriate instances and resources for your workload is key to cost savings. For example, a reporting process might take five hours to run on a smaller server but one hour to run on a larger server that is twice as expensive. Both servers give you the same outcome, but the smaller server incurs more cost over time.A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. For example, rather than maintaining servers to deliver email, you can use a service that charges on a per-message basis.AWS offers a variety of flexible and cost-effective pricing options to acquire instances from Amazon EC2 and other services in a way that best fits your needs. On-DemandInstances allow you to pay for compute capacity by the hour, with no minimum commitments required. Savings Plans and Reserved Instances offer savings of up to 75% o\ufb00 On-Demand pricing. With Spot Instances, you can leverage unused Amazon EC2 capacity and offer savings of up to 90% o\ufb00 On-Demand pricing. Spot Instances are appropriate where the system can tolerate using a \ufb02eet of servers where individual servers can come and go dynamically, such as stateless web servers, batch processing, or when using HPC and big data.Appropriate service selection can also reduce usage and costs; such as CloudFront to minimize data transfer, or completely eliminate costs, such as utilizing Amazon Aurora on RDS to remove expensive database licensing costs.The following questions focus on these considerations for cost optimization.By factoring in cost during service selection, and using tools such as Cost Explorer and AWS Trusted Advisor to regularly review your AWS usage, you can actively monitor your utilization and adjust your deployments accordingly.COST 5: How do you evaluate cost when you select services?Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.COST 6: How do you meet cost targets when you select resource type, size and number?Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number.COST 7: How do you use pricing models to reduce cost?Use the pricing model that is most appropriate for your resources to minimize expense.COST 8: How do you plan for data transfer charges?Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time.\n* As AWS releases new services and features, it's a best practice to review your existing architectural decisions to ensure they continue to be the most cost effective. As your requirements change, be aggressive in decommissioning resources, entire services, and systems that you no longer require.Implementing new features or resource types can optimize your workload incrementally, while minimizing the effort required to implement the change. This provides continual improvements in efficiency over time and ensures you remain on the most updated technology to reduce operating costs. You can also replace or add new components to the workload with new services. This can provide significant increases in efficiency, so it's essential to regularly review your workload, and implement new services and features.The following question focuses on these considerations for cost optimization.When regularly reviewing your deployments, assess how newer services can help save you money. For example, Amazon Aurora on RDS can reduce costs for relational databases. Using serverless such as Lambda can remove the need to operate and manage instances to run code.COST 10: How do you evaluate new services?As AWS releases new services and features, it's a best practice to review your existing architectural decisions to ensure they continue to be the most cost effective.\n\nKnowledge Area: Determining cost-effective AWS database services with appropriate use cases (for example, DynamoDB compared with Amazon RDS, serverless)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which of the following services is the most cost effective for running a disaster recovery plan on AWS?\", \"answer_choices\": [{\"answer\": \"Amazon EC2\", \"is_correct\": \"false\", \"explanation\": \"Amazon EC2 is a building block service, which may not be the most cost effective for running a disaster recovery plan on AWS\"}, {\"answer\": \"Amazon S3\", \"is_correct\": \"false\", \"explanation\": \"Amazon S3 is a storage service, which may not be the most cost effective for running a disaster recovery plan on AWS\"}, {\"answer\": \"AWS Lambda\", \"is_correct\": \"true\", \"explanation\": \"AWS Lambda is a serverless service, which can be cost effective for running a disaster recovery plan on AWS\"}, {\"answer\": \"Amazon EBS\", \"is_correct\": \"false\", \"explanation\": \"Amazon EBS is a block storage service, which may not be the most cost effective for running a disaster recovery plan on AWS\"}]}", "docs": ["('COST05-BP05 Select components of this workload to optimize cost in line with organization priorities - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_select_for_cost.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('Cost-effective resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-cereso.html')", "('Optimize over time - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-opti.html')"]}, {"scenario": "A social media platform wants to use AWS to store and process user-generated content.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Skills in", "item": "Determining cost-effective AWS database types (for example, time series format, columnar format)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA social media platform wants to use AWS to store and process user-generated content.`\"\n\nContext:\n\n* Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.Level of risk exposed if this best practice is not established: LowAnalyze your data access and storage patterns.Store data files in efficient file formats such as Parquet to prevent unnecessary processing (for example, when running analytics) and to reduce the total storage provisioned.Use technologies that work natively with compressed data.Use the database engine that best supports your dominant query pattern.Manage your database indexes to ensure index designs support efficient query execution.Select network protocols that reduce the amount of network capacity consumed.Related documents:Athena Compression Support file formatsCOPY from columnar data formats with Amazon RedshiftConverting Your Input Record Format in Kinesis Data FirehoseFormat Options for ETL Inputs and Outputs in AWS GlueImprove query performance on Amazon Athena by Converting to Columnar FormatsLoading compressed data files from Amazon S3 with Amazon RedshiftMonitoring DB load with Performance Insights on Amazon AuroraMonitoring DB load with Performance Insights on Amazon RDSAWS IoT FleetWiseRelated videos:Building Sustainably on AWS\n* The cloud offers purpose-built database services that address different problems presented by your workload. You can choose from many purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. By picking the best database to solve a specific problem (or a group of problems), you can break away from restrictive one-size-fits-all monolithic databases and focus on building applications to meet the performance needs of your customers.In AWS you can choose from multiple purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. With AWS databases, you don\u2019t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running with self-healing storage and automated scaling, so that you can focus on higher value application development.The following question focuses on these considerations for performance efficiency.Your workload's database approach has a significant impact on performance efficiency. It's often an area that is chosen according to organizational defaults rather than through a data-driven approach. As with storage, it is critical to consider the access patterns of your workload, and also to consider if other non-database solutions could solve the problem more efficiently (such as using graph, time series, or in-memory storage database).PERF 4: How do you select your database solution?The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.\n\nKnowledge Area: Determining cost-effective AWS database types (for example, time series format, columnar format)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When selecting an AWS database type to store and process user-generated content for a social media platform, which of the following is the most cost-effective approach?\", \n\"answer_choices\": [\n    {\"answer\": \"Using relational databases such as Amazon Aurora or Amazon RDS\",\"is_correct\": \"False\",\"explanation\": \"Relational databases are powerful and offer many features, but other database types can often be more cost-effective for the task, such as time series or columnar formats.\"},\n    {\"answer\": \"Using a key-value database such as Amazon DynamoDB\",\"is_correct\": \"False\",\"explanation\": \"Key-value databases are relatively cost-effective, but other database types may be more appropriate for the task, such as time series or columnar formats.\"},\n    {\"answer\": \"Using a time series format database such as Amazon Timestream\",\"is_correct\": \"True\",\"explanation\": \"Time series format databases are a cost-effective and appropriate choice for storing and processing user-generated content.\"},\n    {\"answer\": \"Using a columnar format database such as Amazon Redshift\",\"is_correct\": \"True\",\"explanation\": \"Columnar format databases are a cost-effective and appropriate choice for storing and processing user-generated content.\"}\n]}", "docs": ["('SUS03-BP05 Use software patterns and architectures that best support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a6.html')", "('Database - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-db.html')"]}, {"scenario": "A healthcare company wants to store and process sensitive patient information in the cloud.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 3: Design cost-optimized database solutions", "focus": "Skills in", "item": "Migrating database schemas and data to different locations and/or different database engines", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA healthcare company wants to store and process sensitive patient information in the cloud.`\"\n\nContext:\n\n* The cloud offers purpose-built database services that address different problems presented by your workload. You can choose from many purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. By picking the best database to solve a specific problem (or a group of problems), you can break away from restrictive one-size-fits-all monolithic databases and focus on building applications to meet the performance needs of your customers.In AWS you can choose from multiple purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. With AWS databases, you don\u2019t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running with self-healing storage and automated scaling, so that you can focus on higher value application development.The following question focuses on these considerations for performance efficiency.Your workload's database approach has a significant impact on performance efficiency. It's often an area that is chosen according to organizational defaults rather than through a data-driven approach. As with storage, it is critical to consider the access patterns of your workload, and also to consider if other non-database solutions could solve the problem more efficiently (such as using graph, time series, or in-memory storage database).PERF 4: How do you select your database solution?The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.\n* The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different features to improve performance. Selecting the wrong database solution and features for a system can lead to lower performance efficiency.Best practices\n* Understand how data is used within your workload, consumed by your users, transferred, and stored. Select technologies to minimize data processing and storage requirements.Level of risk exposed if this best practice is not established: LowAnalyze your data access and storage patterns.Store data files in efficient file formats such as Parquet to prevent unnecessary processing (for example, when running analytics) and to reduce the total storage provisioned.Use technologies that work natively with compressed data.Use the database engine that best supports your dominant query pattern.Manage your database indexes to ensure index designs support efficient query execution.Select network protocols that reduce the amount of network capacity consumed.Related documents:Athena Compression Support file formatsCOPY from columnar data formats with Amazon RedshiftConverting Your Input Record Format in Kinesis Data FirehoseFormat Options for ETL Inputs and Outputs in AWS GlueImprove query performance on Amazon Athena by Converting to Columnar FormatsLoading compressed data files from Amazon S3 with Amazon RedshiftMonitoring DB load with Performance Insights on Amazon AuroraMonitoring DB load with Performance Insights on Amazon RDSAWS IoT FleetWiseRelated videos:Building Sustainably on AWS\n\nKnowledge Area: Migrating database schemas and data to different locations and/or different database engines\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When considering which database engine to use when migrating data and database schemas to the cloud, what is an important best practice to follow?\",\n\"answer_choices\": [\n{\"answer\": \"Use the same database engine for all of the data and database schemas you are migrating\",\"is_correct\": \"false\",\"explanation\": \"Using the same database engine for all of your data and database schemas may not be the most efficient solution for your workload and may lead to lower performance efficiency.\"},\n{\"answer\": \"Understand how data is used within your workload, consumed by your users, transferred, and stored\",\"is_correct\": \"true\",\"explanation\": \"It is important to understand how data is used within your workload, consumed by your users, transferred, and stored in order to select the best database engine to solve a specific problem (or a group of problems).\"},\n{\"answer\": \"Always use an in-memory database engine\",\"is_correct\": \"false\",\"explanation\": \"An in-memory database engine may not always be the best solution and should be selected based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability.\"},\n{\"answer\": \"Use a one-size-fits-all monolithic database\",\"is_correct\": \"false\",\"explanation\": \"A one-size-fits-all monolithic database may not be the most efficient solution and may lead to lower performance efficiency.\"}\n]}", "docs": ["('Database - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-db.html')", "('PERF 4\\xa0 How do you select your database solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-04.html')", "('SUS03-BP05 Use software patterns and architectures that best support data access and storage patterns - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_software_a6.html')"]}, {"scenario": "A company wants to store log files from their applications for analysis and compliance purposes.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Knowledge of", "item": "AWS cost management service features (for example, cost allocation tags, multi-account billing)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to store log files from their applications for analysis and compliance purposes.`\"\n\nContext:\n\n* The increased flexibility and agility that the cloud enables encourages innovation and fast-paced development and deployment. It eliminates the manual processes and time associated with provisioning on-premises infrastructure, including identifying hardware specifications, negotiating price quotations, managing purchase orders, scheduling shipments, and then deploying the resources. However, the ease of use and virtually unlimited on-demand capacity requires a new way of thinking about expenditures.Many businesses are composed of multiple systems run by various teams. The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget.In AWS, you create an account structure with AWS Organizations or AWS Control Tower, which provides separation and assists in allocation of your costs and usage. You can also use resource tagging to apply business and organization information to your usage and cost. Use AWS Cost Explorer for visibility into your cost and usage, or create customized dashboards and analytics with Amazon Athena and Amazon QuickSight. Controlling your cost and usage is done by notifications through AWS Budgets, and controls using AWS Identity and Access Management (IAM), and Service Quotas.The following questions focus on these considerations for cost optimization.You can use cost allocation tags to categorize and track your AWS usage and costs. When you apply tags to your AWS resources (such as EC2 instances or S3 buckets), AWS generates a cost and usage report with your usage and your tags. You can apply tags that represent organization categories (such as cost centers, workload names, or owners) to organize your costs across multiple services.Ensure you use the right level of detail and granularity in cost and usage reporting and monitoring. For high level insights and trends, use daily granularity with AWS Cost Explorer. For deeper analysis and inspection use hourly granularity in AWS Cost Explorer, or Amazon Athena and Amazon QuickSight with the Cost and Usage Report (CUR) at an hourly granularity.Combining tagged resources with entity lifecycle tracking (employees, projects) makes it possible to identify orphaned resources or projects that are no longer generating value to the organization and should be decommissioned. You can set up billing alerts to notify you of predicted overspending.COST 2: How do you govern usage?Establish policies and mechanisms to ensure that appropriate costs are incurred while objectives are achieved. By employing a checks-and-balances approach, you can innovate without overspending.COST 3: How do you monitor usage and cost?Establish policies and procedures to monitor and appropriately allocate your costs. This allows you to measure and improve the cost efficiency of this workload.COST 4: How do you decommission resources?Implement change control and resource management from project inception to end-of-life. This ensures you shut down or terminate unused resources to reduce waste.\n* Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.Best practices\n* Configure AWS Cost Explorer and AWS Budgets inline with your organization policies.Level of risk exposed if this best practice is not established: HighTo modify usage and adjust costs, each person in your organization must have access to their cost and usage information. It is recommended that all workloads and teams have the following tooling configured when they use the cloud:Reports: Summarize of all cost and usage informationNotifications: Provide notifications when cost or usage is outside of defined limits.Current State: Configure a dashboard showing current levels of cost and usage. The dashboard should be available in a highly visible place within the work environment (similar to an operations dashboard).Trending: Provide the capability to show the variability in cost and usage over the required period of time, with the required granularity.Forecasts: Provide the capability to show estimated future costs.Tracking: Show the current cost and usage against configured goals or targets.Analysis: Provide the capability for team members to perform custom and deep analysis down to the hourly granularity, with all possible dimensions.You can use AWS native tooling, such as AWS Cost Explorer, AWS Budgets, and Amazon Athena with Amazon QuickSight to provide this capability. You can also use third-party tooling \u2014 however, you must ensure that the costs of this tooling provide value to your organization.Implementation stepsCreate a Cost Optimization group: Configure your account and create a group that has access to the required Cost and Usage reports. This group must include representatives from all teams that own or manage an application. This certifies that every team has access to their cost and usage information.Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set a budget for the overall account spend, and a budget for the workload by using tags.Configure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts. Create a dashboard for the workload that tracks overall spend, and key usage metrics for the workload.Configure advanced tooling: Optionally, you can create custom tooling for your organization that provides additional detail and granularity. You can implement advanced analysis capability using Amazon Athena, and dashboards using Amazon QuickSight.Related documents:Tagging AWS resourcesAnalyzing your costs with AWS BudgetsAnalyzing your costs with Cost ExplorerManaging AWS Cost and Usage ReportsRelated examples:Well-Architected Labs - AWS Account SetupWell-Architected Labs: Billing VisualizationWell-Architected Labs: Cost and Governance UsageWell-Architected Labs: Cost and Usage AnalysisWell-Architected Labs: Cost and Usage Visualization\n\nKnowledge Area: AWS cost management service features (for example, cost allocation tags, multi-account billing)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"Which of the following cost management service features can be used to categorize and track usage and costs in AWS?\", \n\"answer_choices\": [{\"answer\": \"AWS Budgets\",\"is_correct\": \"false\",\"explanation\": \"AWS Budgets provides notifications when cost or usage is outside of defined limits, rather than categorizing and tracking usage and costs.\"},{\"answer\": \"Amazon QuickSight\",\"is_correct\": \"false\",\"explanation\": \"Amazon QuickSight provides dashboards and analytics, rather than categorizing and tracking usage and costs.\"},{\"answer\": \"AWS Cost Explorer\",\"is_correct\": \"false\",\"explanation\": \"AWS Cost Explorer provides visibility into cost and usage, rather than categorizing and tracking usage and costs.\"},{\"answer\": \"Cost allocation tags\",\"is_correct\": \"true\",\"explanation\": \"Cost allocation tags can be used to categorize and track usage and costs in AWS.\"}]}", "docs": ["('Expenditure and usage awareness - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-aware.html')", "('COST 5\\xa0 How do you evaluate cost when you select services? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-05.html')", "('COST03-BP04 Configure billing and cost management tools - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_monitor_usage_config_tools.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Knowledge of", "item": "AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* Configure AWS Cost Explorer and AWS Budgets inline with your organization policies.Level of risk exposed if this best practice is not established: HighTo modify usage and adjust costs, each person in your organization must have access to their cost and usage information. It is recommended that all workloads and teams have the following tooling configured when they use the cloud:Reports: Summarize of all cost and usage informationNotifications: Provide notifications when cost or usage is outside of defined limits.Current State: Configure a dashboard showing current levels of cost and usage. The dashboard should be available in a highly visible place within the work environment (similar to an operations dashboard).Trending: Provide the capability to show the variability in cost and usage over the required period of time, with the required granularity.Forecasts: Provide the capability to show estimated future costs.Tracking: Show the current cost and usage against configured goals or targets.Analysis: Provide the capability for team members to perform custom and deep analysis down to the hourly granularity, with all possible dimensions.You can use AWS native tooling, such as AWS Cost Explorer, AWS Budgets, and Amazon Athena with Amazon QuickSight to provide this capability. You can also use third-party tooling \u2014 however, you must ensure that the costs of this tooling provide value to your organization.Implementation stepsCreate a Cost Optimization group: Configure your account and create a group that has access to the required Cost and Usage reports. This group must include representatives from all teams that own or manage an application. This certifies that every team has access to their cost and usage information.Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set a budget for the overall account spend, and a budget for the workload by using tags.Configure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts. Create a dashboard for the workload that tracks overall spend, and key usage metrics for the workload.Configure advanced tooling: Optionally, you can create custom tooling for your organization that provides additional detail and granularity. You can implement advanced analysis capability using Amazon Athena, and dashboards using Amazon QuickSight.Related documents:Tagging AWS resourcesAnalyzing your costs with AWS BudgetsAnalyzing your costs with Cost ExplorerManaging AWS Cost and Usage ReportsRelated examples:Well-Architected Labs - AWS Account SetupWell-Architected Labs: Billing VisualizationWell-Architected Labs: Cost and Governance UsageWell-Architected Labs: Cost and Usage AnalysisWell-Architected Labs: Cost and Usage Visualization\n* Implement tooling and dashboards to monitor cost proactively for the workload. Regularly review the costs with configured tools or out of the box tools, do not just look at costs and categories when you receive notifications. Monitoring and analyzing costs proactively helps to identify positive trends and allows you to promote them throughout your organization.Level of risk exposed if this best practice is not established: LowIt is recommended to monitor cost and usage proactively within your organization, not just when there are exceptions or anomalies. Highly visible dashboards throughout your office or work environment ensure that key people have access to the information they need, and indicate the organization\u2019s focus on cost optimization. Visible dashboards allow you to actively promote successful outcomes and implement them throughout your organization.Create a daily or frequent routine to use AWS Cost Explorer or any other dashboard such as Amazon QuickSight to see the costs and analyze proactively. Analyze AWS service usage and costs at the AWS account-level, workload-level, or specific AWS service-level with grouping and filtering, and validate whether they are expected or not. Use the hourly- and resource-level granularity and tags to filter and identify incurring costs for the top resources. You can also build your own reports with the Cost Intelligence Dashboard, an Amazon QuickSight solution built by AWS Solutions Architects, and compare your budgets with the actual cost and usage.Implementation stepsReport on cost optimization: Set up a regular cycle to discuss and analyze the efficiency of the workload. Using the metrics established, report on the metrics achieved and the cost of achieving them. Identify and fix any negative trends, and identify positive trends to promote across your organization. Reporting should involve representatives from the application teams and owners, finance, and management.Create and enable daily granularity AWS Budgets for the cost and usage to take timely actions to prevent any potential cost overruns: AWS Budgets allow you to configure alert notifications, so you stay informed if any of your budget types fall out of your pre-configured thresholds. The best way to leverage AWS Budgets is to set your expected cost and usage as your limits, so that anything above your budgets can be considered overspend.Create AWS Cost Anomaly Detection for cost monitor: AWS Cost Anomaly Detection uses advanced Machine Learning technology to identify anomalous spend and root causes, so you can quickly take action. It allows you to configure cost monitors that define spend segments you want to evaluate (for example, individual AWS services, member accounts, cost allocation tags, and cost categories), and lets you set when, where, and how you receive your alert notifications. For each monitor, attach multiple alert subscriptions for business owners and technology teams, including a name, a cost impact threshold, and alerting frequency (individual alerts, daily summary, weekly summary) for each subscription.Use AWS Cost Explorer or integrate your AWS Cost and Usage Report (CUR) data with Amazon QuickSight dashboards to visualize your organization\u2019s costs: AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. The Cost Intelligence Dashboard is a customizable and accessible dashboard to help create the foundation of your own cost management and optimization tool.Related documents:AWS BudgetsAWS Cost ExplorerDaily Cost and Usage BudgetsAWS Cost Anomaly DetectionRelated examples:Well-Architected Labs: VisualizationWell-Architected Labs: Advanced VisualizationWell-Architected Labs: Cloud Intelligence DashboardsWell-Architected Labs: Cost VisualizationAWS Cost Anomaly Detection Alert with Slack\n\nKnowledge Area: AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What tool should the media company use to store large amounts of data that will be used for their applications?\",\n\"answer_choices\": [{\"answer\": \"AWS Cost Explorer\",\"is_correct\": \"false\",\"explanation\": \"AWS Cost Explorer is used to monitor and analyze cost patterns, not store data.\"},{\"answer\": \"AWS Budgets\",\"is_correct\": \"false\",\"explanation\": \"AWS Budgets is used to set budgets and monitor usage against those budgets, not store data.\"},{\"answer\": \"Amazon Athena with Amazon QuickSight\",\"is_correct\": \"false\",\"explanation\": \"Amazon Athena with Amazon QuickSight is used to perform custom and deep analysis, not store data.\"},{\"answer\": \"Amazon S3\",\"is_correct\": \"true\",\"explanation\": \"Amazon S3 is an object storage service that is used to store large amounts of data for applications.\"}]}", "docs": ["('COST03-BP04 Configure billing and cost management tools - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_monitor_usage_config_tools.html')", "('COST01-BP06 Monitor cost proactively - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_cloud_financial_management_proactive_process.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Knowledge of", "item": "Load balancing concepts (for example, Application Load Balancer)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* Distribute traffic across multiple resources or services to allow your workload to take advantage of the elasticity that the cloud provides. You can also use load balancing for offloading encryption termination to improve performance and to manage and route traffic effectively.When implementing a scale-out architecture where you want to use multiple instances for service content, you can use load balancers inside your Amazon VPC. AWS provides multiple models for your applications in the ELB service. Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers.Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. It is capable of handling millions of requests per second while maintaining ultra-low latencies, and it is optimized to handle sudden and volatile traffic patterns.Elastic Load Balancing provides integrated certificate management and SSL/TLS decryption, allowing you the flexibility to centrally manage the SSL settings of the load balancer and offload CPU intensive work from your workload.Common anti-patterns:You route all internet traffic through existing load balancers.You use generic TCP load balancing and making each compute node handle SSL encryption.Benefits of establishing this best practice: A load balancer handles the varying load of your application traffic in a single Availability Zone, or across multiple Availability Zones. Load balancers feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant.Level of risk exposed if this best practice is not established: HighUse the appropriate load balancer for your workload: Select the appropriate load balancer for your workload. If you must load balance HTTP requests, we recommend Application Load Balancer. For network and transport protocols (layer 4 \u2013 TCP, UDP) load balancing, and for extreme performance and low latency applications, we recommend Network Load Balancer. Application Load Balancers support HTTPS and Network Load Balancers support TLS encryption offloading.Enable offload of HTTPS or TLS encryption: Elastic Load Balancing includes integrated certificate management, user-authentication, and SSL/TLS decryption. It provides the flexibility to centrally manage TLS settings and offload CPU intensive workloads from your applications. Encrypt all HTTPS traffic as part of your load balancer deployment.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies.Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public endpoints. You might also choose to evaluate AWS Marketplace software appliances for load balancing and proxying.Consumers of the service your workload provides, whether they are end-users or other services, make requests on these service endpoints. Several AWS resources are available to enable you to provide highly available endpoints.Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.Amazon Route 53 is a scalable and highly available Domain Name System (DNS) service that connects user requests to infrastructure running in AWS such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets\u2013and can also be used to route users to infrastructure outside of AWS.AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network.Distributed Denial of Service (DDoS) attacks risk shutting out legitimate traffic and lowering availability for your users. AWS Shield provides automatic protection against these attacks at no extra cost for AWS service endpoints on your workload. You can augment these features with virtual appliances from APN Partners and the AWS Marketplace to meet your needs.Common anti-patterns:Using public internet addresses on instances or containers and managing the connectivity to them via DNS.Using Internet Protocol addresses instead of domain names for locating services.Providing content (web pages, static assets, media files) to a large geographic area and not using a content delivery network.Benefits of establishing this best practice: By implementing highly available services in your workload, you know that your workload will be available to your users.Level of risk exposed if this best practice is not established: HighEnsure that you have highly available connectivity for users of the workload Amazon Route 53, AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, and Elastic Load Balancing (ELB) all provide highly available public facing endpoints. You may also choose to evaluate AWS Marketplace software appliances for load-balancing and proxying.Ensure that you have a highly available connection to your users.Ensure that you are using a highly available DNS to manage the domain names of your application endpoints.If your users access your application via the internet, use service API operations to confirm the correct usage of Internet Gateways. Also confirm that the route tables entries for the subnets hosting your application endpoints are correct.DescribeInternetGatewaysDescribeRouteTablesEnsure that you are using a highly available reverse proxy or load balancer in front of your application.If your users access your application via your on-premises environment, ensure that your connectivity between AWS and your on-premises environment is highly available.Use Route 53 to manage your domain names.What is Amazon Route 53?Use a third-party DNS provider that meets your requirements.Use Elastic Load Balancing.What is Elastic Load Balancing?Use an AWS Marketplace appliance that meets your requirements.Related documents:APN Partner: partners that can help plan your networkingAWS Direct Connect Resiliency RecommendationsAWS Marketplace for Network InfrastructureAmazon Virtual Private Cloud Connectivity Options WhitepaperMultiple data center HA network connectivityUsing the Direct Connect Resiliency Toolkit to get startedVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is AWS Global Accelerator?What Is Amazon VPC?What Is a Transit Gateway?What is Amazon CloudFront?What is Amazon Route 53?What is Elastic Load Balancing?Working with Direct Connect GatewaysRelated videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n\nKnowledge Area: Load balancing concepts (for example, Application Load Balancer)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"When implementing a scale-out architecture to distribute traffic across multiple resources or services, which load balancer is best suited for load balancing of HTTP and HTTPS traffic?\", \n\"answer_choices\": [{\"answer\": \"Amazon Route 53\", \"is_correct\": \"false\", \"explanation\": \"Amazon Route 53 is a DNS service that connects user requests to infrastructure running in AWS. It is not a load balancer.\"},{\"answer\": \"AWS Global Accelerator\", \"is_correct\": \"false\", \"explanation\": \"AWS Global Accelerator is a network layer service that you can use to direct traffic to optimal endpoints over the AWS global network. It is not a load balancer.\"},{\"answer\": \"Application Load Balancer\", \"is_correct\": \"true\", \"explanation\": \"Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers.\"},{\"answer\": \"Network Load Balancer\", \"is_correct\": \"false\", \"explanation\": \"Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. It is not the best choice for HTTP and HTTPS traffic.\"}]}", "docs": ["('PERF05-BP04 Leverage load-balancing and encryption offloading - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_encryption_offload.html')", "('REL02-BP01 Use highly available network connectivity for your workload public endpoints - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_ha_conn_users.html')"]}, {"scenario": "A gaming company wants to use AWS to host and scale their multiplayer game servers.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Knowledge of", "item": "NAT gateways (for example, NAT instance costs compared with NAT gateway costs)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA gaming company wants to use AWS to host and scale their multiplayer game servers.`\"\n\nContext:\n\n* If more than two network address spaces (for example, VPCs and on-premises networks) are connected via VPC peering, AWS Direct Connect, or VPN, then use a hub-and-spoke model, like that provided by AWS Transit Gateway.If you have only two such networks, you can simply connect them to each other, but as the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing the routing of traffic across your multiple networks.Figure 1: Without AWS Transit Gateway: You need to peer each Amazon VPC to each other and to each onsite location using a VPN connection, which can become complex as it scales.Figure 2: With AWS Transit Gateway: You simply connect each Amazon VPC or VPN to the AWS Transit Gateway and it routes traffic to and from each VPC or VPN.Common anti-patterns:Using VPC peering to connect more than two VPCs.Establishing multiple BGP sessions for each VPC to establish connectivity that spans Virtual Private Clouds (VPCs) spread across multiple AWS Regions.Benefits of establishing this best practice: As the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing routing of traffic among your multiple networks.Level of risk exposed if this best practice is not established: MediumPrefer hub-and-spoke topologies over many-to-many mesh. If more than two network address spaces (VPCs, on-premises networks) are connected via VPC peering, AWS Direct Connect, or VPN, then use a hub-and-spoke model like that provided by AWS Transit Gateway.For only two such networks, you can simply connect them to each other, but as the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing routing of traffic across your multiple networks.What Is a Transit Gateway?Related documents:APN Partner: partners that can help plan your networkingAWS Marketplace for Network InfrastructureMultiple data center HA network connectivityVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is Amazon VPC?What Is a Transit Gateway?Related videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n* Using the appropriate instances and resources for your workload is key to cost savings. For example, a reporting process might take five hours to run on a smaller server but one hour to run on a larger server that is twice as expensive. Both servers give you the same outcome, but the smaller server incurs more cost over time.A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. For example, rather than maintaining servers to deliver email, you can use a service that charges on a per-message basis.AWS offers a variety of flexible and cost-effective pricing options to acquire instances from Amazon EC2 and other services in a way that best fits your needs. On-DemandInstances allow you to pay for compute capacity by the hour, with no minimum commitments required. Savings Plans and Reserved Instances offer savings of up to 75% o\ufb00 On-Demand pricing. With Spot Instances, you can leverage unused Amazon EC2 capacity and offer savings of up to 90% o\ufb00 On-Demand pricing. Spot Instances are appropriate where the system can tolerate using a \ufb02eet of servers where individual servers can come and go dynamically, such as stateless web servers, batch processing, or when using HPC and big data.Appropriate service selection can also reduce usage and costs; such as CloudFront to minimize data transfer, or completely eliminate costs, such as utilizing Amazon Aurora on RDS to remove expensive database licensing costs.The following questions focus on these considerations for cost optimization.By factoring in cost during service selection, and using tools such as Cost Explorer and AWS Trusted Advisor to regularly review your AWS usage, you can actively monitor your utilization and adjust your deployments accordingly.COST 5: How do you evaluate cost when you select services?Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or remove much of your administrative and operational overhead, freeing you to work on applications and business-related activities.COST 6: How do you meet cost targets when you select resource type, size and number?Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number.COST 7: How do you use pricing models to reduce cost?Use the pricing model that is most appropriate for your resources to minimize expense.COST 8: How do you plan for data transfer charges?Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time.\n\nKnowledge Area: NAT gateways (for example, NAT instance costs compared with NAT gateway costs)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the most cost-effective way to connect multiple VPCs using a hub-and-spoke model?\",\n\"answer_choices\": [{\"answer\": \"Using VPC peering to connect more than two VPCs\",\"is_correct\": \"false\",\"explanation\": \"VPC peering is not cost-effective for connecting more than two VPCs.\"},{\"answer\": \"Establishing multiple BGP sessions for each VPC\",\"is_correct\": \"false\",\"explanation\": \"Establishing multiple BGP sessions for each VPC is not cost-effective.\"},{\"answer\": \"Using AWS Transit Gateway\",\"is_correct\": \"true\",\"explanation\": \"Using AWS Transit Gateway is the most cost-effective way to connect multiple VPCs using a hub-and-spoke model.\"},{\"answer\": \"Using Amazon VPC Endpoints\",\"is_correct\": \"false\",\"explanation\": \"Amazon VPC Endpoints are not cost-effective for connecting multiple VPCs using a hub-and-spoke model.\"}]}", "docs": ["('REL02-BP04 Prefer hub-and-spoke topologies over many-to-many mesh - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_prefer_hub_and_spoke.html')", "('Cost-effective resources - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost-cereso.html')"]}, {"scenario": "A media company wants to store large amounts of data that will be used for thier applications", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Knowledge of", "item": "Network connectivity (for example, private lines, dedicated lines, VPNs)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA media company wants to store large amounts of data that will be used for thier applications`\"\n\nContext:\n\n* Any workload that has some form of network connectivity, whether it\u2019s the internet or a private network, requires multiple layers of defense to help protect from external and internal network-based threats.Best practices\n* When a common network is required to connect on-premises and cloud resources in AWS, ensure that you have adequate bandwidth to meet your performance requirements. Estimate the bandwidth and latency requirements for your hybrid workload. These numbers will drive the sizing requirements for AWS Direct Connect or your VPN endpoints.Desired outcome: When deploying a workload that will need hybrid network connectivity, you have multiple configuration options for connectivity, such as managed and non-managed VPNs or Direct Connect. Select the appropriate connection type for each workload while ensuring you have adequate bandwidth and encryption requirements between your location and the cloud.Common anti-patterns:You only evaluate VPN solutions for your network encryption requirements.You don\u2019t evaluate backup or parallel connectivity options.You use default configurations for routers, tunnels, and BGP sessions.You fail to understand or identify all workload requirements (encryption, protocol, bandwidth and traffic needs).Benefits of establishing this best practice: Selecting and configuring appropriately sized hybrid network solutions will increase the reliability of your workload and maximize performance opportunities. By identifying workload requirements, planning ahead, and evaluating hybrid solutions you will minimize expensive physical network changes and operational overhead while increasing your time to market.Level of risk exposed if this best practice is not established: HighDevelop a hybrid networking architecture based on your bandwidth requirements: Estimate the bandwidth and latency requirements of your hybrid applications. Based on your bandwidth requirements, a single VPN or Direct Connect connection might not be enough, and you must architect a hybrid setup to enable traffic load balancing across multiple connections. Direct connect may be required which offers more predictable and consistent performance due to its private network connectivity. It is great for production workloads that require consistent latency and almost zero jitter.AWS Direct Connect provides dedicated connectivity to the AWS environment, from 50 Mbps up to 10 Gbps. This gives you managed and controlled latency and provisioned bandwidth so your workload can connect easily and in a performant way to other environments. Using one of the AWS Direct Connect partners, you can have end-to-end connectivity from multiple environments, thus providing an extended network with consistent performance.The AWS Site-to-Site VPN is a managed VPN service for VPCs. When a VPN connection is created, AWS provides tunnels to two different VPN endpoints. With AWS Transit Gateway, you can simplify the connectivity between multiple VPCs and also connect to any VPC attached to AWS Transit Gateway with a single VPN connection. AWS Transit Gateway also enables you to scale beyond the 1.25Gbps IPsec VPN throughput limit by enabling equal cost multi-path (ECMP) routing support over multiple VPN tunnels.Level of effort for the implementation plan: There is a high level of effort to evaluate workload needs for hybrid networks and to implement hybrid networking solutions.Related documents:Network Load BalancerNetworking Products with AWSTransit GatewayTransitioning to latency-based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsSite-to-Site VPNBuilding a Scalable and Secure Multi-VPC AWS Network InfrastructureDirect ConnectClient VPNRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)AWS Global AcceleratorDirect ConnectTransit Gateway ConnectVPN SolutionsSecurity with VPN SolutionsRelated examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* Workloads often exist in multiple environments. These include multiple cloud environments (both publicly accessible and private) and possibly your existing data center infrastructure. Plans must include network considerations such as intra- and inter-system connectivity, public IP address management, private IP address management, and domain name resolution.Best practices\n* The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.Best practices\n\nKnowledge Area: Network connectivity (for example, private lines, dedicated lines, VPNs)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which option is the best network solution for a workload based on the scenario, context and knowledge area provided?\",\"answer_choices\": [{\"answer\": \"AWS Site-to-Site VPN\",\"is_correct\": \"True\",\"explanation\": \"The AWS Site-to-Site VPN is a managed VPN service for VPCs which provides tunnels to two different VPN endpoints, managed and controlled latency, and provisioned bandwidth so your workload can connect easily and in a performant way to other environments.\"},{\"answer\": \"AWS Direct Connect\",\"is_correct\": \"False\",\"explanation\": \"AWS Direct Connect provides dedicated connectivity to the AWS environment, but it may not be the most optimal solution depending on the workload requirements.\"},{\"answer\": \"VPC Endpoints\",\"is_correct\": \"False\",\"explanation\": \"VPC Endpoints provides private connectivity to services hosted in AWS, but it is not the optimal network solution for the workload based on the scenario, context and knowledge area provided.\"},{\"answer\": \"A Private Network\",\"is_correct\": \"False\",\"explanation\": \"A Private Network is a type of networking solution that can be used, but it is not the optimal network solution for the workload based on the scenario, context and knowledge area provided.\"}]}", "docs": ["('SEC 5\\xa0 How do you protect your network resources? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-05.html')", "('PERF05-BP03 Choose appropriately sized dedicated connectivity or VPN for hybrid workloads - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_hybrid.html')", "('REL 2\\xa0 How do you plan your network topology? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-02.html')", "('PERF 5\\xa0 How do you configure your networking solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-05.html')"]}, {"scenario": "A retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Knowledge of", "item": "Network routing, topology, and peering (for example, AWS Transit Gateway, VPC peering)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons.`\"\n\nContext:\n\n* If more than two network address spaces (for example, VPCs and on-premises networks) are connected via VPC peering, AWS Direct Connect, or VPN, then use a hub-and-spoke model, like that provided by AWS Transit Gateway.If you have only two such networks, you can simply connect them to each other, but as the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing the routing of traffic across your multiple networks.Figure 1: Without AWS Transit Gateway: You need to peer each Amazon VPC to each other and to each onsite location using a VPN connection, which can become complex as it scales.Figure 2: With AWS Transit Gateway: You simply connect each Amazon VPC or VPN to the AWS Transit Gateway and it routes traffic to and from each VPC or VPN.Common anti-patterns:Using VPC peering to connect more than two VPCs.Establishing multiple BGP sessions for each VPC to establish connectivity that spans Virtual Private Clouds (VPCs) spread across multiple AWS Regions.Benefits of establishing this best practice: As the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing routing of traffic among your multiple networks.Level of risk exposed if this best practice is not established: MediumPrefer hub-and-spoke topologies over many-to-many mesh. If more than two network address spaces (VPCs, on-premises networks) are connected via VPC peering, AWS Direct Connect, or VPN, then use a hub-and-spoke model like that provided by AWS Transit Gateway.For only two such networks, you can simply connect them to each other, but as the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing routing of traffic across your multiple networks.What Is a Transit Gateway?Related documents:APN Partner: partners that can help plan your networkingAWS Marketplace for Network InfrastructureMultiple data center HA network connectivityVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is Amazon VPC?What Is a Transit Gateway?Related videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n* When architecting your network topology, you should examine the connectivity requirements of each component. For example, if a component requires internet accessibility (inbound and outbound), connectivity to VPCs, edge services, and external data centers.A VPC allows you to define your network topology that spans an AWS Region with a private IPv4 address range that you set, or an IPv6 address range AWS selects. You should apply multiple controls with a defense in depth approach for both inbound and outbound traffic, including the use of security groups (stateful inspection firewall), Network ACLs, subnets, and route tables. Within a VPC, you can create subnets in an Availability Zone. Each subnet can have an associated route table that defines routing rules for managing the paths that traffic takes within the subnet. You can define an internet routable subnet by having a route that goes to an internet or NAT gateway attached to the VPC, or through another VPC.When an instance, Amazon Relational Database Service(Amazon RDS) database, or other service is launched within a VPC, it has its own security group per network interface. This firewall is outside the operating system layer and can be used to define rules for allowed inbound and outbound traffic. You can also define relationships between security groups. For example, instances within a database tier security group only accept traffic from instances within the application tier, by reference to the security groups applied to the instances involved. Unless you are using non-TCP protocols, it shouldn\u2019t be necessary to have an Amazon Elastic Compute Cloud(Amazon EC2) instance directly accessible by the internet (even with ports restricted by security groups) without a load balancer, or CloudFront. This helps protect it from unintended access through an operating system or application issue. A subnet can also have a network ACL attached to it, which acts as a stateless firewall. You should configure the network ACL to narrow the scope of traffic allowed between layers, note that you need to define both inbound and outbound rules.Some AWS services require components to access the internet for making API calls, where AWS API endpoints are located. Other AWS services use VPC endpoints within your Amazon VPCs. Many AWS services, including Amazon S3 and Amazon DynamoDB, support VPC endpoints, and this technology has been generalized in AWS PrivateLink. We recommend you use this approach to access AWS services, third-party services, and your own services hosted in other VPCs securely. All network traffic on AWS PrivateLink stays on the global AWS backbone and never traverses the internet. Connectivity can only be initiated by the consumer of the service, and not by the provider of the service. Using AWS PrivateLink for external service access allows you to create air-gapped VPCs with no internet access and helps protect your VPCs from external threat vectors. Third-party services can use AWS PrivateLink to allow their customers to connect to the services from their VPCs over private IP addresses. For VPC assets that need to make outbound connections to the internet, these can be made outbound only (one-way) through an AWS managed NAT gateway, outbound only internet gateway, or web proxies that you create and manage.Level of risk exposed if this best practice is not established: HighControl network traffic in a VPC: Implement VPC best practices to control traffic.Amazon VPC securityVPC endpointsAmazon VPC security groupNetwork ACLsControl traffic at the edge: Implement edge services, such as Amazon CloudFront, to provide an additional layer of protection and other features.Amazon CloudFront use casesAWS Global AcceleratorAWS Web Application Firewall (AWS WAF)Amazon Route 53Amazon VPC Ingress RoutingControl private network traffic: Implement services that protect your private traffic for your workload.Amazon VPC PeeringAmazon VPC Endpoint Services (AWS PrivateLink)Amazon VPC Transit GatewayAWS Direct ConnectAWS Site-to-Site VPNAWS Client VPNAmazon S3 Access PointsRelated documents:AWS Firewall ManagerAmazon InspectorGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n\nKnowledge Area: Network routing, topology, and peering (for example, AWS Transit Gateway, VPC peering)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"The retail company wants to leverage AWS to support their e-commerce platform during peak shopping seasons. What should they use to create a hub-and-spoke model when connecting more than two network address spaces (VPCs and on-premises networks)?\",\"answer_choices\": [{\"answer\": \"VPC peering\",\"is_correct\": \"False\",\"explanation\": \"VPC peering should not be used to connect more than two networks. It will become too complex to maintain as the number of networks grows.\"},{\"answer\": \"AWS Direct Connect\",\"is_correct\": \"False\",\"explanation\": \"AWS Direct Connect should not be used to connect more than two networks. It will become too complex to maintain as the number of networks grows.\"},{\"answer\": \"VPN\",\"is_correct\": \"False\",\"explanation\": \"VPN should not be used to connect more than two networks. It will become too complex to maintain as the number of networks grows.\"},{\"answer\": \"AWS Transit Gateway\",\"is_correct\": \"True\",\"explanation\": \"AWS Transit Gateway is the best solution to create a hub-and-spoke model when connecting more than two network address spaces (VPCs and on-premises networks). It provides an easy to maintain hub-and-spoke model, allowing routing of traffic across your multiple networks.\"}]}", "docs": ["('REL02-BP04 Prefer hub-and-spoke topologies over many-to-many mesh - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_prefer_hub_and_spoke.html')", "('SEC05-BP02 Control traffic at all layers - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_layered.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Knowledge of", "item": "Network services with appropriate use cases (for example, DNS)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* For high availability, always (when possible) deploy your workload components to multiple Availability Zones (AZs), as shown in Figure 10. For workloads with extreme resilience requirements, carefully evaluate the options for a multi-Region architecture.\n* Any workload that has some form of network connectivity, whether it\u2019s the internet or a private network, requires multiple layers of defense to help protect from external and internal network-based threats.Best practices\n* Workloads often exist in multiple environments. These include multiple cloud environments (both publicly accessible and private) and possibly your existing data center infrastructure. Plans must include network considerations such as intra- and inter-system connectivity, public IP address management, private IP address management, and domain name resolution.Best practices\n* The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.Best practices\n* Analyze and understand how network-related decisions impact workload performance. The network is responsible for the connectivity between application components, cloud services, edge networks and on-premises data and therefor it can highly impact workload performance. In addition to workload performance, user experience is also impacted by network latency, bandwidth, protocols, location, network congestion, jitter, throughput, and routing rules.Desired outcome: Have a documented list of networking requirements from the workload including latency, packet size, routing rules, protocols, and supporting traffic patterns. Review the available networking solutions and identify which service meets your workload networking characteristics. Cloud-based networks can be quickly rebuilt, so evolving your network architecture over time is necessary to improve performance efficiency.Common anti-patterns:All traffic flows through your existing data centers.You overbuild Direct Connect sessions without understanding the actual usage requirements.You don\u2019t consider workload characteristics and encryption overhead when defining your networking solutions.You use on-premises concepts and strategies for networking solutions in the cloud.Benefits of establishing this best practice: Understanding how networking impacts workload performance will help you identify potential bottlenecks, improve user experience, increase reliability, and lower operational maintenance as the workload changes.Level of risk exposed if this best practice is not established: HighIdentify important network performance metrics of your workload and capture its networking characteristics. Define and document requirements as part of a data-driven approach, using benchmarking or load testing. Use this data to identify where your network solution is constrained, and examine configuration options that could improve the workload. Understand the cloud-native networking features and options available and how they can impact your workload performance based on the requirements. Each networking feature has advantages and disadvantages and can be configured to meet your workload characteristics and scale based on your needs.Implementation steps:Define and document networking performance requirements:Include metrics such as network latency, bandwidth, protocols, locations, traffic patterns (spikes and frequency), throughput, encryption, inspection, and routing rulesCapture your foundational networking characteristics:VPC Flow LogsAWS Transit Gateway metricsAWS PrivateLink metricsCapture your application networking characteristics:Elastic Network AdaptorAWS App Mesh metricsAmazon API Gateway metricsCapture your edge networking characteristics:Amazon CloudFront metricsAmazon Route 53 metricsAWS Global Accelerator metricsCapture your hybrid networking characteristics:Direct Connect metricsAWS Site-to-Site VPN metricsAWS Client VPN metricsAWS Cloud WAN metricsCapture your security networking characteristics:AWS Shield, WAF, and Network Firewall metricsCapture end-to-end performance metrics with tracing tools:AWS X-RayAmazon CloudWatch RUMBenchmark and test network performance:Benchmark network throughput: Some factors that can affect EC2 network performance when the instances are in the same VPC. Measure the network bandwidth between EC2 Linux instances in the same VPC.Perform load tests to experiment with networking solutions and optionsLevel of effort for the implementation plan: There is a medium level of effort to document workload networking requirements, options, and available solutions.Related documents:Application Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to latency-based routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Improve Global Network Performance for ApplicationsEC2 Instances and Performance Optimization Best PracticesOptimizing Network Performance for Amazon EC2 InstancesNetworking best practices and tips with the Well-Architected FrameworkAWS networking best practices in large-scale migrationsRelated examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n\nKnowledge Area: Network services with appropriate use cases (for example, DNS)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which AWS service should be used to deploy a multi-region disaster recovery plan for a company's critical systems?\",\n \"answer_choices\": [\n     {\"answer\": \"Amazon EC2 Placement Groups\", \"is_correct\": false, \"explanation\": \"Amazon EC2 Placement Groups are used to optimize the placement of instances within an Availability Zone to reduce latency, but are not useful for creating a multi-region disaster recovery plan.\"}, \n     {\"answer\": \"AWS Site-to-Site VPN\", \"is_correct\": false, \"explanation\": \"AWS Site-to-Site VPN is used to connect on-premises networks to AWS VPCs, but is not a suitable service for implementing a multi-region disaster recovery plan.\"},\n     {\"answer\": \"AWS Transit Gateway\", \"is_correct\": true, \"explanation\": \"AWS Transit Gateway is the service to use to deploy a multi-region disaster recovery plan. It provides a single gateway to simplify the connectivity between multiple VPCs and on-premises networks as well as across AWS Regions.\"},\n     {\"answer\": \"Amazon CloudFront\", \"is_correct\": false, \"explanation\": \"Amazon CloudFront is a web service for content delivery, but is not suitable for implementing a multi-region disaster recovery plan.\"}\n ]}", "docs": ["('REL10-BP02 Select the appropriate locations for your multi-location deployment - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_fault_isolation_select_location.html')", "('SEC 5\\xa0 How do you protect your network resources? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-05.html')", "('REL 2\\xa0 How do you plan your network topology? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel-02.html')", "('PERF 5\\xa0 How do you configure your networking solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-05.html')", "('PERF05-BP01 Understand how networking impacts performance - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_understand_impact.html')"]}, {"scenario": "A software development team wants to build and deploy a microservices-based application on AWS.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Skills in", "item": "Configuring appropriate NAT gateway types for a network (for example, a single shared NAT gateway compared with NAT gateways for each Availability Zone)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA software development team wants to build and deploy a microservices-based application on AWS.`\"\n\nContext:\n\n* If more than two network address spaces (for example, VPCs and on-premises networks) are connected via VPC peering, AWS Direct Connect, or VPN, then use a hub-and-spoke model, like that provided by AWS Transit Gateway.If you have only two such networks, you can simply connect them to each other, but as the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing the routing of traffic across your multiple networks.Figure 1: Without AWS Transit Gateway: You need to peer each Amazon VPC to each other and to each onsite location using a VPN connection, which can become complex as it scales.Figure 2: With AWS Transit Gateway: You simply connect each Amazon VPC or VPN to the AWS Transit Gateway and it routes traffic to and from each VPC or VPN.Common anti-patterns:Using VPC peering to connect more than two VPCs.Establishing multiple BGP sessions for each VPC to establish connectivity that spans Virtual Private Clouds (VPCs) spread across multiple AWS Regions.Benefits of establishing this best practice: As the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing routing of traffic among your multiple networks.Level of risk exposed if this best practice is not established: MediumPrefer hub-and-spoke topologies over many-to-many mesh. If more than two network address spaces (VPCs, on-premises networks) are connected via VPC peering, AWS Direct Connect, or VPN, then use a hub-and-spoke model like that provided by AWS Transit Gateway.For only two such networks, you can simply connect them to each other, but as the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing routing of traffic across your multiple networks.What Is a Transit Gateway?Related documents:APN Partner: partners that can help plan your networkingAWS Marketplace for Network InfrastructureMultiple data center HA network connectivityVPC Endpoints and VPC Endpoint Services (AWS PrivateLink)What Is Amazon VPC?What Is a Transit Gateway?Related videos:AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)\n* When architecting your network topology, you should examine the connectivity requirements of each component. For example, if a component requires internet accessibility (inbound and outbound), connectivity to VPCs, edge services, and external data centers.A VPC allows you to define your network topology that spans an AWS Region with a private IPv4 address range that you set, or an IPv6 address range AWS selects. You should apply multiple controls with a defense in depth approach for both inbound and outbound traffic, including the use of security groups (stateful inspection firewall), Network ACLs, subnets, and route tables. Within a VPC, you can create subnets in an Availability Zone. Each subnet can have an associated route table that defines routing rules for managing the paths that traffic takes within the subnet. You can define an internet routable subnet by having a route that goes to an internet or NAT gateway attached to the VPC, or through another VPC.When an instance, Amazon Relational Database Service(Amazon RDS) database, or other service is launched within a VPC, it has its own security group per network interface. This firewall is outside the operating system layer and can be used to define rules for allowed inbound and outbound traffic. You can also define relationships between security groups. For example, instances within a database tier security group only accept traffic from instances within the application tier, by reference to the security groups applied to the instances involved. Unless you are using non-TCP protocols, it shouldn\u2019t be necessary to have an Amazon Elastic Compute Cloud(Amazon EC2) instance directly accessible by the internet (even with ports restricted by security groups) without a load balancer, or CloudFront. This helps protect it from unintended access through an operating system or application issue. A subnet can also have a network ACL attached to it, which acts as a stateless firewall. You should configure the network ACL to narrow the scope of traffic allowed between layers, note that you need to define both inbound and outbound rules.Some AWS services require components to access the internet for making API calls, where AWS API endpoints are located. Other AWS services use VPC endpoints within your Amazon VPCs. Many AWS services, including Amazon S3 and Amazon DynamoDB, support VPC endpoints, and this technology has been generalized in AWS PrivateLink. We recommend you use this approach to access AWS services, third-party services, and your own services hosted in other VPCs securely. All network traffic on AWS PrivateLink stays on the global AWS backbone and never traverses the internet. Connectivity can only be initiated by the consumer of the service, and not by the provider of the service. Using AWS PrivateLink for external service access allows you to create air-gapped VPCs with no internet access and helps protect your VPCs from external threat vectors. Third-party services can use AWS PrivateLink to allow their customers to connect to the services from their VPCs over private IP addresses. For VPC assets that need to make outbound connections to the internet, these can be made outbound only (one-way) through an AWS managed NAT gateway, outbound only internet gateway, or web proxies that you create and manage.Level of risk exposed if this best practice is not established: HighControl network traffic in a VPC: Implement VPC best practices to control traffic.Amazon VPC securityVPC endpointsAmazon VPC security groupNetwork ACLsControl traffic at the edge: Implement edge services, such as Amazon CloudFront, to provide an additional layer of protection and other features.Amazon CloudFront use casesAWS Global AcceleratorAWS Web Application Firewall (AWS WAF)Amazon Route 53Amazon VPC Ingress RoutingControl private network traffic: Implement services that protect your private traffic for your workload.Amazon VPC PeeringAmazon VPC Endpoint Services (AWS PrivateLink)Amazon VPC Transit GatewayAWS Direct ConnectAWS Site-to-Site VPNAWS Client VPNAmazon S3 Access PointsRelated documents:AWS Firewall ManagerAmazon InspectorGetting started with AWS WAFRelated videos:AWS Transit Gateway reference architectures for many VPCsApplication Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS ShieldRelated examples:Lab: Automated Deployment of VPC\n\nKnowledge Area: Configuring appropriate NAT gateway types for a network (for example, a single shared NAT gateway compared with NAT gateways for each Availability Zone)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"In a scenario where more than two network address spaces need to be connected via VPC peering, AWS Direct Connect, or VPN, what is the best practice for setting up a hub-and-spoke model to manage the routing of traffic across multiple networks?\", \"answer_choices\": [{\"answer\": \"Create multiple BGP connections for each VPC to establish connectivity across multiple AWS Regions\",\"is_correct\": \"false\",\"explanation\": \"This is an anti-pattern and will increase the complexity of the connections.\"},{\"answer\": \"Use VPC peering to connect more than two VPCs\",\"is_correct\": \"false\",\"explanation\": \"This is an anti-pattern and will increase the complexity of the connections.\"},{\"answer\": \"Use AWS Transit Gateway to provide an easy to maintain hub-and-spoke model\",\"is_correct\": \"true\",\"explanation\": \"AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing routing of traffic across your multiple networks.\"},{\"answer\": \"Connect two networks directly to each other\",\"is_correct\": \"false\",\"explanation\": \"This is only appropriate when there are only two networks, but it becomes untenable when there are more than two networks.\"}]}", "docs": ["('REL02-BP04 Prefer hub-and-spoke topologies over many-to-many mesh - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_planning_network_topology_prefer_hub_and_spoke.html')", "('SEC05-BP02 Control traffic at all layers - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_network_protection_layered.html')"]}, {"scenario": "A company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Skills in", "item": "Configuring appropriate network connections (for example, Direct Connect compared with VPN compared with internet)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run batch processing jobs on AWS, but wants to minimize costs when the jobs are not running.`\"\n\nContext:\n\n* When a common network is required to connect on-premises and cloud resources in AWS, ensure that you have adequate bandwidth to meet your performance requirements. Estimate the bandwidth and latency requirements for your hybrid workload. These numbers will drive the sizing requirements for AWS Direct Connect or your VPN endpoints.Desired outcome: When deploying a workload that will need hybrid network connectivity, you have multiple configuration options for connectivity, such as managed and non-managed VPNs or Direct Connect. Select the appropriate connection type for each workload while ensuring you have adequate bandwidth and encryption requirements between your location and the cloud.Common anti-patterns:You only evaluate VPN solutions for your network encryption requirements.You don\u2019t evaluate backup or parallel connectivity options.You use default configurations for routers, tunnels, and BGP sessions.You fail to understand or identify all workload requirements (encryption, protocol, bandwidth and traffic needs).Benefits of establishing this best practice: Selecting and configuring appropriately sized hybrid network solutions will increase the reliability of your workload and maximize performance opportunities. By identifying workload requirements, planning ahead, and evaluating hybrid solutions you will minimize expensive physical network changes and operational overhead while increasing your time to market.Level of risk exposed if this best practice is not established: HighDevelop a hybrid networking architecture based on your bandwidth requirements: Estimate the bandwidth and latency requirements of your hybrid applications. Based on your bandwidth requirements, a single VPN or Direct Connect connection might not be enough, and you must architect a hybrid setup to enable traffic load balancing across multiple connections. Direct connect may be required which offers more predictable and consistent performance due to its private network connectivity. It is great for production workloads that require consistent latency and almost zero jitter.AWS Direct Connect provides dedicated connectivity to the AWS environment, from 50 Mbps up to 10 Gbps. This gives you managed and controlled latency and provisioned bandwidth so your workload can connect easily and in a performant way to other environments. Using one of the AWS Direct Connect partners, you can have end-to-end connectivity from multiple environments, thus providing an extended network with consistent performance.The AWS Site-to-Site VPN is a managed VPN service for VPCs. When a VPN connection is created, AWS provides tunnels to two different VPN endpoints. With AWS Transit Gateway, you can simplify the connectivity between multiple VPCs and also connect to any VPC attached to AWS Transit Gateway with a single VPN connection. AWS Transit Gateway also enables you to scale beyond the 1.25Gbps IPsec VPN throughput limit by enabling equal cost multi-path (ECMP) routing support over multiple VPN tunnels.Level of effort for the implementation plan: There is a high level of effort to evaluate workload needs for hybrid networks and to implement hybrid networking solutions.Related documents:Network Load BalancerNetworking Products with AWSTransit GatewayTransitioning to latency-based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsSite-to-Site VPNBuilding a Scalable and Secure Multi-VPC AWS Network InfrastructureDirect ConnectClient VPNRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)AWS Global AcceleratorDirect ConnectTransit Gateway ConnectVPN SolutionsSecurity with VPN SolutionsRelated examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n\nKnowledge Area: Configuring appropriate network connections (for example, Direct Connect compared with VPN compared with internet)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which network connection type should be used for a workload that requires consistent latency and almost zero jitter?\",\n \"answer_choices\": [{\"answer\": \"AWS Site-to-Site VPN\", \"is_correct\": \"false\",\"explanation\": \"AWS Site-to-Site VPN provides managed VPN service for VPCs and is not suitable for workloads that require consistent latency and almost zero jitter.\"},{\"answer\": \"AWS Direct Connect\", \"is_correct\": \"true\",\"explanation\": \"AWS Direct Connect provides dedicated connectivity to the AWS environment and is suitable for production workloads that require consistent latency and almost zero jitter.\"},{\"answer\": \"AWS Client VPN\", \"is_correct\": \"false\",\"explanation\": \"AWS Client VPN is used for client applications for secure client connectivity and is not suitable for workloads that require consistent latency and almost zero jitter.\"},{\"answer\": \"AWS Transit Gateway\", \"is_correct\": \"false\",\"explanation\": \"AWS Transit Gateway is used for simplifying connectivity between multiple VPCs and is not suitable for workloads that require consistent latency and almost zero jitter.\"}]}", "docs": ["('PERF05-BP03 Choose appropriately sized dedicated connectivity or VPN for hybrid workloads - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_hybrid.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Skills in", "item": "Configuring appropriate network routes to minimize network transfer costs (for example, Region to Region, Availability Zone to Availability Zone, private to public, Global Accelerator, VPC endpoints)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* Use the cloud location options available to reduce network latency or improve throughput. Use AWS Regions, Availability Zones, placement groups, and edge locations such as AWS Outposts, AWS Local Zones, and AWS Wavelength, to reduce network latency or improve throughput.The AWS Cloud infrastructure is built around Regions and Availability Zones. A Region is a physical location in the world having multiple Availability Zones.Availability Zones consist of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. These Availability Zones offer you the ability to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data centerChoose the appropriate Region or Regions for your deployment based on the following key elements:Where your users are located: Choosing a Region close to your workload\u2019s users ensures lower latency when they use the workload.Where your data is located: For data-heavy applications, the major bottleneck in latency is data transfer. Application code should execute as close to the data as possible.Other constraints: Consider constraints such as security and compliance.Amazon EC2 provides placement groups for networking. A placement group is a logical grouping of instances to decrease latency or increase reliability. Using placement groups with supported instance types and an Elastic Network Adapter (ENA) enables workloads to participate in a low-latency, 25 Gbps network. Placement groups are recommended for workloads that benefit from low network latency, high network throughput, or both. Using placement groups has the benefit of lowering jitter in network communications.Latency-sensitive services are delivered at the edge using a global network of edge locations. These edge locations commonly provide services such as content delivery network (CDN) and domain name system (DNS). By having these services at the edge, workloads can respond with low latency to requests for content or DNS resolution. These services also provide geographic services such as geo targeting of content (providing different content based on the end users\u2019 location), or latency-based routing to direct end users to the nearest Region (minimum latency).Amazon CloudFront is a global CDN that can be used to accelerate both static content such as images, scripts, and videos, as well as dynamic content such as APIs or web applications. It relies on a global network of edge locations that will cache the content and provide high-performance network connectivity to your users. CloudFront also accelerates many other features such as content uploading and dynamic applications, making it a performance addition to all applications serving traffic over the internet. Lambda@Edge is a feature of Amazon CloudFront that will let you run code closer to users of your workload, which improves performance and reduces latency.Amazon Route 53 is a highly available and scalable cloud DNS web service. It\u2019s designed to give developers and businesses an extremely reliable and cost-effective way to route end users to internet applications by translating names, like www.example.com, into numeric IP addresses, like 192.168.2.1, that computers use to connect to each other. Route 53 is fully compliant with IPv6.AWS Outposts is designed for workloads that need to remain on-premises due to latency requirements, where you want that workload to run seamlessly with the rest of your other workloads in AWS. AWS Outposts are fully managed and configurable compute and storage racks built with AWS-designed hardware that allow you to run compute and storage on-premises, while seamlessly connecting to the broad array of AWS services in in the cloud.AWS Local Zones is designed to run workloads that require single-digit millisecond latency, like video rendering and graphics intensive, virtual desktop applications. Local Zones allow you to gain all the benefits of having compute and storage resources closer to end-users.AWS Wavelength is designed to deliver ultra-low latency applications to 5G devices by extending AWS infrastructure, services, APIs, and tools to 5G networks. Wavelength embeds storage and compute inside telco providers 5G networks to help your 5G workload if it requires single-digit millisecond latency, such as IoT devices, game streaming, autonomous vehicles, and live media production.Use edge services to reduce latency and to enable content caching. Ensure that you have configured cache control correctly for both DNS and HTTP/HTTPS to gain the most benefit from these approaches.Common anti-patterns:You consolidate all workload resources into one geographic location.You chose the closest region to your location but not to the workload end user.Benefits of establishing this best practice: You must ensure that your network is available wherever you want to reach customers. Using the AWS private global network ensures that your customers get the lowest latency experience by deploying workloads into the locations nearest them.Level of risk exposed if this best practice is not established: MediumReduce latency by selecting the correct locations: Identify where your users and data are located. Take advantage of AWS Regions, Availability Zones, placement groups, and edge locations to reduce latency.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n\nKnowledge Area: Configuring appropriate network routes to minimize network transfer costs (for example, Region to Region, Availability Zone to Availability Zone, private to public, Global Accelerator, VPC endpoints)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which strategy should the transportation company use to reduce network latency or improve throughput for their connected vehicle data processing?\",\"answer_choices\": [{\"answer\": \"Deploy workloads into the AWS regions closest to the end user\",\"is_correct\": \"true\",\"explanation\": \"Choosing a Region close to the workload\u2019s user ensures lower latency when they use the workload.\"},{\"answer\": \"Use AWS Global Accelerator\",\"is_correct\": \"false\",\"explanation\": \"AWS Global Accelerator is used to route traffic to the optimal AWS Region or Availability Zone, but will not reduce network latency or improve throughput.\"},{\"answer\": \"Use AWS Wavelength\",\"is_correct\": \"false\",\"explanation\": \"AWS Wavelength is designed to deliver ultra-low latency applications to 5G devices, but is not appropriate for the transportation company's connected vehicle data processing.\"},{\"answer\": \"Use an Elastic Network Adapter\",\"is_correct\": \"false\",\"explanation\": \"Using an Elastic Network Adaptor with supported instance types enables workloads to participate in a low-latency, 25 Gbps network, but will not reduce network latency or improve throughput.\"}]}", "docs": ["('PERF05-BP06 Choose your workload\u2019s location based on network requirements - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_location.html')"]}, {"scenario": "An education organization wants to use AWS to deliver online learning courses to students globally.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Skills in", "item": "Determining strategic needs for content delivery networks (CDNs) and edge caching", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nAn education organization wants to use AWS to deliver online learning courses to students globally.`\"\n\nContext:\n\n* Implement services to reduce data transfer. For example, using a content delivery network (CDN) such as Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using AWS Direct Connect instead of VPN for connectivity to AWS.Level of risk exposed if this best practice is not established: LowAmazon CloudFront is a global content delivery network that delivers data with low latency and high transfer speeds. It caches data at edge locations across the world, which reduces the load on your resources. By using CloudFront, you can reduce the administrative effort in delivering content to large numbers of users globally, with minimum latency.AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce network costs, increase bandwidth, and provide a more consistent network experience than internet-based connections.AWS VPN allows you to establish a secure and private connection between your private network and the AWS global network. It is ideal for small offices or business partners because it provides quick and easy connectivity, and it is a fully managed and elastic service.VPC Endpoints allow connectivity between AWS services over private networking and can be used to reduce public data transfer and NAT gateways costs. Gateway VPC endpoints have no hourly charges, and support Amazon Simple Storage Service(Amazon S3) and Amazon DynamoDB. Interface VPC endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost.Implementation stepsImplement services: Using the data transfer modeling, look at where the largest costs and highest volume flows are. Review the AWS services and assess whether there is a service that reduces or removes the transfer, specifically networking and content delivery. Also look for caching services where there is repeated access to data, or large amounts of data.Related documents:AWS Direct ConnectAWS Explore Our ProductsAWS caching solutionsAmazon CloudFrontDeliver content faster with Amazon CloudFront\n* Analyze network access patterns to identify where your customers are connecting from geographically. Select Regions and services that reduce the distance network traffic must travel to decrease the total network resources required to support your workload.Common anti-patterns:You select the workload's Region based on your own location.Benefits of establishing this best practice: Placing a workload close to its customers provides the lowest latency while decreasing data movement across the network and lowering environmental impact.Level of risk exposed if this best practice is not established: MediumSelect the Regions for your workload deployment based on the following key elements:Your Sustainability goal: as explained in Region selection.Where your data is located: For data-heavy applications (such as big data and machine learning), application code should execute as close to the data as possible.Where your users are located: For user-facing applications, choose a Region close to your workload\u2019s customer base.Other constraints: Consider constraints such as security and compliance as explained in What to Consider when Selecting a Region for your Workloads.Use AWS Local Zones to run workloads like video rendering and graphics-intensive virtual desktop applications. Local Zones allow you to benefit from having compute and storage resources closer to end users.Use local caching or AWS Caching Solutions for frequently used resources to improve performance, reduce data movement, and lower environmental impact.Amazon CloudFrontUse to cache static content such as images, scripts, and videos, as well as dynamic content such as API responses or web applications.Amazon ElastiCacheUse to cache content for web applications.DynamoDB AcceleratorUse to add in-memory acceleration to your DynamoDB tables.Use services that can help you run code closer to users of your workload:Lambda@EdgeUse for compute-heavy operations that are executed when objects are not in the cache.Amazon CloudFront FunctionsUse for simple use cases like HTTP(s) request or response manipulations that can be executed by short-lived functions.AWS IoT GreengrassUse to run local compute, messaging, and data caching for connected devices.Use connection pooling to enable connection reuse, and reduce required resources.Use distributed data stores that don\u2019t rely on persistent connections and synchronous updates for consistency to serve regional populations.Replace pre-provisioned static network capacity with shared dynamic capacity, and share the sustainability impact of network capacity with other subscribers.Related documents:Optimizing your AWS Infrastructure for Sustainability, Part III: NetworkingAmazon ElastiCache DocumentationWhat is Amazon CloudFront?Amazon CloudFront Key FeaturesLambda@EdgeCloudFront FunctionsAWS IoT GreengrassRelated videos:Building Sustainably on AWSRelated examples:AWS Networking WorkshopsServiceWhen to useAmazon CloudFrontUse to cache static content such as images, scripts, and videos, as well as dynamic content such as API responses or web applications.Amazon ElastiCacheUse to cache content for web applications.DynamoDB AcceleratorUse to add in-memory acceleration to your DynamoDB tables.ServiceWhen to useLambda@EdgeUse for compute-heavy operations that are executed when objects are not in the cache.Amazon CloudFront FunctionsUse for simple use cases like HTTP(s) request or response manipulations that can be executed by short-lived functions.AWS IoT GreengrassUse to run local compute, messaging, and data caching for connected devices.\n* The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.Best practices\n* Evaluate networking features in the cloud that may increase performance. Measure the impact of these features through testing, metrics, and analysis. For example, take advantage of network-level features that are available to reduce latency, packet loss, or jitter.\n\nKnowledge Area: Determining strategic needs for content delivery networks (CDNs) and edge caching\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"Which is the best solution for an education organization to use AWS to deliver online learning courses to students globally?\", \n\"answer_choices\": [{\"answer\": \"Using Amazon CloudFront to cache static content and Amazon ElastiCache for web applications\",\"is_correct\": \"True\",\"explanation\": \"Using Amazon CloudFront to cache static content such as images, scripts, and videos, as well as dynamic content such as API responses or web applications, and Amazon ElastiCache to cache content for web applications is the best solution for an education organization to use AWS to deliver online learning courses to students globally.\"},{\"answer\": \"Using a combination of Amazon VPC Endpoints and AWS Direct Connect\",\"is_correct\": \"False\",\"explanation\": \"Using Amazon VPC Endpoints and AWS Direct Connect is not the best solution for an education organization to use AWS to deliver online learning courses to students globally. Amazon CloudFront is a better solution for this purpose.\"},{\"answer\": \"Using Amazon S3 and DynamoDB Accelerator\",\"is_correct\": \"False\",\"explanation\": \"Using Amazon S3 and DynamoDB Accelerator is not the best solution for an education organization to use AWS to deliver online learning courses to students globally. Amazon CloudFront is a better solution for this purpose.\"},{\"answer\": \"Using AWS VPN and gateway VPC endpoints\",\"is_correct\": \"False\",\"explanation\": \"Using AWS VPN and gateway VPC endpoints is not the best solution for an education organization to use AWS to deliver online learning courses to students globally. Amazon CloudFront is a better solution for this purpose.\"}]}", "docs": ["('COST08-BP03 Implement services to reduce data transfer costs - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_data_transfer_implement_services.html')", "('SUS02-BP04 Optimize geographic placement of workloads for user locations - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_user_a5.html')", "('PERF 5\\xa0 How do you configure your networking solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-05.html')", "('PERF05-BP02 Evaluate available networking features - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_evaluate_features.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Skills in", "item": "Reviewing existing workloads for network optimizations", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* Workloads can change over time. Some services or features are more cost effective at different usage levels. By performing the analysis on each component over time and at projected usage, the workload remains cost-effective over its lifetime.Level of risk exposed if this best practice is not established: LowAs AWS releases new services and features, the optimal services for your workload may change. Effort required should reflect potential benefits. Workload review frequency depends on your organization requirements. If it is a workload of significant cost, implementing new services sooner will maximize cost savings, so more frequent review can be advantageous. Another trigger for review is change in usage patterns. Significant changes in usage can indicate that alternate services would be more optimal. For example, for higher data transfer rates a direct connect service may be cheaper than a VPN, and provide the required connectivity. Predict the potential impact of service changes, so you can monitor for these usage level triggers and implement the most cost-effective services sooner.Implementation stepsDefine predicted usage patterns: Working with your organization, such as marketing and product owners, document what the expected and predicted usage patterns will be for the workload.Perform cost analysis at predicted usage: Using the usage patterns defined, perform the analysis at each of these points. The analysis effort should reflect the potential outcome. For example, if the change in usage is large, a thorough analysis should be performed to verify any costs and changes.Related documents:AWS Total Cost of Ownership (TCO) CalculatorAmazon S3 storage classesCloud products\n* Existing workloads are regularly reviewed based on for each defined processes.Level of risk exposed if this best practice is not established: LowTo realize the benefits of new AWS services and features, you must execute the review process on your workloads and implement new services and features as required. For example, you might review your workloads and replace the messaging component with Amazon Simple Email Service (Amazon SES). This removes the cost of operating and maintaining a fleet of instances, while providing all the functionality at a reduced cost.Implementation stepsRegularly review the workload: Using your defined process, perform reviews with the frequency specified. Verify that you spend the correct amount of effort on each component. This process would be similar to the initial design process where you selected services for cost optimization. Analyze the services and the benefits they would bring, this time factor in the cost of making the change, not just the long-term benefits.Implement new services: If the outcome of the analysis is to implement changes, first perform a baseline of the workload to know the current cost for each output. Implement the changes, then perform an analysis to confirm the new cost for each output.Related documents:AWS News BlogTypes of Cloud ComputingWhat's New with AWS\n* Analyze and understand how network-related decisions impact workload performance. The network is responsible for the connectivity between application components, cloud services, edge networks and on-premises data and therefor it can highly impact workload performance. In addition to workload performance, user experience is also impacted by network latency, bandwidth, protocols, location, network congestion, jitter, throughput, and routing rules.Desired outcome: Have a documented list of networking requirements from the workload including latency, packet size, routing rules, protocols, and supporting traffic patterns. Review the available networking solutions and identify which service meets your workload networking characteristics. Cloud-based networks can be quickly rebuilt, so evolving your network architecture over time is necessary to improve performance efficiency.Common anti-patterns:All traffic flows through your existing data centers.You overbuild Direct Connect sessions without understanding the actual usage requirements.You don\u2019t consider workload characteristics and encryption overhead when defining your networking solutions.You use on-premises concepts and strategies for networking solutions in the cloud.Benefits of establishing this best practice: Understanding how networking impacts workload performance will help you identify potential bottlenecks, improve user experience, increase reliability, and lower operational maintenance as the workload changes.Level of risk exposed if this best practice is not established: HighIdentify important network performance metrics of your workload and capture its networking characteristics. Define and document requirements as part of a data-driven approach, using benchmarking or load testing. Use this data to identify where your network solution is constrained, and examine configuration options that could improve the workload. Understand the cloud-native networking features and options available and how they can impact your workload performance based on the requirements. Each networking feature has advantages and disadvantages and can be configured to meet your workload characteristics and scale based on your needs.Implementation steps:Define and document networking performance requirements:Include metrics such as network latency, bandwidth, protocols, locations, traffic patterns (spikes and frequency), throughput, encryption, inspection, and routing rulesCapture your foundational networking characteristics:VPC Flow LogsAWS Transit Gateway metricsAWS PrivateLink metricsCapture your application networking characteristics:Elastic Network AdaptorAWS App Mesh metricsAmazon API Gateway metricsCapture your edge networking characteristics:Amazon CloudFront metricsAmazon Route 53 metricsAWS Global Accelerator metricsCapture your hybrid networking characteristics:Direct Connect metricsAWS Site-to-Site VPN metricsAWS Client VPN metricsAWS Cloud WAN metricsCapture your security networking characteristics:AWS Shield, WAF, and Network Firewall metricsCapture end-to-end performance metrics with tracing tools:AWS X-RayAmazon CloudWatch RUMBenchmark and test network performance:Benchmark network throughput: Some factors that can affect EC2 network performance when the instances are in the same VPC. Measure the network bandwidth between EC2 Linux instances in the same VPC.Perform load tests to experiment with networking solutions and optionsLevel of effort for the implementation plan: There is a medium level of effort to document workload networking requirements, options, and available solutions.Related documents:Application Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to latency-based routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Improve Global Network Performance for ApplicationsEC2 Instances and Performance Optimization Best PracticesOptimizing Network Performance for Amazon EC2 InstancesNetworking best practices and tips with the Well-Architected FrameworkAWS networking best practices in large-scale migrationsRelated examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* Evaluate networking features in the cloud that may increase performance. Measure the impact of these features through testing, metrics, and analysis. For example, take advantage of network-level features that are available to reduce latency, packet loss, or jitter.\n\nKnowledge Area: Reviewing existing workloads for network optimizations\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"What is the best practice when reviewing existing workloads in the cloud to optimize network performance?\",\n\"answer_choices\": [\n    {\"answer\": \"Evaluate networking features available in the cloud that may increase performance by benchmarking, load testing, and collecting metrics.\",\"is_correct\": \"true\",\"explanation\": \"Reviewing workloads for network optimization should include evaluating available networking features in the cloud and measuring impact through testing, metrics, and analysis.\"},\n    {\"answer\": \"Replace the messaging component with Amazon Simple Email Service (Amazon SES).\",\"is_correct\": \"false\",\"explanation\": \"This is not the best practice for reviewing existing workloads for network optimization. The correct answer is to evaluate networking features available in the cloud that may increase performance by benchmarking, load testing, and collecting metrics.\"},\n    {\"answer\": \"Regularly review the workload using a defined process.\",\"is_correct\": \"false\",\"explanation\": \"This is not the best practice when reviewing existing workloads for network optimization. The correct answer is to evaluate networking features available in the cloud that may increase performance by benchmarking, load testing, and collecting metrics.\"},\n    {\"answer\": \"Define predicted usage patterns and perform a cost analysis at predicted usage.\",\"is_correct\": \"false\",\"explanation\": \"This is not the best practice when reviewing existing workloads for network optimization. The correct answer is to evaluate networking features available in the cloud that may increase performance by benchmarking, load testing, and collecting metrics.\"}\n]}", "docs": ["('COST05-BP06 Perform cost analysis for different usage over time - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_select_service_analyze_over_time.html')", "('COST10-BP02 Review and analyze this workload regularly - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_evaluate_new_services_review_workload.html')", "('PERF05-BP01 Understand how networking impacts performance - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_understand_impact.html')", "('PERF05-BP02 Evaluate available networking features - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_evaluate_features.html')"]}, {"scenario": "A company wants to run a disaster recovery plan for their critical systems on AWS.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Skills in", "item": "Selecting an appropriate throttling strategy", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA company wants to run a disaster recovery plan for their critical systems on AWS.`\"\n\nContext:\n\n* Buffering and throttling modify the demand on your workload, smoothing out any peaks. Implement throttling when your clients perform retries. Implement buffering to store the request and defer processing until a later time. Verify that your throttles and buffers are designed so clients receive a response in the required time.Level of risk exposed if this best practice is not established: LowThrottling: If the source of the demand has retry capability, then you can implement throttling. Throttling tells the source that if it cannot service the request at the current time it should try again later. The source will wait for a period of time and then re-try the request. Implementing throttling has the advantage of limiting the maximum amount of resources and costs of the workload. In AWS, you can use Amazon API Gateway to implement throttling. Refer to the Well-Architected Reliability pillar whitepaper for more details on implementing throttling.Buffer based: Similar to throttling, a buffer defers request processing, allowing applications that run at different rates to communicate effectively. A buffer-based approach uses a queue to accept messages (units of work) from producers. Messages are read by consumers and processed, allowing the messages to run at the rate that meets the consumers\u2019 business requirements. You don\u2019t have to worry about producers having to deal with throttling issues, such as data durability and backpressure (where producers slow down because their consumer is running slowly).In AWS, you can choose from multiple services to implement a buffering approach. Amazon Simple Queue Service(Amazon SQS) is a managed service that provides queues that allow a single consumer to read individual messages. Amazon Kinesis provides a stream that allows many consumers to read the same messages.When architecting with a buffer-based approach, ensure that you architect your workload to service the request in the required time, and that you are able to handle duplicate requests for work.Implementation stepsAnalyze the client requirements: Analyze the client requests to determine if they are capable of performing retries. For clients that cannot perform retries, buffers will need to be implemented. Analyze the overall demand, rate of change, and required response time to determine the size of throttle or buffer required.Implement a buffer or throttle: Implement a buffer or throttle in the workload. A queue such as Amazon Simple Queue Service (Amazon SQS) can provide a buffer to your workload components. Amazon API Gateway can provide throttling for your workload components.Related documents:AWS Auto ScalingAWS Instance SchedulerAmazon API GatewayAmazon Simple Queue ServiceGetting started with Amazon SQSAmazon Kinesis\n* Throttling requests is a mitigation pattern to respond to an unexpected increase in demand. Some requests are honored but those over a defined limit are rejected and return a message indicating they have been throttled. The expectation on clients is that they will back off and abandon the request or try again at a slower rate.Your services should be designed to handle a known capacity of requests that each node or cell can process. This capacity can be established through load testing. You then need to track the arrival rate of requests and if the temporary arrival rate exceeds this limit, the appropriate response is to signal that the request has been throttled. This allows the user to retry, potentially to a different node or cell that might have available capacity. Amazon API Gateway provides methods for throttling requests. Amazon SQS and Amazon Kinesis can buffer requests, smooth out the request rate, and alleviate the need for throttling for requests that can be addressed asynchronously.Level of risk exposed if this best practice is not established: HighThrottle requests. This is a mitigation pattern to respond to an unexpected increase in demand. Some requests are honored but those over a defined limit are rejected and return a message indicating they have been throttled. The expectation on clients is that they will back off and abandon the request or try again at a slower rate.Use Amazon API GatewayThrottle API Requests for Better ThroughputRelated documents:Amazon API Gateway: Throttle API Requests for Better ThroughputError Retries and Exponential Backoff in AWSThe Amazon Builders' Library: Avoiding fallback in distributed systemsThe Amazon Builders' Library: Avoiding insurmountable queue backlogsThe Amazon Builders' Library: Timeouts, retries, and backoff with jitterThrottle API Requests for Better ThroughputRelated videos:Retry, backoff, and jitter: AWS re:Invent 2019: Introducing The Amazon Builders\u2019 Library (DOP328)\n* Where applicable, use multiple strategies to improve performance. For example, using strategies like caching data to prevent excessive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking.As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Common anti-patterns:You assume that workload performance is adequate if customers are not complaining.You only collect data on performance after you have made performance-related changes.Benefits of establishing this best practice: To optimize performance and resource utilization, you need a unified operational view, real-time granular data, and historical reference. You can create dashboards and perform metric math on your data to derive operational and utilization insights for your workloads as they change over time.Level of risk exposed if this best practice is not established: LowUse a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.Related documents:Amazon Builders\u2019 LibraryBest Practices for Implementing Amazon ElastiCacheAWS Database CachingAmazon CloudWatch RUMDistributed Load Testing on AWSRelated videos:Introducing The Amazon Builders\u2019 Library (DOP328)AWS purpose-built databases (DAT209-L)Optimize applications through Amazon CloudWatch RUMRelated examples:Measure page load time with Amazon CloudWatch SyntheticsAmazon CloudWatch RUM Web ClientDistributed Load Testing on AWS\n\nKnowledge Area: Selecting an appropriate throttling strategy\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\": \"In the given scenario, what would be the most appropriate throttling strategy for the company to use for their disaster recovery plan on AWS?\", \n\"answer_choices\": [\n{\"answer\": \"Use Amazon SQS to buffer requests and smooth out the request rate\",\"is_correct\": \"false\",\"explanation\": \"SQS is used for buffering messages, not requests\"}, \n{\"answer\": \"Use Amazon Kinesis to store the requests and defer processing until a later time\",\"is_correct\": \"false\",\"explanation\": \"Kinesis is used for streaming messages, not requests\"}, \n{\"answer\": \"Use Amazon API Gateway to implement throttling\",\"is_correct\": \"true\",\"explanation\": \"API Gateway is used for throttling requests\"}, \n{\"answer\": \"Use Amazon CloudWatch RUM to measure the impacts to the end user\",\"is_correct\": \"false\",\"explanation\": \"CloudWatch RUM is used to measure the performance and resource utilization of the workload\"} \n]}", "docs": ["('COST09-BP02 Implement a buffer or throttle to manage demand - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_manage_demand_resources_buffer_throttle.html')", "('REL05-BP02 Throttle requests - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_mitigate_interaction_failure_throttle_requests.html')", "('PERF08-BP05 Use various performance-related strategies - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_tradeoffs_performance_implement_strategy.html')"]}, {"scenario": "A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.", "domain": "Domain 4: Design Cost-Optimized Architectures", "task": "Task Statement 4: Design cost-optimized network architectures", "focus": "Skills in", "item": "Selecting the appropriate bandwidth allocation for a network device (for example, a single VPN compared with multiple VPNs, Direct Connect speed)", "prompt": "Generate a scenario-based multiple-choice question for the AWS Certified Solutions Architect Associate Exam using the provided scenario, context, and knowledge area. The response must be returned in the specified JSON format with nothing else. There should be one correct answer and 3 incorrect answers. The incorrect answers should be response options that a candidate with incomplete knowledge or skill might choose. Provide an explanation for the answer to each question as well. The question must be about a scenario, and not a simple definition question such as What type of storage is Amazon S3. The answers must also be action-oriented and not just the name of a service.\n\nScenario:\nA transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations.`\"\n\nContext:\n\n* When a common network is required to connect on-premises and cloud resources in AWS, ensure that you have adequate bandwidth to meet your performance requirements. Estimate the bandwidth and latency requirements for your hybrid workload. These numbers will drive the sizing requirements for AWS Direct Connect or your VPN endpoints.Desired outcome: When deploying a workload that will need hybrid network connectivity, you have multiple configuration options for connectivity, such as managed and non-managed VPNs or Direct Connect. Select the appropriate connection type for each workload while ensuring you have adequate bandwidth and encryption requirements between your location and the cloud.Common anti-patterns:You only evaluate VPN solutions for your network encryption requirements.You don\u2019t evaluate backup or parallel connectivity options.You use default configurations for routers, tunnels, and BGP sessions.You fail to understand or identify all workload requirements (encryption, protocol, bandwidth and traffic needs).Benefits of establishing this best practice: Selecting and configuring appropriately sized hybrid network solutions will increase the reliability of your workload and maximize performance opportunities. By identifying workload requirements, planning ahead, and evaluating hybrid solutions you will minimize expensive physical network changes and operational overhead while increasing your time to market.Level of risk exposed if this best practice is not established: HighDevelop a hybrid networking architecture based on your bandwidth requirements: Estimate the bandwidth and latency requirements of your hybrid applications. Based on your bandwidth requirements, a single VPN or Direct Connect connection might not be enough, and you must architect a hybrid setup to enable traffic load balancing across multiple connections. Direct connect may be required which offers more predictable and consistent performance due to its private network connectivity. It is great for production workloads that require consistent latency and almost zero jitter.AWS Direct Connect provides dedicated connectivity to the AWS environment, from 50 Mbps up to 10 Gbps. This gives you managed and controlled latency and provisioned bandwidth so your workload can connect easily and in a performant way to other environments. Using one of the AWS Direct Connect partners, you can have end-to-end connectivity from multiple environments, thus providing an extended network with consistent performance.The AWS Site-to-Site VPN is a managed VPN service for VPCs. When a VPN connection is created, AWS provides tunnels to two different VPN endpoints. With AWS Transit Gateway, you can simplify the connectivity between multiple VPCs and also connect to any VPC attached to AWS Transit Gateway with a single VPN connection. AWS Transit Gateway also enables you to scale beyond the 1.25Gbps IPsec VPN throughput limit by enabling equal cost multi-path (ECMP) routing support over multiple VPN tunnels.Level of effort for the implementation plan: There is a high level of effort to evaluate workload needs for hybrid networks and to implement hybrid networking solutions.Related documents:Network Load BalancerNetworking Products with AWSTransit GatewayTransitioning to latency-based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsSite-to-Site VPNBuilding a Scalable and Secure Multi-VPC AWS Network InfrastructureDirect ConnectClient VPNRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)AWS Global AcceleratorDirect ConnectTransit Gateway ConnectVPN SolutionsSecurity with VPN SolutionsRelated examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.Best practices\n* Make decisions about protocols for communication between systems and networks based on the impact to the workload\u2019s performance.There is a relationship between latency and bandwidth to achieve throughput. If your file transfer is using TCP, higher latencies will reduce overall throughput. There are approaches to fix this with TCP tuning and optimized transfer protocols, some approaches use UDP.Common anti-patterns:You use TCP for all workloads regardless of performance requirements.Benefits of establishing this best practice: Selecting the proper protocol for communication between workload components ensures that you are getting the best performance for that workload. Connection-less UDP allows for high speed, but it doesn't offer retransmission or high reliability. TCP is a full featured protocol, but it requires greater overhead for processing the packets.Level of risk exposed if this best practice is not established: MediumOptimize network traffic: Select the appropriate protocol to optimize the performance of your workload. There is a relationship between latency and bandwidth to achieve throughput. If your file transfer is using TCP, higher latencies reduce overall throughput. There are approaches to fix latency with TCP tuning and optimized transfer protocols, some which use UDP.Related documents:Amazon EBS - Optimized InstancesApplication Load BalancerEC2 Enhanced Networking on LinuxEC2 Enhanced Networking on WindowsEC2 Placement GroupsEnabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux InstancesNetwork Load BalancerNetworking Products with AWSTransit GatewayTransitioning to Latency-Based Routing in Amazon Route 53VPC EndpointsVPC Flow LogsRelated videos:Connectivity to AWS and hybrid AWS network architectures (NET317-R1)Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)Related examples:AWS Transit Gateway and Scalable Security SolutionsAWS Networking Workshops\n* Since the network is between all workload components, it can have great impacts, both positive and negative, on workload performance and behavior. There are also workloads that are heavily dependent on network performance such as High Performance Computing (HPC) where deep network understanding is important to increase cluster performance. You must determine the workload requirements for bandwidth, latency, jitter, and throughput.On AWS, networking is virtualized and is available in a number of different types and configurations. This makes it easier to match your networking methods with your needs. AWS offers product features (for example, Enhanced Networking, Amazon EBS-optimized instances, Amazon S3 transfer acceleration, and dynamic Amazon CloudFront) to optimize network traffic. AWS also offers networking features (for example, Amazon Route 53 latency routing, Amazon VPC endpoints, AWS Direct Connect, and AWS Global Accelerator) to reduce network distance or jitter.The following question focuses on these considerations for performance efficiency.You must consider location when deploying your network. You can choose to place resources close to where they will be used to reduce distance. Use networking metrics to make changes to networking configuration as the workload evolves. By taking advantage of Regions, placement groups, and edge services, you can significantly improve performance. Cloud based networks can be quickly re-built or modified, so evolving your network architecture over time is necessary to maintain performance efficiency.PERF 5: How do you configure your networking solution?The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location options. These constraints can be offset with edge locations or resource placement.\n\nKnowledge Area: Selecting the appropriate bandwidth allocation for a network device (for example, a single VPN compared with multiple VPNs, Direct Connect speed)\n\nJSON Format:\n{\"question\": \"\",\"answer_choices\": [{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"},{\"answer\": \"\",\"is_correct\": \"\",\"explanation\": \"\"}]}\n", "question": "{\"question\":\"A transportation company wants to use AWS to process real-time data from connected vehicles to optimize their operations. What is the best way to configure the networking solution for this workload to ensure optimal performance efficiency?\",\n\"answer_choices\": [ \n{\"answer\": \"Use TCP for all workloads regardless of performance requirements.\",\"is_correct\":\"false\",\"explanation\":\"Using TCP for all workloads regardless of performance requirements will not optimize the workload performance. TCP requires greater overhead for processing the packets, while connection-less UDP can allow for high speed but with no retransmission or reliability.\"}, \n{\"answer\": \"Select the appropriate protocol to optimize the performance of your workload.\",\"is_correct\":\"true\",\"explanation\":\"The optimal network solution for a workload varies based on latency, throughput requirements, jitter, and bandwidth. Selecting the proper protocol for communication between workload components ensures that you are getting the best performance for that workload. Connection-less UDP allows for high speed, but it doesn't offer retransmission or high reliability. TCP is a full featured protocol, but it requires greater overhead for processing the packets.\"}, \n{\"answer\": \"Choose AWS Direct Connect for dedicated connectivity to the AWS environment.\",\"is_correct\":\"false\",\"explanation\":\"AWS Direct Connect provides dedicated connectivity to the AWS environment, from 50 Mbps up to 10 Gbps. However, depending on the bandwidth requirements, a single VPN or Direct Connect connection might not be enough, and you must architect a hybrid setup to enable traffic load balancing across multiple connections.\"}, \n{\"answer\": \"Place resources close to where they will be used to reduce distance.\",\"is_correct\":\"false\",\"explanation\":\"You must consider location when deploying your network. You can choose to place resources close to where they will be used to reduce distance, but you must also take into account the workload requirements for bandwidth, latency, jitter, and throughput to ensure optimal performance efficiency.\"}]}", "docs": ["('PERF05-BP03 Choose appropriately sized dedicated connectivity or VPN for hybrid workloads - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_hybrid.html')", "('PERF 5\\xa0 How do you configure your networking solution? - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-05.html')", "('PERF05-BP05 Choose network protocols to improve performance - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_select_network_protocols.html')", "('Network - AWS Well-Architected Framework', 'https://docs.aws.amazon.com/wellarchitected/latest/framework/perf-network.html')"]}]}